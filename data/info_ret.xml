<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>http://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.25wmf17</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2600" case="first-letter">Topic</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Category:Searching</title>
    <ns>14</ns>
    <id>1419629</id>
    <revision>
      <id>588940245</id>
      <parentid>547830207</parentid>
      <timestamp>2014-01-03T07:22:06Z</timestamp>
      <contributor>
        <username>BotMultichill</username>
        <id>4080734</id>
      </contributor>
      <minor/>
      <comment>Adding Commons category link to [[:Commons:Category:Searching|category with the same name]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="208">{{Commons category|Searching}}
[[Category:Information retrieval]]
&lt;!-- used to be on cat search engines that was merged into this, but didn't think it really applied... [[Category:Information technology]] --&gt;</text>
      <sha1>3sd7oje0wgnbt74yfqlwo8app3xf5n9</sha1>
    </revision>
  </page>
  <page>
    <title>Proximity search (text)</title>
    <ns>0</ns>
    <id>1934622</id>
    <revision>
      <id>637153653</id>
      <parentid>636168239</parentid>
      <timestamp>2014-12-08T11:31:22Z</timestamp>
      <contributor>
        <ip>77.64.195.170</ip>
      </contributor>
      <comment>Syntactical correction of a reference for the AROUND operator.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6717">In [[natural language processing|text processing]], a '''proximity search''' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.

For example, a search could be used to find &quot;red brick house&quot;, and match phrases such as &quot;red house of brick&quot; or &quot;house made of red brick&quot;. By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.

== Rationale ==
The basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.

Commercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].

== Boolean syntax and operators ==
Note that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, &quot;brick NEAR house&quot;.

== Usage in commercial search engines ==
In regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good &quot;overall proximity score&quot; in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).

[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]] and [[Altavista]]:
* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.&lt;ref&gt;[http://www.walhello.com/aboutgl.html &quot;About Walhello&quot;], visited 23 December 2009&lt;/ref&gt;
* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is &lt;tt&gt;(keyword1 NEAR/n keyword2)&lt;/tt&gt; where n is the number of words.&lt;ref&gt;[http://www.exalead.com/search/web/search-syntax/#proximity_search &quot;Web Search Syntax&quot;], visited 23 December 2009&lt;/ref&gt;
* [[Yandex]] uses the syntax &lt;tt&gt;keyword1 /n keyword2&lt;/tt&gt; to search for two keywords separated by at most &lt;math&gt;n - 1&lt;/math&gt; words, and supports a few other variations of this syntax.&lt;ref&gt;[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)&lt;/ref&gt;
* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.&lt;ref&gt;[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional &quot;Successful Yahoo! proximity query&quot;] (22 Feb 2010)&lt;/ref&gt;&lt;ref&gt;[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused &quot;Unsuccessful Yahoo! proximity query&quot;] (22 Feb 2010)&lt;/ref&gt; The syntax is &lt;tt&gt;keyword1 NEAR keyword2&lt;/tt&gt;.
* Google supports AROUND(#).&lt;ref&gt;[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ &quot;GuidingTech: Meet Google Search's Little Known AROUND Operator&quot;]&lt;/ref&gt;

Ordered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,&lt;ref&gt;[http://www.google.com/support/websearch/bin/answer.py?answer=136861 &quot;More Google Search Help&quot; visited 23 December 2009]&lt;/ref&gt; and an in Yahoo! Search this matches exactly one word.&lt;ref&gt;[http://www.searchengineshowdown.com/features/yahoo/review.html &quot;Review of Yahoo! Search&quot;, by Search Engine Showdown, visited 23 December 2009]&lt;/ref&gt;  (This is easily verified by searching for the following phrase in both Google and Yahoo!: &quot;addictive * of biblioscopy&quot;.)

To emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of &quot;house&quot; and &quot;dog&quot;, the following search-expression could be specified: &quot;house dog&quot; OR &quot;dog house&quot; OR &quot;house * dog&quot; OR &quot;dog * house&quot; OR &quot;house * * dog&quot; OR &quot;dog * * house&quot;.

== See also ==
* [[Compound term processing]]
* [[Edit distance]]
* [[Information retrieval]]
* [[Search engine]]
* [[Search engine indexing]] - how texts are indexed to support proximity search
* [[Semantic proximity]]

== Notes ==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Internet search algorithms]]</text>
      <sha1>lun7mxwqivv8qn40i0n260way6veike</sha1>
    </revision>
  </page>
  <page>
    <title>Document retrieval</title>
    <ns>0</ns>
    <id>731640</id>
    <revision>
      <id>641824598</id>
      <parentid>619306508</parentid>
      <timestamp>2015-01-10T03:48:10Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5766">'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.

Document retrieval is sometimes referred to as, or as a branch of, '''Text Retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.

==Description==
Document retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.

A document retrieval system has two main tasks:
# Find relevant documents to user queries
# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].

Internet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.

==Variations==
There are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.

===Form based===
Form based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.

===Content based===
The content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.

A ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.

==Example: PubMed==
The [[PubMed]]&lt;ref&gt;{{cite journal |author=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319–23 |year=2001 |pmid=11825203 |pmc=2243528 }}
&lt;/ref&gt; form interface features the &quot;related articles&quot; search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.&lt;ref&gt;{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}&lt;/ref&gt;

== See also ==

* [[Compound term processing]]
* [[Document classification]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Latent semantic indexing]]
* [[Search engine]]

== References ==

&lt;references/&gt;

==Further reading==
* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems (TOIS)|volume=2|issue=4|year=1984|pages=267–288|doi=10.1145/2275.357411}}
* {{cite journal|author=Justin Zobel, Alistair Moffat and Kotagiri Ramamohanarao|title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems (TODS)|volume=23|issue=4|year=1998|pages= 453–490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}
* {{cite journal|author=Ben Carterette and Fazli Can|title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613–633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}

== External links ==
* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College

[[Category:Information retrieval]]
[[Category:Electronic documents]]
[[Category:Substring indices]]

[[zh:文本信息检索]]</text>
      <sha1>qkr675kavfhg74mpqbs5lclj2qt348v</sha1>
    </revision>
  </page>
  <page>
    <title>Subsetting</title>
    <ns>0</ns>
    <id>3231582</id>
    <revision>
      <id>613461225</id>
      <parentid>613460565</parentid>
      <timestamp>2014-06-18T18:36:44Z</timestamp>
      <contributor>
        <username>Staticshakedown</username>
        <id>6179713</id>
      </contributor>
      <comment>added ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1061">In research communities (for example, [[earth science]]s), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client—server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.

Subsetting may be favorable for the following reasons:&lt;ref name=&quot;Institute2012&quot;&gt;{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=http://books.google.com/books?id=OE0UfAhit4kC&amp;pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}&lt;/ref&gt;
* restrict the time range
* select [[Cross-sectional data|cross section]]s of data
* select particular kinds of [[time series]]
* exclude particular obersvations

==References==
{{reflist}}


==External links==
*[http://www.subset.org/index.jsp Subset.org]

[[Category:Information retrieval]]

{{Statistics-stub}}</text>
      <sha1>rrav4iq0793wvrrukxq9qx8yybwnhwz</sha1>
    </revision>
  </page>
  <page>
    <title>Divergence-from-randomness model</title>
    <ns>0</ns>
    <id>1798853</id>
    <revision>
      <id>592501389</id>
      <parentid>585958470</parentid>
      <timestamp>2014-01-26T17:22:12Z</timestamp>
      <contributor>
        <ip>146.50.68.247</ip>
      </contributor>
      <comment>Adds Category:Ranking functions</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="566">In the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.

Term weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.

==External links==
*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]
*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]

[[Category:Ranking functions]]
[[Category:Information retrieval]]
[[Category:Probabilistic models]]


{{comp-sci-stub}}</text>
      <sha1>2mkwjuppic38bzj85czvoff604g6aba</sha1>
    </revision>
  </page>
  <page>
    <title>Category:String similarity measures</title>
    <ns>14</ns>
    <id>9833053</id>
    <revision>
      <id>544714325</id>
      <parentid>524344363</parentid>
      <timestamp>2013-03-16T18:58:46Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7013670]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="209">{{Cat main|String metrics}}

[[Category:Algorithms on strings|Similarity]]
[[Category:Information retrieval]]
[[Category:Metric geometry]]
[[Category:Information theory]]
[[Category:String (computer science)]]</text>
      <sha1>8j6reasao8xk5jbvi92mx9o1v4nuck1</sha1>
    </revision>
  </page>
  <page>
    <title>IFACnet</title>
    <ns>0</ns>
    <id>7344222</id>
    <revision>
      <id>633386914</id>
      <parentid>577927923</parentid>
      <timestamp>2014-11-11T14:54:10Z</timestamp>
      <contributor>
        <username>Conquerist</username>
        <id>2198133</id>
      </contributor>
      <minor/>
      <comment>Disambiguated: [[IFAC]] → [[International Federation of Accountants]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2719">'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.

The following 31 organizations participate in IFACnet:

*[[American Institute of Certified Public Accountants]] (AICPA)
*[[Association of Chartered Certified Accountants]] (ACCA)
*[[Canadian Institute of Chartered Accountants]]
*[[Certified General Accountants Association of Canada]]
*[[Chartered Institute of Management Accountants]] (CIMA)
*[[Chartered Institute of Public Finance and Accountancy]]
*[[CMA Canada]]
*[[Compagnie Nationale des Commissaires aux Comptes]]
*[[Conseil Supérieur de l'Ordre des Experts-Comptables]]
*[[Consiglio Nazionale Dottori Commercialisti]]
*[[CPA Australia]]
*[[Délégation Internationale Pour l'Audit et la Comptabilité]]
*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)
*[[International Federation of Accountants]]  (IFAC)
*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)
*[[Institute of Certified Public Accountants in Ireland]]
*[[Institute of Certified Public Accountants of Singapore]]
*[[Institute of Chartered Accountants of Australia]]
*[[Institute of Chartered Accountants in England &amp; Wales]] (ICAEW)
*[[Institute of Chartered Accountants in Ireland]]
*[[Institute of Chartered Accountants of India]]
*[[Institute of Chartered Accountants of Pakistan]]
*[[Institute of Chartered Accountants of Scotland]] (ICAS)
*[[Institute of Management Accountants]]
*[[Japanese Institute of Certified Public Accountants]] (JICPA)
*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)
*[[Malaysian Institute of Accountants]]
*[[Malta Institute of Accountants]]
*[[National Association of State Boards of Accountancy]] (NASBA)
*[[South African Institute of Chartered Accountants]] (SAICA)
*[[Union of Chambers of Certified Public Accountants of Turkey]] (TÜRMOB)

==External links==
*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]
*[http://www.ifac.org/ International Federation of Accountants Homepage]

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Accounting organizations]]</text>
      <sha1>2vovxnu1liaqi8uuactex6n6l3ex5ai</sha1>
    </revision>
  </page>
  <page>
    <title>European Conference on Information Retrieval</title>
    <ns>0</ns>
    <id>10328235</id>
    <revision>
      <id>605442658</id>
      <parentid>574483817</parentid>
      <timestamp>2014-04-23T12:08:06Z</timestamp>
      <contributor>
        <username>Rbierig</username>
        <id>21249416</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3172">The '''European Conference on Information Retrieval''' (ECIR) is the main 
European research conference for the presentation of new results in the field of [[information retrieval]] (IR).
It is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).
      
The event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was 
held in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has
alternated between the United Kingdom and continental Europe. To mark the metamorphosis
from a small informal colloquium to a major event in the IR research calendar, the 
BCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,
ECIR has continued to grow and has become the major European forum for the discussion
of research in the field of Information Retrieval. 

Some of the topics dealt with include:
* IR models, techniques, and algorithms
* IR applications
* IR system architectures
* Test and evaluation methods for IR
* [[Natural Language Processing]] for IR
* Distributed IR
* Multimedia and cross-media IR

==Time and Location==

Traditionally, the ECIR is held in Spring, near the Easter weekend. Previous locations include
the following:

* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]
* [[Moscow, Russia]], 2013 [http://ecir2013.org/]
* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]
* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]
* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]
* [[Toulouse]], 2009 [http://ecir09.irit.fr/]
* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]
* [[Rome]], 2007 [http://ecir2007.fub.it/]
* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]
* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]
* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]
* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]
* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*
* [[Darmstadt]], 2001* (organized by GMD)
* [[Cambridge]], 2000* (organized by Microsoft Research)
* [[Glasgow]], 1999*
* [[Grenoble]], 1998*
* [[Aberdeen, Scotland|Aberdeen]], 1997*
* [[Manchester]], 1996*
* [[Crewe]], 1995* (organized by Manchester Metropolitan University)
* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)
* [[Glasgow]], 1993* (organized by Strathclyde University)
* [[Lancaster, Lancashire|Lancaster]], 1992*
* [[Lancaster, Lancashire|Lancaster]], 1991*
* [[Huddersfield]], 1990*
* [[Huddersfield]], 1989*
* [[Huddersfield]], 1988*
* [[Glasgow]], 1987*
* [[Glasgow]], 1986*
* [[Bradford]], 1985*
* [[Bradford]], 1984*
* [[Sheffield]], 1983*
* [[Sheffield]], 1982*
* [[Birmingham]], 1981*
* [[Leeds]], 1980*
* [[Leeds]], 1979*

&lt;br /&gt; *as the Annual Colloquium on Information Retrieval Research

Future locations include:
* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]

==External links==
* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]

[[Category:Information retrieval]]
[[Category:Computer science conferences]]</text>
      <sha1>cx9t9czo3d03943zponie29k77avgo4</sha1>
    </revision>
  </page>
  <page>
    <title>National Centre for Text Mining</title>
    <ns>0</ns>
    <id>10795520</id>
    <revision>
      <id>595980617</id>
      <parentid>549011234</parentid>
      <timestamp>2014-02-18T04:34:35Z</timestamp>
      <contributor>
        <username>Mr. Granger</username>
        <id>4871659</id>
      </contributor>
      <comment>Repairing links to disambiguation pages - [[Wikipedia:Disambiguation pages with links|You can help!]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4505">The '''National Centre for Text Mining''' (NaCTeM)
&lt;ref name=&quot;ariadne&quot;&gt;{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}&lt;/ref&gt; is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community. 

The [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest - examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].

The Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[information extraction]], [[natural language processing]] and parallel and distributed data mining systems in biomedical and clinical applications.

==Services==
[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them. &lt;ref name=&quot;multi-word&quot;&gt;{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117–132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}&lt;/ref&gt; 

[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[Disambiguation|disambiguates]] them.&lt;ref name=&quot;pmid17050571&quot;&gt;{{cite journal| author=Okazaki N, Ananiadou S| title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089–95 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&amp;tool=sumsearch.org/cite&amp;retmode=ref&amp;cmd=prlinks&amp;id=17050571  }} &lt;/ref&gt;

[http://www-tsujii.is.s.u-tokyo.ac.jp/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts.

[http://refine1-nactem.mc.man.ac.uk/facta/ ''' Facta+'''] is a MEDLINE search engine for finding associations between biomedical concepts.&lt;ref name=&quot;pmid18772154&quot;&gt;{{cite journal| author=Tsuruoka Y, Tsujii J, Ananiadou S| title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559–60 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }} &lt;/ref&gt;

[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system based on MEDLINE.

[https://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].

==Resources==

[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] a large-scale terminological resource for the biomedical domain

[http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=GENIA+corpus '''GENIA'''] a collection of reference materials for the development of biomedical text mining systems

==References==
{{Reflist}}

==External links==
* http://www.nactem.ac.uk

[[Category:Computational linguistics]]
[[Category:Computer science organizations]]
[[Category:Information retrieval]]
[[Category:Linguistics organizations]]
[[Category:School of Computer Science, University of Manchester]]</text>
      <sha1>ghn2teiphilaerhg1wajgwedfnm8tde</sha1>
    </revision>
  </page>
  <page>
    <title>Bioinformatic Harvester</title>
    <ns>0</ns>
    <id>2611971</id>
    <revision>
      <id>598624796</id>
      <parentid>590766975</parentid>
      <timestamp>2014-03-08T00:28:34Z</timestamp>
      <contributor>
        <username>Terrycojones</username>
        <id>119120</id>
      </contributor>
      <minor/>
      <comment>/* top */ Correct awkward 10.000s to &quot;tens of thousands&quot;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6196">The '''Bioinformatic Harvester''' is a bioinformatic meta [[search engine]] created by the [[European Molecular Biology Laboratory]]&lt;ref&gt;{{Cite journal|title= 	Information retrieval on Internet using meta-search engines: A review|authors=Manoj, M, Elizabeth, Jacob |date=Oct 2008|publisher=CSIR|pages=739–746|issn=0022-4456
|journal=JSIR |volume=67 (10)}}&lt;/ref&gt; and subsequently hosted and further developed by KIT [[Karlsruhe Institute of Technology]] for [[gene]]s and protein-associated information. Harvester currently works for [[human]], [[mouse]], [[rat]], [[zebrafish]], [[drosophila]] and [[arabidopsis thaliana]] based information. Harvester cross-links &gt;50 popular bioinformatic resources and allows cross searches. Harvester serves tens of thousands of pages every day to scientists and physicians.

{{Infobox software
| name                  = Bioinformatic Harvester
|developer              = Urban Liebel, Björn Kindler
|latest release version = 4
|latest release date    = {{release date and age|2011|05|24}}
|operating_system       = Web based
|genre                  = Bioinformatics tool
|license                = Public Domain
|website                = http://harvester.kit.edu
}}

== How Harvester works ==

Harvester collects information from [[protein]] and gene databases along with information from so called &quot;prediction servers.&quot; Prediction server e.g. provide online sequence analysis for a single protein. Harvesters search index is based on the [[International Protein Index|IPI]] and [[UniProt]] protein information collection. The collections consists of:

* ~72.000 human, ~57.000 mouse, ~41.000 rat, ~51.000 zebrafish, ~35.000 arabidopsis protein pages, which cross-link ~50 major bioinfiormatic resources.

&lt;!-- Deleted image removed: [[Image:harvester-kit.JPG|thumb| A screenshot of the [http://harvester.kit.edu/ Harvester search engine]]] --&gt;

== Harvester crosslinks several types of information ==

===Text based information===
from the following databases:

* [[UniProt]], world largest protein database
* [[SOURCE]], convenient gene information overview
* [[Simple Modular Architecture Research Tool]] (SMART),
* [[SOSUI]], predicts transmembrane domains
* [[PSORT]], predicts protein localisation
* [[HomoloGene]], compares proteins from different species
* [[gfp-cdna]], protein localisation with fluorescence microscopy
* [[International Protein Index]] (IPI).

=== Databases rich in graphical elements ===
...are not collected, but crosslinked via [[iframe]]s. Iframes are transparent windows within a [[HTML]] pages. The iframe windows allows up-to-date viewing of the &quot;iframed,&quot; linked databases. Several such iframes are combined on a Harvester protein page. This method allows convenient comparison of information from several databases.

* NCBI-[[BLAST]], an algorithm for comparing biological sequences from the [[National Center for Biotechnology Information|NCBI]].
* [[Ensembl]], automatic gene annotation by the EMBL-[[European Bioinformatics Institute|EBI]] and [[Sanger Institute]]
* [[FlyBase]] is a database of model organism ''[[Drosophila melanogaster]]''.
* [[GoPubMed]] is a knowledge-based search engine for biomedical texts.
* [[Information Hyperlinked over Proteins|iHOP]], information hyperlinked over proteins via gene/protein synonyms
* [[Mendelian Inheritance in Man]] project catalogues all the known diseases.
* [[RZPD]], German resources Center for genome research in Berlin/Heidelberg.
* [[STRING]], Search Tool for the Retrieval of Interacting Genes/Proteins, developed by [[EMBL]], [[Swiss Institute of Bioinformatics|SIB]] and [[University of Zurich|UZH]].
* [[Zebrafish Information Network]].
* [http://locate.imb.uq.edu.au/ LOCATE] subcellular localization database (mouse).

=== Access from external application ===

* [[Genome browser]], working draft assemblies for genomes [[University of California, Santa Cruz|UCSC]]
* [[Google Scholar]]
* [[Mitocheck]]
* [[PolyMeta]], meta search engine for Google, Yahoo, MSN, Ask, Exalead, AllTheWeb, GigaBlast

== What one can find ==

Harvester allows a combination of different search terms and single words.

Search Examples:

* Gene-name: &quot;golga3&quot;
* Gene-alias: &quot;ADAP-S ADAS ADHAPS ADPS&quot; (one gene name is sufficient)
* Gene-Ontologies: &quot;Enzyme linked receptor protein signaling pathway&quot;
* [[UniGene|Unigene]]-Cluster: &quot;Hs.449360&quot;

* Go-annotation: &quot;intra-Golgi transport&quot;
* Molecular function: &quot;protein kinase binding&quot;
* Protein: &quot;Q9NPD3&quot;
* Protein domain: &quot;SH2 sar&quot;
* Protein Localisation: &quot;endoplasmic reticulum&quot;

* Chromosome: &quot;2q31&quot;
* Disease relevant: use the word &quot;diseaselink&quot;
* Combinations: &quot;golgi diseaselink&quot; (finds all golgi proteins associated with a disease)
* [[mRNA]]: &quot;AL136897&quot;

* Word: &quot;Cancer&quot;
* Comment: &quot;highly expressed in heart&quot;
* Author: &quot;Merkel, Schmidt&quot;
* Publication or project: &quot;[[cDNA]] sequencing project&quot;

==See also==

* [[Biological database]]s
* [[Entrez]]
* [[European Bioinformatics Institute]]
* [[HPRD|Human Protein Reference Database]]
* [[Metadata]]
* [[Sequence profiling tool]]

== Literature ==
*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title='Harvester': a fast meta search engine of human protein resources |journal=Bioinformatics |volume=20 |issue=12 |pages=1962–3 |date=August 2004 |pmid=14988114 |doi=10.1093/bioinformatics/bth146 |url=http://bioinformatics.oxfordjournals.org/cgi/pmidlookup?view=long&amp;pmid=14988114}}
*{{cite journal |author=Liebel U, Kindler B, Pepperkok R |title=Bioinformatic &quot;Harvester&quot;: a search engine for genome-wide human, mouse, and rat protein resources |journal=Meth. Enzymol. |volume=404 |issue= |pages=19–26 |year=2005 |pmid=16413254 |doi=10.1016/S0076-6879(05)04003-6 |url=http://linkinghub.elsevier.com/retrieve/pii/S0076-6879(05)04003-6}}

== Notes and references ==
&lt;references/&gt;

== External links ==
* http://harvester.kit.edu Bioinformatic Harvester V at KIT [[Karlsruhe Institute of Technology]]
* [http://harvester42.fzk.de Harvester42] at KIT - integrating 50 general search engines

[[Category:Bioinformatics software]]
[[Category:Biological databases]]
[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Biology websites]]</text>
      <sha1>9im0pzkbw2b3l31q1gu7dne3cnurxas</sha1>
    </revision>
  </page>
  <page>
    <title>Harshness</title>
    <ns>0</ns>
    <id>11178092</id>
    <revision>
      <id>544798455</id>
      <parentid>406256362</parentid>
      <timestamp>2013-03-17T00:47:02Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q2133204]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1105">{{Original research|date=September 2007}}
{{Other uses|Harsh (disambiguation)}}
'''Harshness''' (also called '''raucousness'''), in [[music information retrieval]], is a Non-Contextual Low-Level Audio Descriptors (NLDs) that represents one dimension of the multi-dimensional [[psychoacoustic]] feature called as musical [[timbre]].

Classical timbre’ NLDs are [[surface roughness|roughness]], [[spectral centroid]], and [[spectral flux]]. While harmonicity and inharmonicity can also be considered NLDs, harshness differs from them, as well as from roughness, once it reckons for a distinguished perceptual audio feature expressed by the summary spectral periodicity. This feature is especially clear in single-[[Pitch (music)|pitch]], single-[[note]], musical audio, where the timbre of two different musical instruments can greatly differ in levels of harshness (e.g., the difference in harshness between a flute and a saxophone is evident). As it is supposed to be, harshness is independent of all others NLDs.

[[Category:Musicology]]
[[Category:Music technology]]
[[Category:Information retrieval]]</text>
      <sha1>30un9j61x88go4hg1b75koakrzpgq9r</sha1>
    </revision>
  </page>
  <page>
    <title>ChemRefer</title>
    <ns>0</ns>
    <id>11242818</id>
    <revision>
      <id>643004850</id>
      <parentid>558900067</parentid>
      <timestamp>2015-01-18T04:43:32Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>±[[Category:Internet search engines]]→[[Category:Scholarly search services]]; ±[[Category:Open access (publishing)]]→[[Category:Open access projects]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2964">{{Orphan|date=February 2009}}
{{Infobox website
| name = Chemrefer
| logo = [[Image:Chemrefer.png]]
| screenshot = 
| caption = 
| url = http://www.chemrefer.com
| commercial = Yes
| type = [[Search engine]]
| language = English
| registration = Not Applicable
| owner = ChemRefer Limited
| author = William James Griffiths
| launch date = 2006
| current status = Offline
| revenue = 
}}
'''ChemRefer''' is a service that allows searching of freely-available and full-text chemical and pharmaceutical literature that is published by authoritative sources.&lt;ref&gt;{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}&lt;/ref&gt;

Features include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.

ChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.

==See also==
*[[Google Scholar]]
*[[Windows Live Academic]]
*[[BASE (search engine)|BASE]]
*[[PubMed]]

==References==
{{reflist}}

==External links==
===Recommendations &amp; reviews===
*[http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an &quot;Internet Site of the Week&quot;] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]
*[http://infoweb.nrl.navy.mil/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]
*[http://www.mta.ca/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]
*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine
*[http://recherche-technologie.wallonie.be/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium
*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki

===Background===
*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine
*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College

[[Category:Scholarly search services]]
[[Category:Chemistry literature]]
[[Category:Information retrieval]]
[[Category:Open access projects]]

{{searchengine-website-stub}}</text>
      <sha1>979pw9eh56m76k7492numn3wslnuybo</sha1>
    </revision>
  </page>
  <page>
    <title>BASE (search engine)</title>
    <ns>0</ns>
    <id>11253805</id>
    <revision>
      <id>641299110</id>
      <parentid>637082069</parentid>
      <timestamp>2015-01-06T21:05:00Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes / Tagging using [[Project:AWB|AWB]] (10691)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1705">{{multiple issues|
{{notability|Web|date=February 2012}}
{{refimprove|date=June 2009}}
{{primary sources|date=February 2012}}
{{one source|date=February 2012}}
{{no footnotes|date=February 2012}}
}}

'''BASE''' ('''Bielefeld Academic Search Engine''') is a multi-disciplinary [[search engine]] to scholarly internet resources, created by [[Bielefeld University]] Library in [[Bielefeld]], [[Germany]]. It is based on search technology provided by [[Fast Search &amp; Transfer]] (FAST), a [[Norway|Norwegian]] company. It [[Web harvesting|harvests]] OAI metadata from scientific [[Digital repository|digital repositories]] that implement the [[Open Archives Initiative Protocol for Metadata Harvesting]] (OAI-PMH), and are [[Index (search engine)|indexed]] using FAST's software. In addition to OAI [[metadata]], the library indexes selected web sites and local data collections, all of which can be searched via a single search interface.

It allows those who use the search engine to search metadata, when available, as well as conducting [[full text search]]es. It contrasts with commercial search engines in multiple ways, including in the types and kinds of resources it searches and the information it offers about the results it finds. Where available, [[Bibliographic database|bibliographic data]] is provided, and the results may be sorted by multiple fields, such as by author or year of publication.

== See also ==
* [[List of academic databases and search engines]]

==External links==
* [http://www.base-search.net/ BASE search]

[[Category:Internet search engines]]
[[Category:Information retrieval]]
[[Category:Open access (publishing)]]
[[Category:Bibliographic databases]]


{{software-stub}}</text>
      <sha1>a70e6oj6gtgkk51pb9u1vke2xt8vxc3</sha1>
    </revision>
  </page>
  <page>
    <title>Mooers' law</title>
    <ns>0</ns>
    <id>11373842</id>
    <revision>
      <id>594708398</id>
      <parentid>544808884</parentid>
      <timestamp>2014-02-09T19:11:04Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* References */Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated date parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3893">{{For|the observation regarding integrated circuits|Moore's law}}
{{Refimprove|date=September 2011}}

'''Mooers' law''' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.

{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]&lt;ref name=&quot;morville&quot;&gt;{{cite book|url=http://books.google.com/books?id=xJNLJXXbhusC&amp;printsec=frontcover&amp;dq=isbn:9780596007652&amp;hl=en&amp;sa=X&amp;ei=qvWhT5DfHITs2QX1rNzPCA&amp;ved=0CDAQ6AEwAA#v=onepage&amp;q=mooers'%20law&amp;f=false |title= Ambient findability |series= O'Reilly Series. Marketing/Technology &amp; Society |author= Peter Morville |edition= illustrated |publisher= O'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}&lt;/ref&gt;}}

==Original interpretation==

Mooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the users personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.&lt;ref&gt;{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}&lt;/ref&gt;

==Out-of-context interpretation==

The more commonly used interpretation of Mooers' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker 'gives up', and the Law is often paraphrased to increase the focus on the retrieval system:

{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}
{{quote|Mooers' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit &lt;ref name=&quot;morville&quot;/&gt;}}

In this interpretation, &quot;painful and troublesome&quot; comes from ''using'' the retrieval system.

==References==
{{reflist}}

*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=pp 607–609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}

==External links==
* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.
* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.
* [http://www.phillyimc.org/en/gasoline-7-17-moors-law-kent-moors-authority Another empirical observation with a similar-sounding name is Moors' law], named for Kent Moors of Duquesne University, which states crude oil prices double every five years. 
[[Category:Empirical laws]]
[[Category:Library science]]
[[Category:Information retrieval]]</text>
      <sha1>ks21zgtpkga08lik54vzsime1khbtf4</sha1>
    </revision>
  </page>
  <page>
    <title>Web search query</title>
    <ns>0</ns>
    <id>11525372</id>
    <revision>
      <id>615002181</id>
      <parentid>612484495</parentid>
      <timestamp>2014-06-30T11:12:59Z</timestamp>
      <contributor>
        <ip>2.36.163.137</ip>
      </contributor>
      <comment>According to its dedicated Wikipedia page, &quot;Google started using Hummingbird about 30 August 2013&quot;, not in 2014.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8763">A '''web search query''' is a query that a user enters into a web [[search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as &quot;and&quot;/&quot;or&quot; with &quot;-&quot; to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].

== Types ==
There are three broad categories that cover most web search queries: informational, navigational, and transactional. These are often called &quot;do, know, go.&quot;&lt;ref&gt;{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}&lt;/ref&gt;

* '''Informational queries''' – Queries that cover a broad topic (e.g., ''colorado'' or ''trucks'') for which there may be thousands of relevant results.

* '''Navigational queries''' – Queries that seek a single website or web page of a single entity (e.g., ''youtube'' or ''delta air lines'').

* '''Transactional queries''' – Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.

Search engines often support a fourth type of query that is used far less frequently:

* '''Connectivity queries''' – Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).&lt;ref&gt;{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}&lt;/ref&gt;

== Characteristics ==

Most commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.&lt;ref&gt;Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]&lt;/ref&gt; Nevertheless, a study in 2001&lt;ref&gt;{{cite journal|author = Amanda Spink, Dietmar Wolfram, Major B. J. Jansen, Tefko Saracevic | year = 2001 | title = Searching the web: The public and their queries | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226–234 | doi = 10.1002/1097-4571(2000)9999:9999&lt;::AID-ASI1591&gt;3.3.CO;2-I }}&lt;/ref&gt; analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:

* The average length of a search query was 2.4 terms. 
* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. 
* Close to half of the users examined only the first one or two pages of results (10 results per page).
* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).
* The top four most frequently used terms were , '' (empty search), and, of, ''and'' sex.

A study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).&lt;ref&gt;{{cite conference | author = Mark Sanderson and Janet Kohler | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR '04) | url =http://supremacyseo.com/analyzing-geographic-queries }}&lt;/ref&gt;

A 2005 study of Yahoo's query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.&lt;ref&gt;{{cite conference | author = Jaime Teevan, Eytan Adar, Rosie Jones, Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo's query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR '06) | pages = 703–704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}&lt;/ref&gt; This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries &lt;ref&gt;http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx&lt;/ref&gt;

In addition, much research has shown that query term frequency distributions conform to the [[power law]], or ''long tail'' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. &gt; 100 million queries) are used most often, while the remaining terms are used less often individually.&lt;ref name=&quot;baezayates1&quot;&gt;{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 7–22 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}&lt;/ref&gt; This example of the [[Pareto principle]] (or ''80–20 rule'') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching.

But in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.&lt;ref&gt;{{cite journal | author = Mona Taghavi, Ahmed Patel, Nikita Schmidt, Christopher Wills, Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards &amp; Interfaces | pages = 162–170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}&lt;/ref&gt; Google has implemented the [[Google_Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (ie &quot;where is the nearest coffee shop?&quot;).&lt;ref&gt;{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google “Hummingbird” Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}&lt;/ref&gt; 
For longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.&lt;ref&gt;{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}&lt;/ref&gt; For multi-sentence queries where keywords statistics and [[Tf–idf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.&lt;ref&gt;{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets
for Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037
}}&lt;/ref&gt;

== Structured queries ==
With search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or ''facets'' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as &lt;code&gt;vehicles OR cars OR automobiles&lt;/code&gt;. A ''faceted query'' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as &lt;code&gt;(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)&lt;/code&gt; is likely to find documents about electronic voting even if they omit one of the words &quot;electronic&quot; and &quot;voting&quot;, or even both.&lt;ref&gt;{{Cite web
|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf
|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness
|author=Vojkan Mihajlović, Djoerd Hiemstra, Henk Ernst Blok, Peter M.G. Apers
|postscript=&lt;!--None--&gt;}}&lt;/ref&gt;

== See also ==
* [[Information retrieval]]
* [[Web search engine]]
* [[Web query classification]]
* [[Taxonomy for search engines]]

== References ==
{{reflist|2}}

{{Internet search}}

[[Category:Information retrieval]]
[[Category:Internet search]]</text>
      <sha1>jsge1w4qi06krlznau382uq6npd44s4</sha1>
    </revision>
  </page>
  <page>
    <title>Latent semantic mapping</title>
    <ns>0</ns>
    <id>11989095</id>
    <revision>
      <id>570019893</id>
      <parentid>563888431</parentid>
      <timestamp>2013-08-24T17:27:17Z</timestamp>
      <contributor>
        <username>CaptSolo</username>
        <id>7728661</id>
      </contributor>
      <comment>/* References */ fix a URL that has a permanent HTTP 404 Not Found error</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1919">'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.

LSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.

[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.&lt;ref&gt;[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

== See also ==
* [[Latent semantic analysis]]

== Notes ==
{{reflist}}

== References ==
* {{cite journal
 | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf
 | title=Latent semantic mapping [information retrieval]
 | author=Bellegarda, J.R.
 | date=2005
}}
* {{cite conference
 | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp
 | title=Latent semantic mapping: Principles and applications
 | author=J. Bellegarda
 | booktitle=ICASSP 2006
 | date=2006
}}

[[Category:Information retrieval]]
[[Category:Natural language processing]]


{{semantics-stub}}
{{compu-stub}}</text>
      <sha1>p3261pbmuk6jcfidrao2cvr4pzl2sot</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data management</title>
    <ns>14</ns>
    <id>762162</id>
    <revision>
      <id>543805931</id>
      <parentid>532384592</parentid>
      <timestamp>2013-03-13T09:43:35Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 28 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7011992]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="336">{{catdiffuse}}

'''[[Data management]]''' comprises all the disciplines related to managing data as a valuable resource.


{{Commons cat|Data management}}

[[Category:Computer data|Management]]
[[Category:Data|Management]]
[[Category:Project management]]
[[Category:Information retrieval]]
[[Category:Information technology management]]</text>
      <sha1>pycjk14lftokn0tnz8t4cn8j3vpmuun</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Electronic documents</title>
    <ns>14</ns>
    <id>699814</id>
    <revision>
      <id>546483581</id>
      <parentid>525385068</parentid>
      <timestamp>2013-03-23T06:27:20Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 14 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7210821]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="119">[[Category:Documents]]
[[Category:Digital media]]
[[Category:Information retrieval]]
[[Category:Electronic publishing]]</text>
      <sha1>t1806qygmvdgbyf72c9lq6hqh776vz9</sha1>
    </revision>
  </page>
  <page>
    <title>Poliqarp</title>
    <ns>0</ns>
    <id>2398780</id>
    <revision>
      <id>607634522</id>
      <parentid>220787637</parentid>
      <timestamp>2014-05-08T14:30:30Z</timestamp>
      <contributor>
        <username>Bansp</username>
        <id>686353</id>
      </contributor>
      <comment>correction of links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="944">'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].

==Features==

* Custom [[query language]].
* Two-level [[regular expressions]]:
** operating at the level of characters in words
** operating at the level of words in statements/paragraphs
* Good performance
* Compact corpus representation (compared to similar projects)
* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]
* Lack of portability across [[endianness]] (current release works only on little endian devices)

==External links==

* [http://www.korpus.pl/index.php?lang=en&amp;page=welcome Polish corpus website (in English)]
* [http://poliqarp.sourceforge.net/ Project website on SourceForge]
* [http://poliqarp.suxx.pl/ Search plugin for Firefox]
&lt;br /&gt;
[[Category:Information retrieval]]</text>
      <sha1>kec8wazbkm6g7wi682vilqwt2v4qshf</sha1>
    </revision>
  </page>
  <page>
    <title>Information retrieval applications</title>
    <ns>0</ns>
    <id>13324645</id>
    <revision>
      <id>544967141</id>
      <parentid>442758494</parentid>
      <timestamp>2013-03-17T14:47:06Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6031188]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1430">Areas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):

==General applications of information retrieval==
* [[Digital libraries]]
*  [[Information filtering]]
** [[Recommender systems]]
*  Media search
** Blog search
** [[Image retrieval]]
** [[Music information retrieval|Music retrieval]]
** News search
** Speech retrieval
** Video retrieval
* [[Search engines]]
** [[Desktop search]]
** [[Enterprise search]]
** [[Federated search]]
** [[Mobile search]]
** [[Social search]]
** [[Web search engine|Web search]]

==Domain specific applications of information retrieval==
* Expert search finding
* Genomic information retrieval
* [[Geographic information retrieval]]
*  Information retrieval for chemical structures
* Information retrieval in [[software engineering]]
* [[Legal information retrieval]]
* [[Vertical search]]

==Other retrieval methods==
Methods/Techniques in which [[information retrieval]] techniques are employed include:
* [[Adversarial information retrieval]]
* [[Automatic summarization]]
**[[Multi-document summarization]]
* [[Compound term processing]]
* [[Cross-language information retrieval|Cross-lingual retrieval]]
* [[Document classification]]
* [[Spam filtering]]
* [[Question answering]]

== See also ==
* [[Information retrieval]]

{{DEFAULTSORT:Information Retrieval Applications}}
[[Category:Information retrieval|*]]</text>
      <sha1>aglveb2fhv0fwt0lr7dpxfsh5w8x957</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic technology</title>
    <ns>0</ns>
    <id>4416107</id>
    <revision>
      <id>644574947</id>
      <parentid>644574575</parentid>
      <timestamp>2015-01-28T16:56:08Z</timestamp>
      <contributor>
        <username>Van Parunak</username>
        <id>3834666</id>
      </contributor>
      <minor/>
      <comment>Added link to Sheth and Ramakrishnan</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3831">{{no footnotes|date=March 2013}}
[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]
In [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. 

This enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.

With traditional [[information technology]], on the other hand, meanings and relationships must be predefined and “hard wired” into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.

Off-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.

Semantic technologies are “meaning-centered.” They include tools for:

* autorecognition of topics and concepts, 
* information and meaning extraction, and
* categorization. 

Given a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.

Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.

== See also ==
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Metadata]]
* [[Ontology (computer science)]]
* [[Semantic targeting]]
* [[Semantic web]]

==References==

* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004
* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 — Proc. of the 12th international conference on World Wide Web'', pp 700–709. [[ACM Press]], 2003.
* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.
* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.
* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.
* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, 
* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September
2004.
* P. Hitzler, M. Krötzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&amp;Hall/CRC, 2009, ISBN 978-1-4200-9050-5

== External links ==
* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]
* [http://www.semanticarts.com Semantic Technology and the Enterprise]

[[Category:Information retrieval]]
[[Category:Semantics]]</text>
      <sha1>6bgndoop9e9qa4w4a9ubs5qqw0zn4ie</sha1>
    </revision>
  </page>
  <page>
    <title>Key Word in Context</title>
    <ns>0</ns>
    <id>41908</id>
    <revision>
      <id>642481836</id>
      <parentid>642481765</parentid>
      <timestamp>2015-01-14T17:36:00Z</timestamp>
      <contributor>
        <username>ChPietsch</username>
        <id>17583657</id>
      </contributor>
      <minor/>
      <comment>bold</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4116">'''KWIC''' is an acronym for '''Key Word In Context''', the most common format for [[concordance (publishing)|concordance]] lines. The term KWIC was first coined by [[Hans Peter Luhn]].&lt;ref&gt;Manning, C. D., Schütze, H.: &quot;Foundations of Statistical Natural Language Processing&quot;, p.35. The MIT Press, 1999&lt;/ref&gt; The system was based on a concept called ''keyword in titles'' which was first proposed for Manchester libraries in 1864 by [[Andrea Crestadoro]].&lt;ref name=&quot;index&quot;&gt;{{cite book|title=Advanced Indexing and Abstracting Practices|url=http://books.google.co.uk/books?id=nIUkl7bLzYUC&amp;pg=PA41&amp;dq=Andrea+Crestadoro#v=onepage&amp;q=Andrea%20Crestadoro&amp;f=false}}&lt;/ref&gt;

A '''KWIC''' index is formed by sorting and aligning the words within an article title to allow each word (except the [[stop words]]) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized [[full text search]] became common.

For example, a search query including all of the words in the title statement of this article (&quot;KWIC is an acronym for Key Word In Context, the most common format for concordance lines&quot;) and the [[Wikipedia:Slogans|Wikipedia slogan]] in English (&quot;the free encyclopedia&quot;), searched against this very webpages, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).

{| nowrap
|-
|align=right|KWIC is an
|'''acronym''' for Key Word In Context, ...
|page 1
|-
|align=right|... Key Word In Context, the most 
|'''common''' format for concordance lines.
|page 1
|-
|align=right|... the most common format for 
|'''concordance''' lines.
|page 1
|-
|align=right|... is an acronym for Key Word In 
|'''Context''', the most common format ...
|page 1
|-
|align=right|Wikipedia, The Free 
|'''Encyclopedia'''
|page 0
|-
|align=right|... In Context, the most common 
|'''format''' for concordance lines.
|page 1
|-
|align=right|Wikipedia, The 
|'''Free''' Encyclopedia
|page 0
|-
|align=right|KWIC is an acronym for 
|'''Key''' Word In Context, the most ...
|page 1
|-
|&amp;nbsp;
|'''KWIC''' is an acronym for Key Word ...
|page 1
|-
|align=right|... common format for concordance 
|'''lines'''.
|page 1
|-
|align=right|... for Key Word In Context, the 
|'''most''' common format for concordance ...
|page 1
|-
|&amp;nbsp;
|'''Wikipedia''', The Free Encyclopedia
|page 0
|-
|align=right|KWIC is an acronym for Key
|'''Word''' In Context, the most common ...
|page 1
|}

A KWIC index is a special case of a '''permuted index'''. This term refers to the fact that it indexes all [[cyclic permutation]]s of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of [[Manual page (Unix)|manual pages]], often ended with a '''permuted index''' section, allowing the reader to easily find a section by any word from its heading. This practice, also known as '''KWOC''' (“'''Key Word Out of Context'''”), is no longer common.

==References in Literature==

''Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all.''

* [[David Parnas|David L. Parnas]] uses a KWIC Index as an example on how to perform modular design in his paper [http://portal.acm.org/citation.cfm?id=361623&amp;coll=ACM&amp;dl=ACM&amp;CFID=9516243&amp;CFTOKEN=98251202 ''On the Criteria To Be Used in Decomposing Systems into Modules''], available as an [http://www.acm.org/classics/may96/ ACM Classic Paper]
* Christopher D. Manning and Hinrich Schütze describe a KWIC index and computer concordancing in section 1.4.5 of their book ''Foundations of Statistical Natural Language Processing''

==References==
{{reflist|2}}

==See also==
* &lt;tt&gt;[[Ptx (Unix)|ptx]]&lt;/tt&gt;, a Unix command-line utility producing a [[permuted index]]
*[[Concordancer]]
*[[Concordance (publishing)]]
*[[Burrows–Wheeler transform]]
*[[Hans Peter Luhn]]
*[[Suffix tree]]

[[Category:Indexing]]
[[Category:Information retrieval]]
[[Category:Reference]]
[[Category:Searching]]</text>
      <sha1>a2d6ff596y863vu3s1gucb7a5g81vym</sha1>
    </revision>
  </page>
  <page>
    <title>Audio mining</title>
    <ns>0</ns>
    <id>14004969</id>
    <revision>
      <id>545015261</id>
      <parentid>486139588</parentid>
      <timestamp>2013-03-17T18:15:39Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q3079876]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1779">{{unreferenced|date=January 2012}}
'''Audio mining''' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time &quot;word spotting&quot; system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.

The results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.

Audio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. 

Musical audio mining (also known as [[Music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.

==See also==
* [[Speech Analytics]]


[[Category:Speech recognition]]
[[Category:Information retrieval]]
[[Category:Computational linguistics]]</text>
      <sha1>tevyv67wx8frtd04ie1xkwq1vjwra8s</sha1>
    </revision>
  </page>
  <page>
    <title>Dynatext</title>
    <ns>0</ns>
    <id>14460441</id>
    <revision>
      <id>454651053</id>
      <parentid>381939029</parentid>
      <timestamp>2011-10-09T03:08:48Z</timestamp>
      <contributor>
        <username>Stuartyeates</username>
        <id>154991</id>
      </contributor>
      <comment>Added {{[[Template:primary sources|primary sources]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1490">{{primary sources|date=October 2011}}
'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.

DynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.

DynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996.

DynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as the origin of the notion of [[well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].

[[Inso]] corporation went out of business in 2002. 

==References==
*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]]

[[Category:Information retrieval]]

{{markup-languages-stub}}</text>
      <sha1>l690sc3shh0r7gzusqfzk9ujfy3ovtp</sha1>
    </revision>
  </page>
  <page>
    <title>Poison words</title>
    <ns>0</ns>
    <id>14484974</id>
    <revision>
      <id>545043918</id>
      <parentid>517877550</parentid>
      <timestamp>2013-03-17T20:20:53Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q254435]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1163">{{Missing information|Examples of poison words|date=September 2008}}
{{Unreferenced|date=December 2007}}

'''Poison words''', or '''forbidden words''', is the name given to words or phrases that trigger suspicion, mistrust and loss of respect, or are of inappropriate character for a given web site in its consideration for a [[search engine]].

There is no definite list of poison words which all natural language processing tools incorporate. 

This is different from harmless but useless words that are called [[Stop words]].

Adult (obscene) words can put a web page in an adult category where it is filtered out by various filters at search engines, so this is one set of poison words. But some consider any words that lower your ranking in a search engine as poison words. Some people consider any words that encourage ads to pervade a whole site and displace much higher earning ads as poison words.

== See also ==

* [[Bayesian poisoning]]
* [[Natural language processing]]
* [[Text mining]]
* [[Index (search engine)|Search engine indexing]]

== External links ==


{{SearchEngineOptimization}}

[[Category:Information retrieval]]
[[Category:Searching]]</text>
      <sha1>2b7yas439d5jfac8m39ex4748w70non</sha1>
    </revision>
  </page>
  <page>
    <title>Precision and recall</title>
    <ns>0</ns>
    <id>14343887</id>
    <revision>
      <id>635850583</id>
      <parentid>635019994</parentid>
      <timestamp>2014-11-29T03:02:44Z</timestamp>
      <contributor>
        <ip>174.52.65.246</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20489">[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]
In [[pattern recognition]] and [[information retrieval]] with [[binary classification]], '''precision''' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while '''recall''' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]]. Suppose a program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9.  When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.

In [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &amp;minus; 4 = 3 type I errors and 9 &amp;minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or ''quality'', whereas recall is a measure of completeness or ''quantity''.

In simple terms, high '''precision''' means that an algorithm returned substantially more relevant results than irrelevant, while high '''recall''' means that an algorithm returned most of the relevant results.

==Introduction==
As an example, in an [[information retrieval]] scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, &quot;relevant&quot; and &quot;not relevant&quot;.  In this case, the &quot;relevant&quot; documents are simply those that belong to the &quot;relevant&quot; category.  Recall is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of existing relevant documents'', while precision is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of documents retrieved'' by that search.

In a [[classification (machine learning)|classification]] task, the precision for a class is the ''number of '''true positives''''' (i.e. the ''number of items correctly labeled as belonging to the positive class'') ''divided by the total number of elements labeled as belonging to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false positives]]''', which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the ''number of true positives'' ''divided by the total number of elements that actually belong to the positive class'' (i.e. the sum of true positives and '''[[Type I and type II errors|false negatives]]''', which are items which were not labeled as belonging to the positive class but should have been).

In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).

In a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).

Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an obvious example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).

Usually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. ''precision at a recall level of 0.75'') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s Informedness (DeltaP') and Markedness (DeltaP).&lt;ref name=&quot;Powers2007&quot;&gt;{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Factor  to ROC, Informedness, Markedness &amp; Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=http://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}&lt;/ref&gt; [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).&lt;ref name=&quot;Powers2007&quot;/&gt; Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.&lt;ref name=&quot;Powers2007&quot;/&gt;  The first problem is 'solved' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is 'solved' by discounting the chance component and renormalizing to [[Cohen's kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, Informedness and Markedness are Kappa-like renormalizations of Recall and Precision,&lt;ref&gt;{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}&lt;/ref&gt; and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.

== Definition (information retrieval context) ==

In [[information retrieval]] contexts, precision and recall are defined in terms of a set of '''retrieved documents''' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of '''relevant documents''' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]].

===[[Positive predictive value | Precision]]===

In the field of [[information retrieval]], '''precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the find:

:&lt;math&gt; \text{precision}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{retrieved documents}\}|} &lt;/math&gt;

Precision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called '''precision at n''' or '''P@n'''.

For example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.

Precision is also used with [[recall (information retrieval)|recall]], the percent of ''all'' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.

Note that the meaning and usage of &quot;precision&quot; in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.

===Recall===

Recall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.

:&lt;math&gt; \text{recall}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{relevant documents}\}|} &lt;/math&gt;

For example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned

In binary classification, recall is called [[Sensitivity_and_specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

== Definition (classification context) ==
{| class=&quot;wikitable&quot; align=&quot;right&quot; width=35% style=&quot;font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;&quot;
|+ Terminology and derivations&lt;br 
/&gt;from a confusion matrix
|- valign=top
|
; true positive (TP)
:eqv. with hit
; true negative (TN)
:eqv. with correct rejection
; false positive (FP)
:eqv. with [[false alarm]], [[Type I error]]
; false negative (FN)
:eqv. with miss, [[Type II error]]
--------------------------------------------------------
; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)
:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]
:&lt;math&gt;\mathit{TPR} = \mathit{TP} / P = \mathit{TP} / (\mathit{TP}+\mathit{FN})&lt;/math&gt;
; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate
:&lt;math&gt;\mathit{SPC} = \mathit{TN} / N = \mathit{TN} / (\mathit{FP} + \mathit{TN}) &lt;/math&gt;
; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)
:&lt;math&gt;\mathit{PPV} = \mathit{TP} / (\mathit{TP} + \mathit{FP})&lt;/math&gt;
; [[negative predictive value]] (NPV)
:&lt;math&gt;\mathit{NPV} = \mathit{TN} / (\mathit{TN} + \mathit{FN})&lt;/math&gt;
; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)
:&lt;math&gt;\mathit{FPR} = \mathit{FP} / N = \mathit{FP} / (\mathit{FP} + \mathit{TN})&lt;/math&gt;
; [[false discovery rate]] (FDR)
:&lt;math&gt;\mathit{FDR} = \mathit{FP} / (\mathit{FP} + \mathit{TP}) = 1 - \mathit{PPV} &lt;/math&gt;
; [[false negative rate]] (FNR)
:&lt;math&gt;\mathit{FNR} = \mathit{FN} / (\mathit{FN} + \mathit{TP}) = 1 - \mathit{TPR} &lt;/math&gt;
------------------------------------------------
; [[accuracy]] (ACC)
:&lt;math&gt;\mathit{ACC} = (\mathit{TP} + \mathit{TN}) / (P + N)&lt;/math&gt;
;[[F1 score]]
: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]
:&lt;math&gt;\mathit{F1} = 2 \mathit{TP} / (2 \mathit{TP} + \mathit{FP} + \mathit{FN})&lt;/math&gt;
; [[Matthews correlation coefficient]] (MCC)
:&lt;math&gt; \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
&lt;/math&gt;
;
&lt;span style=&quot;font-size:90%;&quot;&gt;''Source: Fawcett (2006).''&lt;ref name=Fawcelt2006&gt;{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}&lt;/ref&gt;&lt;/span&gt;
|}

For classification tasks, the terms '''true positives''', '''true negatives''', '''false positives''', and '''false negatives''' (see also [[Type I and type II errors]]) compare the results of the classifier under test with trusted external judgments.  The terms ''positive'' and ''negative'' refer to the classifier's prediction (sometimes known as the ''expectation''), and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation''). 

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

&lt;!--
{| border=&quot;0&quot; align=&quot;center&quot; style=&quot;text-align: center; background: #FFFFFF;&quot;
|+
!
! colspan=&quot;2&quot; style=&quot;background: #ddffdd;&quot;|actual class &lt;br/&gt; (observation)
|-
!
|-----
|+
! rowspan=&quot;2&quot; style=&quot;background: #ffdddd;&quot;|predicted class &lt;br/&gt; (expectation)
| '''tp''' &lt;br&gt; (true positive) &lt;br/&gt; Correct result
| '''fp''' &lt;br&gt; (false positive) &lt;br/&gt; Unexpected result
|-bgcolor=&quot;#EFEFEF&quot;
| '''fn''' &lt;br&gt; (false negative) &lt;br/&gt; Missing result
| '''tn''' &lt;br&gt; (true negative) &lt;br/&gt; Correct absence of result
|+
|}

--&gt;



Precision and recall are then defined as:&lt;ref name=&quot;OlsonDelen&quot;&gt;Olson, David L.; and Delen, Dursun (2008); ''Advanced Data Mining Techniques'', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1&lt;/ref&gt;

: &lt;math&gt;\text{Precision}=\frac{tp}{tp+fp} \, &lt;/math&gt;

: &lt;math&gt;\text{Recall}=\frac{tp}{tp+fn} \, &lt;/math&gt;

Recall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy_and_precision#In_binary_classification|accuracy]].&lt;ref name=&quot;OlsonDelen&quot; /&gt; True negative rate is also called [[Specificity_(tests)#Specificity|specificity]].

: &lt;math&gt;\text{True negative rate}=\frac{tn}{tn+fp} \, &lt;/math&gt;

: &lt;math&gt;\text{Accuracy}=\frac{tp+tn}{tp+tn+fp+fn} \, &lt;/math&gt;

== Probabilistic interpretation ==

It is possible to interpret precision and recall not as ratios but as probabilities:

* '''Precision''' is the probability that a (randomly selected) retrieved document is relevant.

* '''Recall''' is the probability that a (randomly selected) relevant document is retrieved in a search.

Note that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by '''randomly selected retrieved document''', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected. 

Note that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation). 

Another interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.

== F-measure ==
{{main|F1 score}}
A measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:

: &lt;math&gt;F = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{ \mathrm{precision} + \mathrm{recall}}&lt;/math&gt;

There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric. &lt;ref&gt;{{cite journal|last=POWERS|first=D.M.W.|title=EVALUATION: FROM PRECISION, RECALL AND F-MEASURE TO ROC, INFORMEDNESS, MARKEDNESS &amp; CORRELATION|journal=Journal of Machine Learning Technologies|date=February 27, 2011|volume=2|issue=1|pages=37-63|url=http://www.bioinfo.in/contents.php?id=51}}&lt;/ref&gt; This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

It is a special case of the general &lt;math&gt;F_\beta&lt;/math&gt; measure (for non-negative real values of&amp;nbsp;&lt;math&gt;\beta&lt;/math&gt;):

:&lt;math&gt;F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall} }{ \beta^2 \cdot \mathrm{precision} + \mathrm{recall}}&lt;/math&gt;

Two other commonly used &lt;math&gt;F&lt;/math&gt; measures are the &lt;math&gt;F_2&lt;/math&gt; measure, which weights recall higher than precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which puts more emphasis on precision than recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; &quot;measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision&quot;.  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;.  Their relationship is &lt;math&gt;F_\beta = 1 - E&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;.

==Limitations as goals==
There are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).&lt;ref&gt;Zygmunt Zając. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/&lt;/ref&gt; 

For [[web document]] retrieval, if the user's objectives are not clear, the  precision and recall can't be optimized. As summarized by Lopresti,&lt;ref&gt;Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm ''WDA 2001 panel'']&lt;/ref&gt;
:''&quot;[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).''
:* ''Search results don't have to be very good.''
:* ''Recall?    Not important (as long as you get at least some good hits).''
:* ''Precision? Not important (as long as at least some of the hits on the first page you return are good).&quot;''

==See also==
* [[Binary classification]]
* [[Information retrieval]]
* [[Receiver operating characteristic]]
* [[Relevance]]
* [[Sensitivity and specificity]]
* [[Type I and type II errors]], where ''false positives'' and ''false negatives'' are defined
* [[Uncertainty coefficient]], aka Proficiency

== Sources ==
&lt;references&gt;
* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). ''Modern Information Retrieval''. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X
* Hjørland, Birger (2010); ''The foundation of the concept of relevance'', Journal of the American Society for Information Science and Technology, 61(2), 217-237
* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 ''Performance measures for information extraction''], in ''Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999''
* van Rijsbergen, Cornelis Joost &quot;Keith&quot; (1979); ''Information Retrieval'', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4
&lt;/references&gt;

== External links ==
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval – C. J. van Rijsbergen 1979]
* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]

[[Category:Information retrieval]]
[[Category:Information science]]
[[Category:Bioinformatics]]
[[Category:Summary statistics for contingency tables]]

[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]</text>
      <sha1>r3yq13psved056zovpp1o8kb7b26jhz</sha1>
    </revision>
  </page>
  <page>
    <title>ZyLAB Technologies</title>
    <ns>0</ns>
    <id>2744940</id>
    <revision>
      <id>647212407</id>
      <parentid>641270517</parentid>
      <timestamp>2015-02-15T07:32:01Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>clean up using [[Project:AWB|AWB]] (10823)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12769">{{advert|date=April 2012}}
{{Infobox company |
  name   = ZyLAB |
  logo   = &lt;!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --&gt; |
  slogan = &quot;eDiscovery &amp; Information Risk Management&quot; |
  type   = Private |
  foundation     = 1983 |
  location       = [[McLean, Virginia]]&lt;br&gt;[[Amsterdam]] |
  key_people     = [[Pieter Varkevisser]], president &amp; CEO&lt;br&gt;[[Dr. Johannes C. Scholtes]], chairman &amp; chief strategy officer | Mary Mack, Enterprise Technology Counsel
  num_employees  = 140 |
  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |
  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email &amp; SharePoint archiving, text-analytics, visualization, contract management, and workflow. |

  homepage       = [http://www.zylab.com/ www.zylab.com]
}}

'''ZyLAB''' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLAB’s most important products are ZyLAB eDiscovery &amp; Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.

== History ==
In 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.

In 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]’s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.

In 1998, the company developed support to full-text search email, including attachments.

In 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.

In 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].

2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.

Platforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.

2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.

==Customers==
Initial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).

Other well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson's defense team]], war crime tribunals such as the [[trial of Slobodan Milošević]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLAB’s software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].

Public websites also use the ZyLAB Webserver.

[[Gartner]] positioned ZyLAB in the &quot;Leaders&quot; quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.

ZyLAB’s chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.

==System overview and compatibility==
According to the company’s website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:

Systems:
*ZyLAB eDiscovery and Production
*ZyLAB Compliance and Litigation readiness
*ZyLAB Law Enforcement and Investigations
*ZyLAB Communications Intelligence
*ZyLAB Digital Print and Media Archiving
*ZyLAB Enterprise Information Management

Bundles:
*E-Mail Archiving Bundle
*Microsoft SharePoint Bundle
*Analytics Bundle
*eDiscovery EDRM Processing bundle
*DoD and Sox Compliant RMA Bundle
*TIFF Archiving and Production Bundle
*WebPublishing Bundle
*Commercial Publishing Bundle
*Business Process Automation Bundle
*Development and Integrators Bundle
*Scanning Bundle
*Digital Copier Bundle
*Professional Text Mining
*Machine translation

===Supported configurations===
*'''Server OS''': Windows 2003, Windows 2008
*'''Databases''': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL
*'''Web Servers''': IIS
*'''Client OS''': Windows XP, Windows Vista, Windows 7
*'''Clustering''': Support for Active/Passive Failover.
*'''Authentication''': Active Directory, LDAP, XML, NTFS, IBM Tripoli.
*'''Virtualization''': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.

===Languages supported===
*'''Unicode'''. Support for documents in all languages.
*'''Internationalization'''. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB's recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB's technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.

==Zy-IMAGE-nation Annual Conference==
The annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.

==See also==
* [[Electronic discovery|e-Discovery]]
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Document Imaging]]
* [[E-mail archiving|E-mail Archiving]]
* [[Knowledge Management]]
* [[Document management system|Document Management (System)]]
* [[Enterprise content management|Enterprise Content Management]]
* [[Records management|Records Management]]
* [[Contract management|Contract Management]]
* [[Workflow]]
* [[Text mining|Text Mining]]
* [[Text analytics|Text Analytics]]
* [[Machine translation|Automatic Machine Translation]]
* [[Data visualization|Data Visualization]]

==References==
{{Reflist}}
*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&amp;i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&amp;i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in ''[[InformationWeek]]''
*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]
*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the ''[[New York Times]]''
*[http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on ''Computerwoche.de'' (article in German)
*[http://www.computerwoche.de/index.cfm?pid=2123&amp;pk=1096333 Review] of ZyIMAGE's webserver on ''Computerwoche.de'' (article in German)
*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&amp;s_site=miami&amp;p_multi=MH&amp;p_theme=realcities&amp;p_action=search&amp;p_maxdocs=200&amp;p_topdoc=1&amp;p_text_direct-0=0EB367D56736E685&amp;p_field_direct-0=document_id&amp;p_perpage=10&amp;p_sort=YMD_date: Review] of ZyINDEX in the ''[[Miami Herald]]''
*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]
*[http://www.fcw.com/print/6_31/news/70014-1.html Review] of ZyIMAGE on ''Federal Computer Week (FCW.com)''
*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge,&quot; Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.
*Knibbe, &quot;ZyImage 2 boosts, OCR, batch duties,&quot; InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&amp;nbsp;20.
*Knibbe, &quot;ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software,&quot; InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&amp;nbsp;22.
*Marshall, &quot;Text retrieval alternatives: 10 more ways to pinpoint important information,&quot; Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&amp;nbsp;88–89.
*Marshall, &quot;ZyImage adds scanning access to ZyIndex,&quot; InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&amp;nbsp;73, 76, and 77.
*Marshall, &quot;ZyImage is ZyIndex plus a scan interface integrated,&quot; InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&amp;nbsp;100.
*Marshall et al., &quot;ZyIndex for Windows, Version 5.0,&quot; InfoWorld, v. 15, n. 21, May 1993, pp.&amp;nbsp;127, 129, 133 and 137.
*Simon, &quot;ZyImage: A Winning Combination of OCR And Text Indexing,&quot; PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&amp;nbsp;56.
*Rooney, &quot;Text-retrieval veterans prepare Windows attack,&quot; PC Week, v. 9, n. 24, Jun. 1992, p.&amp;nbsp;46.
*Rooney, &quot;ZyLab partners with Calera: firms roll out document-image system,&quot; PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&amp;nbsp;22.
*Torgan, &quot;ZyImage: Document Imaging and Retrieval System,&quot; PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&amp;nbsp;62.

===Gartner reports===
*Introduction to Investigative Case Management Products (18 April 2007)
*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)
*MarketScope for Contract Management, 2007 (16 July 2007)
*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)
*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)
*Magic Quadrant for Information Access Technology, 2008
*Magic Quadrant for Information Access Technology, 2009
*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)
*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)
*MarketScope for E-Discovery Product Vendors, 2008
*MarketScope for E-Discovery Product Vendors, 2009
*MarketScope for Records Management (20 May 2008)
*Hype Cycle for Content Management, 2008 (8 July 2008)
*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)
*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)
*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)

==External links==
*[http://www.zylab.com/ ZyLAB official website]
*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]
*[http://www.aiim.org/ AIIM]

[[Category:Companies established in 1983]]
[[Category:Software companies of the United States]]
[[Category:Information retrieval]]</text>
      <sha1>fpzf4le2yxc2v9smz58eaozv8h9dxv0</sha1>
    </revision>
  </page>
  <page>
    <title>Document clustering</title>
    <ns>0</ns>
    <id>14663145</id>
    <revision>
      <id>645178865</id>
      <parentid>644884015</parentid>
      <timestamp>2015-02-01T16:46:45Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>Reverted to revision 611059817 by [[Special:Contributions/Qwertyus|Qwertyus]] ([[User talk:Qwertyus|talk]]): Looks promotional. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4113">{{disputed|date=March 2014}}
{{inline|date=March 2014}}
'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.

==Overview==
Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.

The application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared offline applications.

In general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment – each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft – a document’s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters. [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)&lt;ref&gt;http://nlp.stanford.edu/IR-book/pdf/16flat.pdf&lt;/ref&gt; and [[topic model]]s.

Other algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.

Given a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.

==Clustering in search engines==
A [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by Enterprise Search engines such as [[Northern Light Group|Northern Light]] and [[Vivisimo]], consumer search engines such as [http://www.polymeta.com/ PolyMeta] and [http://www.helioid.com Helioid], or open source software such as [[Carrot2]].

Examples:

* Clustering divides the results of a search for &quot;cell&quot; into groups like &quot;biology,&quot; &quot;battery,&quot; and &quot;prison.&quot;

* [http://FirstGov.gov FirstGov.gov], the official Web portal for the U.S. government, uses document clustering to automatically organize its search results into categories.  For example, if a user submits “immigration”, next to their list of results they will see categories for “Immigration Reform”, “Citizenship and Immigration Services”, “Employment”, “Department of Homeland Security”, and more.

== References ==
{{reflist}}
Publications:
* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]
* Claudio Carpineto, Stanislaw Osiński, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys (CSUR), Volume 41, Issue 3 (July 2009), Article No. 17, ISSN:0360-0300 
* http://semanticsearchart.com/researchBest.html - comparison of several popular clustering algorithms, data and software to reproduce the result.
* Tanmay Basu, C.A. Murthy , CUES: A New Hierarchical Approach for Document Clustering, 2013 [http://jprr.org  JPRR]

==See Also==
*[[Cluster Analysis]]
*[[Fuzzy clustering]]
[[Category:Information retrieval]]</text>
      <sha1>2boxo4xutpe6neqkyentlck5tmmhy58</sha1>
    </revision>
  </page>
  <page>
    <title>Ordered weighted averaging aggregation operator</title>
    <ns>0</ns>
    <id>14893994</id>
    <revision>
      <id>634524902</id>
      <parentid>634524744</parentid>
      <timestamp>2014-11-19T12:07:50Z</timestamp>
      <contributor>
        <username>Aemrouz</username>
        <id>10413759</id>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9935">In applied mathematics – specifically in [[fuzzy logic]] – the '''ordered weighted averaging (OWA) operators''' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.

== Definition ==

Formally an '''OWA''' operator of dimension &lt;math&gt; \ n &lt;/math&gt; is a mapping &lt;math&gt; F: R_n \rightarrow R &lt;/math&gt; that has an associated collection of weights &lt;math&gt; \  W = [w_1, \ldots, w_n] &lt;/math&gt; lying in the unit interval and summing to one and with 		

:&lt;math&gt; F(a_1, \ldots , a_n) =  \sum_{j=1}^n  w_j b_j&lt;/math&gt;

where &lt;math&gt; b_j &lt;/math&gt; is the ''j''&lt;sup&gt;th&lt;/sup&gt; largest of the &lt;math&gt; a_i &lt;/math&gt;.

By choosing different ''W'' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the ''b''&lt;sub&gt;''j''&lt;/sub&gt;.

== Properties ==

The OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.

{|class=&quot;wikitable&quot;
|[[Bounded operator|Bounded]]
|&lt;math&gt;   \min(a_1, \ldots, a_n) \le F(a_1, \ldots, a_n) \le \max(a_1, \ldots, a_n) &lt;/math&gt;
|-
|[[Monotonic]]
|&lt;math&gt;   F(a_1, \ldots, a_n) \ge F(g_1, \ldots, g_n) &lt;/math&gt; if &lt;math&gt; a_i \ge g_i &lt;/math&gt; for &lt;math&gt;\ i = 1,2,\ldots,n &lt;/math&gt;
|-
|[[symmetric operator|Symmetric]]
|&lt;math&gt;   F(a_1, \ldots, a_n)  = F(a_\boldsymbol{\pi(1)}, \ldots, a_\boldsymbol{\pi(n)})&lt;/math&gt; if &lt;math&gt;\boldsymbol{\pi} &lt;/math&gt; is a permutation map
|-
|[[Idempotent]]
|&lt;math&gt;  \ F(a_1, \ldots, a_n)  =  a &lt;/math&gt; if all &lt;math&gt; \ a_i = a &lt;/math&gt;
|}

== Notable OWA operators ==
:&lt;math&gt; \ F(a_1, \ldots, a_n) = \max(a_1, \ldots, a_n) &lt;/math&gt; if &lt;math&gt; \ w_1 = 1 &lt;/math&gt; and &lt;math&gt; \ w_j = 0 &lt;/math&gt; for &lt;math&gt; j \ne 1 &lt;/math&gt;

:&lt;math&gt; \ F(a_1, \ldots, a_n) = \min(a_1, \ldots, a_n) &lt;/math&gt; if &lt;math&gt; \ w_n = 1 &lt;/math&gt; and &lt;math&gt; \ w_j = 0 &lt;/math&gt; for &lt;math&gt; j \ne n &lt;/math&gt;

== Characterizing features ==

Two features have been used to characterize the OWA operators. The first is the attitudinal character(orness).

This is defined as
:&lt;math&gt;A-C(W)= \frac{1}{n-1} \sum_{j=1}^n (n - j) w_j. &lt;/math&gt;

It is known that &lt;math&gt; A-C(W) \in [0, 1] &lt;/math&gt;.

In addition ''A''&amp;nbsp;&amp;minus;&amp;nbsp;''C''(max) = 1, A&amp;nbsp;&amp;minus;&amp;nbsp;C(ave) = A&amp;nbsp;&amp;minus;&amp;nbsp;C(med) = 0.5 and A&amp;nbsp;&amp;minus;&amp;nbsp;C(min) = 0. Thus the A&amp;nbsp;&amp;minus;&amp;nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).

The second feature is the dispersion. This defined as

:&lt;math&gt;H(W) = -\sum_{j=1}^n w_j \ln (w_j).&lt;/math&gt;

An alternative definition is &lt;math&gt;E(W) = \sum_{j=1}^n w_j^2 .&lt;/math&gt; The dispersion characterizes how uniformly the arguments are being used

== A literature survey: OWA (1988-2014)==
The historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183–190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full) 

== Type-1 OWA aggregation operators ==

The above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The
'''[[Type-1 OWA operators]]''' have been proposed for this purpose. So the '''[[type-1 OWA operators]]''' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.

The '''[[Type-1 OWA operators|type-1 OWA operator]]''' is defined according to the alpha-cuts of fuzzy sets as follows:

Given the ''n'' linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, then for each &lt;math&gt;\alpha \in [0,\;1]&lt;/math&gt;, an &lt;math&gt;\alpha &lt;/math&gt;-level type-1 OWA operator with &lt;math&gt;\alpha &lt;/math&gt;-level sets &lt;math&gt;\left\{ {W_\alpha ^i } \right\}_{i = 1}^n &lt;/math&gt; to aggregate the &lt;math&gt;\alpha &lt;/math&gt;-cuts of fuzzy sets &lt;math&gt;\left\{ {A^i} \right\}_{i =1}^n &lt;/math&gt; is given as

: &lt;math&gt;
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}&lt;/math&gt;

where &lt;math&gt;W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}&lt;/math&gt;, and &lt;math&gt;\sigma :\{\;1, \ldots ,n\;\} \to \{\;1, \ldots ,n\;\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \ldots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma (i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th largest
element in the set &lt;math&gt;\left\{ {a_1 , \ldots ,a_n } \right\}&lt;/math&gt;.

The computation of the '''[[Type-1 OWA operators|type-1 OWA]]''' output is implemented by computing the left end-points and right end-points of the intervals &lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)&lt;/math&gt;:
&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_{-} &lt;/math&gt; and &lt;math&gt;
\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_ {+},&lt;/math&gt;
where &lt;math&gt;A_\alpha ^i=[A_{\alpha-}^i, A_{\alpha+}^i], W_\alpha ^i=[W_{\alpha-}^i, W_{\alpha+}^i]&lt;/math&gt;. Then membership function of resulting aggregation fuzzy set is:

:&lt;math&gt;\mu _{G} (x) = \mathop \vee \limits_{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha &lt;/math&gt;

For the left end-points, we need to solve the following programming problem:

:&lt;math&gt; \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \mathop {\min }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } &lt;/math&gt;

while for the right end-points, we need to solve the following programming problem:

:&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \mathop {\max }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } &lt;/math&gt;

[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.

== References ==

* Yager, R. R., &quot;On ordered weighted averaging aggregation operators in multi-criteria decision making,&quot; IEEE Transactions on Systems, Man and Cybernetics 18, 183–190, 1988.

* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.

* Liu, X., &quot;The solution equivalence of minimax disparity and minimum variance problems for OWA operators,&quot; International Journal of Approximate Reasoning 45, 68–81, 2007.

* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]

* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988–2014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  &amp; http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&amp;s=c0d8bdd220a31c876eb5885521cfa16d191f334d]. 

* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.

* Majlender, P., &quot;OWA operators with maximal Rényi entropy,&quot; Fuzzy Sets and Systems 155, 340–360, 2005.

* Szekely, G. J. and Buczolich, Z., &quot; When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?&quot; Advances in Applied Mathematics 10, 1989, 439–456.

* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, &quot;Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers,&quot; Fuzzy Sets and Systems, Vol.159, No.24, pp.&amp;nbsp;3281–3296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]

* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, &quot;Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments,&quot; IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&amp;nbsp;1455–1468.[http://dx.doi.org/10.1109/TKDE.2010.191]

* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, &quot;On aggregating uncertain information by type-2 OWA operators for soft decision making,&quot; International Journal of Intelligent Systems, vol. 25, no.6, pp.&amp;nbsp;540–558, 2010.[http://dx.doi.org/10.1002/int.20420]

[[Category:Artificial intelligence]]
[[Category:Logic in computer science]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval]]</text>
      <sha1>c73shcydu2k4utetjjdrhz4hcufry2v</sha1>
    </revision>
  </page>
  <page>
    <title>Information needs</title>
    <ns>0</ns>
    <id>11016342</id>
    <revision>
      <id>589685156</id>
      <parentid>541786203</parentid>
      <timestamp>2014-01-08T00:06:02Z</timestamp>
      <contributor>
        <ip>117.204.26.111</ip>
      </contributor>
      <comment>/* Background */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5122">'''Information need''' is an individual or group's desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The ‘information’ and ‘need’ in ‘information need’ are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:
# The explanation of observed phenomena of information use or expressed need;
# The prediction of instances of information uses;
# The control and thereby improvement of the utilization of information manipulation of essentials conditions.

Information needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.

== Background ==

The concept of information needs was coined by an American information gernalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 &quot;The Process of Asking Questions&quot;] published in American Documentation (Now is Journal of the American Society of Information Science and Technology).

In this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.

According to Taylor, information need has four levels:
# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the “ideal question” — the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information
# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.
# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirer’s doubts.
# The question as presented to the information system.

There are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).

Herbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:
# When approached from the point of view of the scientist or technologists, these are studies of scientists’ communication behaviour;
# When approached from the point of view of any communication medium, they are use studies;
# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.

William J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:
# The scientist within his culture.
# The scientist within a political system.
# The scientist within a membership group.
# The scientist within a reference group.
# The scientist within an invisible college.
# The scientist within a formal organization.
# The scientist within a work team.
# The scientist within his own head.
# The scientist within a legal/economical system.
# The scientist within a formal.

==See also==
* [[Information retrieval]]

==References==
* Menzel, Herbert. “Information Needs and Uses in Science and Technology.” Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.
* Paisley, William J. “Information Needs and Uses.” Annual Review of Information Science and Technology, Vol.3, Encyclopædia Britannica, Inc. Chicago 1968, pp.1-30.
* Taylor, Robert S. “The Process of Asking Questions” American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.
* Wilson, T.D. “On User Studies and Information Needs.” Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15

[[Category:Information retrieval]]
[[Category:Searching]]</text>
      <sha1>k0nzv0ucqtxclkajlyhu7d6jfkumqyr</sha1>
    </revision>
  </page>
  <page>
    <title>Noisy text analytics</title>
    <ns>0</ns>
    <id>6026708</id>
    <revision>
      <id>506708525</id>
      <parentid>500316265</parentid>
      <timestamp>2012-08-10T11:20:44Z</timestamp>
      <contributor>
        <ip>188.193.72.68</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6047">'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as “um” and “uh” and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.

== Techniques for noisy text analysis ==
Missing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[Part-of-speech tagging]]
and [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed. 

== Possible source of noisy text ==
* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.
* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.
* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.
* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.

== References ==
*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&amp;pi=0 &quot;Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007.&quot;]
*[http://arXiv.org/abs/0810.0332 &quot;Wong, W., Liu, W. &amp; Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND); Hyderabad, India.&quot;].
&lt;references /&gt;


==See also==
* [[Text analytics]]
* [[Information extraction]]
* [[Computational linguistics]]
* [[Natural language processing]]
* [[Named entity recognition]]
* [[Text mining]]
* [[Automatic summarization]]
* [[Statistical classification]]
* [[Data quality]]

[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
[[Category:Statistical natural language processing]]

[[es:Extracción de la información]]</text>
      <sha1>b1agbh0s5svxzou50fvta74xcx78w80</sha1>
    </revision>
  </page>
  <page>
    <title>Term Discrimination</title>
    <ns>0</ns>
    <id>15101979</id>
    <revision>
      <id>489442140</id>
      <parentid>489434963</parentid>
      <timestamp>2012-04-27T08:53:53Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <minor/>
      <comment>[[Help:Reverting|Reverted]] edits by [[Special:Contributions/213.55.102.49|213.55.102.49]] ([[User talk:213.55.102.49|talk]]) to last version by John of Reading</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2713">'''Term Discrimination''' is a way to rank keywords in how useful they are for [[Information Retrieval]].

== Overview ==

This is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.

This method uses the concept of ''Vector Space Density'' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.

An optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.  

The discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density.

 Let:
 &lt;math&gt;A&lt;/math&gt; be the occurrence matrix
 &lt;math&gt;A_k&lt;/math&gt; be the occurrence matrix without the index term &lt;math&gt;k&lt;/math&gt;
 and &lt;math&gt;Q(A)&lt;/math&gt; be density of &lt;math&gt;A&lt;/math&gt;.
 Then:
 The discrimination value of the index term &lt;math&gt;k&lt;/math&gt; is: 
 &lt;math&gt;DV_k = Q(A) - Q(A_k)&lt;/math&gt;

== How to compute ==

Given an [[occurrency matrix]]: &lt;math&gt;A&lt;/math&gt; and one keyword: &lt;math&gt;k&lt;/math&gt;
* Find the global document [[centroid]]: &lt;math&gt;C&lt;/math&gt; (this is just the average document vector)
* Find the average [[euclidean distance]] from every document vector, &lt;math&gt;D_i&lt;/math&gt; to &lt;math&gt;C&lt;/math&gt;
* Find the average euclidean distance from every document vector, &lt;math&gt;D_i&lt;/math&gt; to &lt;math&gt;C&lt;/math&gt; ''IGNORING'' &lt;math&gt;k&lt;/math&gt;
* The difference between the two values in the above step is the ''discrimination value'' for keyword &lt;math&gt;K&lt;/math&gt;

A higher value is better because including the keyword will result in better information retrieval.

== Qualitative Observations ==
Keywords that are ''[[Sparse matrix|sparse]]'' should be poor discriminators because they have poor ''[[Precision and recall|recall]],''
whereas
keywords that are ''frequent'' should be poor discriminators because they have poor ''[[Precision and recall|precision]].''

== References ==
* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), &quot;[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing],&quot; ''Communications of the ACM'', vol. 18, nr. 11, pages 613–620. ''(The article in which the vector space model was first presented)''

* Can, F., Ozkarahan, E. A (1987), &quot;Computation of term/document discrimination values by use of the cover coefficient concept.&quot; ''Journal of the American Society for Information Science'', vol. 38, nr. 3, pages 171-183.

[[Category:Information retrieval]]</text>
      <sha1>1m0rn3h2uiukvpk94fcyghw7d3a23cr</sha1>
    </revision>
  </page>
  <page>
    <title>IR evaluation</title>
    <ns>0</ns>
    <id>15637980</id>
    <revision>
      <id>623371139</id>
      <parentid>601791008</parentid>
      <timestamp>2014-08-29T22:23:12Z</timestamp>
      <contributor>
        <ip>187.253.146.239</ip>
      </contributor>
      <comment>/* IR Evaluation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2823">{{multiple issues|
{{Orphan|date=February 2009}}
{{Unreferenced|date=March 2008}}
}}

== IR Evaluation ==
The evaluation of information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].

*'''Precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:

:&lt;math&gt; \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} &lt;/math&gt;

*'''Recall''' is the fraction of the documents relevant to the query that are successfully retrieved:

:&lt;math&gt; \mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} &lt;/math&gt;

*'''F-measure''' is the harmonic mean of precision and recall:

:&lt;math&gt; \mbox{F-measure}= 2 * \frac{\{\mbox{precision}\}*\{\mbox{recall}\}}{\{\mbox{precision}\}+\{\mbox{recall}\}} &lt;/math&gt;

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.

F-measure tends to be a better single metric when compared to precision and recall because both of them give different information that can complement each other when combined. If one of them excels more than the other, this metric will reflect it.

Virtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.

==See also==
* [[Information retrieval]]
* [[Precision and recall]]
* [[Web search engine]]

==Further reading==
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&amp;uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.
*Stefan B&amp;uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

[[Category:Information retrieval]]
[[Category:Searching]]</text>
      <sha1>r1eolsp4scqe9ngb0d4lkqhjrn07m16</sha1>
    </revision>
  </page>
  <page>
    <title>Coveo</title>
    <ns>0</ns>
    <id>16001013</id>
    <revision>
      <id>647239541</id>
      <parentid>618131495</parentid>
      <timestamp>2015-02-15T12:47:03Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>clean up using [[Project:AWB|AWB]] (10823)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3036">{{Infobox company
| name = Coveo Solutions Inc.
| logo = [[Image:Coveo logo.png|120px]]
| type = Private
| slogan = 
| foundation =  2004
| location_city = [[Quebec City]], [[Canada]]
| key_people = Louis Têtu, Chairman and CEO &lt;br /&gt;Laurent Simoneau, President and CTO
| num_employees =
| industry = [[Enterprise search]]
| products = Coveo Search &amp; Relevance Platform,&lt;br /&gt;Coveo for Sitecore,&lt;br /&gt;Coveo for Salesforce
| homepage = http://www.coveo.com
}}

'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for Salesforce.com, Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.

==History==
Coveo Solutions Inc. was founded in 2004 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.&lt;ref&gt;http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/&lt;/ref&gt;

==Products==
'''Coveo Search &amp; Relevance Platform'''

Coveo Search &amp; Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.

'''Coveo for Sitecore'''

Coveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore’s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.

'''Coveo for Salesforce'''

Coveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.

==Customers==
Coveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley &amp; Aldrich, GEICO, Lockheed Martin, P&amp;G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.&lt;ref&gt;{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}&lt;/ref&gt; These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}

==References==
{{reflist}}

==External links==
* [http://www.coveo.com/ Coveo.com]

[[Category:Companies based in Quebec City]]
[[Category:Information retrieval]]
[[Category:BlackBerry development software]]</text>
      <sha1>jfsmplw74h8rv2zic4k80to2ux218kf</sha1>
    </revision>
  </page>
  <page>
    <title>Information Retrieval Facility</title>
    <ns>0</ns>
    <id>16635934</id>
    <revision>
      <id>581761998</id>
      <parentid>555437878</parentid>
      <timestamp>2013-11-15T12:36:15Z</timestamp>
      <contributor>
        <ip>128.131.168.182</ip>
      </contributor>
      <comment>The IRF is no more. Updated summary to reflect this.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8836">{{Advert|date=May 2012}}

[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]

The '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.

The IRF had members in the following categories:

* Researchers in [[information retrieval]] (IR) or related scientific areas
* Industrial/corporate information management professionals
* Patent authorities and governmental institutions
* Students of one of the above

==The Scientific Board==
'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]

'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],
Director of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]

'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]

'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]

'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]

'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]

'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]

'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]

'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]

'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]

'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]

'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]

'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]

'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',
[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]

==Scientific Goals==

* Modelling innovative and specialised information retrieval systems for global patent document collections.
* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.&lt;
* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.
* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.
* Ability to create different views of patent data depending on the focus of the information need.
* Defining standardised methods for benchmarking the information retrieval process in patent document collections.
* Ability to handle text and non-text parts of a patent in a coherent manner.
* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.
* Integrating the temporal dimension of patent documents in retrieval strategies.
* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.
* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.
* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.
* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.
* Investigating and experimenting with computing architectures for very high-capacity information management.
* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.
* Discovering and investigating novel use cases and business applications deriving from intellectual property information.
* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.
* Development and integration of different information access methods.
* Research on effective methods for interactive information retrieval.

==Semantic Supercomputing==
Current technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:

* multi-node clusters (currently 80 cores, up to 1024)
* highest speed interconnect technology
* single system image with large compound memory (currently 320 GB, up to 4 TB)
* fully integrated configurable computing (currently 4 FPGA cores, up to 256)

The combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.

==The World Patent Corpus==
The IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. 
The entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. What’s more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.

== Research Collections ==
The IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.

The MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.

The ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.

==External links==
* [http://www.ir-facility.org/ Official site: ir-facility.org]
* [http://youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] 
* [http://youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]

==References==
* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]
* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]

[[Category:Organizations established in 2006]]
[[Category:Computer science organizations]]
[[Category:Information retrieval]]
[[Category:Education in Vienna]]</text>
      <sha1>0a2phiqbt1lt8p0r35tbou004bea37a</sha1>
    </revision>
  </page>
  <page>
    <title>Relevance (information retrieval)</title>
    <ns>0</ns>
    <id>442684</id>
    <revision>
      <id>627015748</id>
      <parentid>619333020</parentid>
      <timestamp>2014-09-25T11:39:14Z</timestamp>
      <contributor>
        <ip>146.50.69.218</ip>
      </contributor>
      <comment>/* Additional reading */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9035">{{Other uses|Relevance}}

In [[information science]] and [[information retrieval]], '''relevance''' denotes how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.

== History ==

The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.

The formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term &quot;relevant&quot; to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.&lt;ref&gt;Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810‐832.&lt;/ref&gt;

Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between &quot;relevance to a subject&quot; or &quot;topical relevance&quot; and &quot;user relevance&quot;.

Recently, Zhao and Callan (2010)&lt;ref&gt;Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.&lt;/ref&gt; showed a connection between the [[Binary Independence Model|relevance probability]] and the [[vocabulary mismatch]] problem in retrieval, which could lead to at least 50-300% gains in retrieval accuracy.&lt;ref&gt;Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.&lt;/ref&gt;

== Evaluation ==

The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.

In order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.

In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).

== Clustering and relevance ==

The [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.&lt;ref name=diazthesis&gt;F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.&lt;/ref&gt;    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:
* cluster-based information retrieval&lt;ref name=croftcbir&gt;W. B. Croft, “A model of cluster searching based on classification,” Information Systems, vol. 5, pp. 189–195, 1980.&lt;/ref&gt;&lt;ref name=griffithscbir&gt;A. Griffiths, H. C. Luckhurst, and P. Willett, “Using interdocument similarity information in document retrieval systems,” Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3–11, 1986.&lt;/ref&gt;
* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.&lt;ref name=lmcbir&gt;X. Liu and W. B. Croft, “Cluster-based retrieval using language models,” in SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186–193, ACM Press, 2004.&lt;/ref&gt;    It is important to ensure that clusters – either in isolation or combination – successfully model the set of possible relevant documents.

A second interpretation, most notably advanced by Ellen Voorhees,&lt;ref name=voorheescbir&gt;E. M. Voorhees, “The cluster hypothesis revisited,” in SIGIR ’85: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188–196, ACM Press, 1985.&lt;/ref&gt;    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,
* multiple cluster retrieval&lt;ref name=griffithscbir/&gt;&lt;ref name=voorheescbir/&gt;
* spreading activation&lt;ref name=preece&gt;S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.&lt;/ref&gt; and relevance propagation&lt;ref name=relprop&gt;T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, “A study of relevance propagation for web search,” in SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408–415, ACM Press, 2005.&lt;/ref&gt; methods
* local document expansion&lt;ref name=docexpansion&gt;A. Singhal and F. Pereira, “Document expansion for speech retrieval,” in SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34–41, ACM Press, 1999.&lt;/ref&gt;
* score regularization&lt;ref name=diazreg&gt;F. Diaz, “Regularizing query-based retrieval scores,” Information Retrieval, vol. 10, pp. 531–562, December 2007.&lt;/ref&gt;
Local methods require an accurate and appropriate document similarity measure.

==Epistemological issues==
{{Section OR|date=May 2014}}
Are users best at evaluating the relevance of a given document, or is it better to use experts?
Most research about relevance in information retrieval in recent years have implicitly assumed that the users' evaluation of the output a given system should be used to increase &quot;relevance&quot; output. An alternative strategy would be to use journal [[impact factor]] to rank output and thus base relevance on expert evaluations. Other strategies, such as including diversity of the search results, may be used as well. The important thing to recognize is, however, that relevance is fundamentally a question of [[epistemology]], not [[psychology]]. (Peoples' psychology reflects certain epistemological influences).

==References==
 {{reflist}}

==Additional reading==
*Hjørland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.

*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9

*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])

*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])

*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])

[[Category:Information retrieval]]</text>
      <sha1>f2iwsnar6qkun8iyn6rlbf1em0zws53</sha1>
    </revision>
  </page>
  <page>
    <title>Mean reciprocal rank</title>
    <ns>0</ns>
    <id>11184711</id>
    <revision>
      <id>598881551</id>
      <parentid>547334495</parentid>
      <timestamp>2014-03-09T20:15:42Z</timestamp>
      <contributor>
        <ip>84.24.185.157</ip>
      </contributor>
      <comment>/* Example */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2078">{{Refimprove|date=June 2007}}
'''Mean reciprocal rank''' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:&lt;ref&gt;{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&amp;ndash;82}}&lt;/ref&gt;

:&lt;math&gt; \text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}. \!&lt;/math&gt;

The reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.

== Example ==
For example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:

{| class=&quot;wikitable&quot;
|-
! Query
! Results
! Correct response
! Rank
! Reciprocal rank
|-
| cat
| catten, cati, '''cats'''
| cats
| 3
| 1/3
|-
| torus
| torii, '''tori''', toruses
| tori
| 2
| 1/2
|-
| virus
| '''viruses''', virii, viri
| viruses
| 1
| 1
|}

Given those three samples, we could calculate the mean reciprocal rank as (1/3&amp;nbsp;+&amp;nbsp;1/2&amp;nbsp;+&amp;nbsp;1)/3 = 11/18 or about 0.61.

This basic definition does not specify what to do if...
# none of the proposed results are correct (use reciprocal rank 0), or if
# there are multiple correct answers in the list. Consider using [[Information_retrieval#Mean_average_precision|mean average precision (MAP)]].

See also [[Information retrieval]] and [[Question answering]].&lt;ref&gt;{{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC | author=D. R. Radev, H. Qi, H. Wu, W. Fan |year=2002 }}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Summary statistics]]
[[Category:Information retrieval]]</text>
      <sha1>kobhmf2l1xfylau3z2d17bjk3q2d4fx</sha1>
    </revision>
  </page>
  <page>
    <title>Concept Searching Limited</title>
    <ns>0</ns>
    <id>17770654</id>
    <revision>
      <id>647079365</id>
      <parentid>646518175</parentid>
      <timestamp>2015-02-14T10:06:37Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>clean up using [[Project:AWB|AWB]] (10823)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3344">{{Infobox company |
  name   = Concept Searching Limited |
  logo = [[Image:conceptSearching.jpg]] |
  company_slogan = &quot;Retrieval Just Got Smarter&quot; |
  type   =  [[Privately held company|Private]] |
  foundation     = 2002|
  location       = [[UK]], [[USA]] |
  area_served    = Global |
  industry       = [[Information retrieval]] |
  products       = conceptSearch&lt;br/&gt;conceptClassifier&lt;br/&gt;conceptClassifier for SharePoint&lt;br/&gt;conceptClassifier for SharePoint Online&lt;br/&gt;Taxonomy Manager&lt;br/&gt;Taxonomy Workflow |
  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]
}}

'''Concept Searching Limited''' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].

==History==
Concept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].&lt;ref&gt;[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&amp;ETOC=RN Lateral thinking in information retrieval] ''Information Management and Technology.'' 2003. vol 36; part 4, pp 169-173&lt;/ref&gt;&lt;ref&gt;[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval&lt;/ref&gt;

Compound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.&lt;ref&gt;[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008&lt;/ref&gt;

The company's products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.&lt;ref&gt;[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile&lt;/ref&gt;

Concept Searching has developed the '''Smart Content Framework''', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center &lt;ref&gt;[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework&lt;/ref&gt;

== Awards ==
* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014 &lt;ref&gt;{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-Companies-That-Matter-in-Knowledge-Management-94933.aspx |title=KMWorld Magazine}}&lt;/ref&gt;
* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014 &lt;ref&gt;{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2014-98792.aspx |title=Trend-Setting Products}}&lt;/ref&gt;

==See also==
* [[Compound term processing]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Concept Search]]

==References==
{{Reflist}}

==External links==
*[http://www.conceptsearching.com/ Company Website]

[[Category:Information retrieval]]
[[Category:Privately held companies of the United Kingdom]]</text>
      <sha1>0f2x4j90tjrkz1dxm9bkz283cysxbwq</sha1>
    </revision>
  </page>
  <page>
    <title>Globrix</title>
    <ns>0</ns>
    <id>16137133</id>
    <revision>
      <id>577749846</id>
      <parentid>575753062</parentid>
      <timestamp>2013-10-18T18:59:40Z</timestamp>
      <contributor>
        <username>LilHelpa</username>
        <id>8024439</id>
      </contributor>
      <minor/>
      <comment>it's → its</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5575">'''Globrix''' was a UK [[real estate]] [[Web search engine|search engine]] that was launched in January 2008. It was launched as a joint venture with [[News International]], publishers of ''[[The Sunday Times]]'', ''[[The Sun (newspaper)|The Sun]]'', ''[[The Times]]'', ''[[The News of the World]]'' and ''[[Thelondonpaper]]''.&lt;ref&gt;[http://www.nma.co.uk/news/news-international-invests-in-property-site-globrix/35492.article News International invests in property site, Globrix - NMA article]&lt;/ref&gt;

[[Estate agent]]s and [[letting agent]]s could list their properties for free. This competed with traditional paid-listings sites such as [[Rightmove]] (originally a joint venture between four of the UK's largest property agents, now a [[public limited company]]), [[Zoopla|Propertyfinder]] (also partly backed by News International) and [[Primelocation]] (owned by [[Daily Mail and General Trust]]). Unlike most property websites, Globrix directed users to agent websites rather than hosting the property details and capturing the lead on Globrix itself. Globrix gathered its property listings in three different ways; crawling agent websites, taking data feeds and by agents manually uploading via the Globrix extranet. Because Globrix was 'free to list', Globrix was able to gain substantial market coverage and claimed to list more properties than any other UK property website. Unlike websites like [[Gumtree]] and [[Oodle]], private sellers and landlords were not allowed to list their properties on the site.

The website charged property professionals and property related services companies for geo-targeted [[Web banner|banner ads]]. There were also premium services available to estate and letting agents (such as [[Search Engine Optimization]] consultancy, branded email alerts and increased traffic) and [[Google Ads]] were displayed in unsold advertising positions on the right hand side of search results.

==Functionality==

The basic property search functionality is kept simple with just one text box on the homepage. Users can search for property by location (e.g. city, town, full postcode, partial postcode or, unusually for property portals, street name), places of interest (e.g. schools, stations, landmarks) or by key features (e.g. swimming pool, garden, double glazing, helipad).

Search results can then be refined further by changing the price parameters, number of bedrooms and bathrooms, property type (e.g. detached, bungalow, flat), outside space, nearby stations and schools and property features (e.g. wooden floors, sea view). Registered users are able to search by additional parameters such as price change.

As an alternative to the regular 'list view' of property results, users can also opt to see the search results plotted on [[Bing Maps]] (previously they used [[Google map]]) to allow users to look for property by location. (Some users are unimpressed with the lack of precision of the inferior Bing offering, which often manages to put the marker in a field, compared to the accuracy and ease of use of Googlemaps).  Users are able to drag and zoom the map, with relevant properties automatically placed in view. It is also possible for users to draw a catchment area directly onto the map of where they would like to search.

==Data==

Globrix data was sometimes used by the national media to illustrate stories on house prices,&lt;ref&gt;House prices drop £100,000 in two weeks in race to sell before Christmas - Daily Mail [http://www.dailymail.co.uk/news/article-1089563/House-prices-drop-100-000-WEEKS-race-sell-Christmas.html]&lt;/ref&gt; the economy, area trends, consumer confidence&lt;ref&gt;[http://news.bbc.co.uk/1/hi/business/7737507.stm House sales rise as prices fall - BBC News]&lt;/ref&gt; and the property market.&lt;ref&gt;[http://www.telegraph.co.uk/finance/personalfinance/borrowing/mortgages/3268208/Housing-market-stagnates-as-buyers-disappear.html Housing market stagnates as buyers disappear - Daily Telegraph]&lt;/ref&gt;

==Awards==

In 2008, Globrix was awarded 'Best Property Portal UK' which is awarded by one of the group's own newspapers, the [[The Daily Mail]].&lt;ref&gt;[http://www.residentialpropertyawards.net/index.php/International/Winners/Winners-of-2008.html Daily Mail Property Awards 2008]&lt;/ref&gt; Globrix also won 'Estate Agency Service Firm of the Year' at The Negotiator Awards.&lt;ref&gt;[http://negotiator-magazine.co.uk/events/awards/categories-and-finalists/agency-service-firm-of-the-year/ The Negotiator Awards 2008]&lt;/ref&gt;

==Founders==

Globrix was founded by Dan Lee and Ian Parry, both ex employees of UK-based search company [[Autonomy Corporation|Autonomy]] and the Norwegian search company [[Fast Search &amp; Transfer|FAST]].

==Merged with Zoopla==

In December 2012 Globrix merged with [[Zoopla]].&lt;ref name=&quot;Estate Agent Today&quot;&gt;{{cite web|title=Zoopla acquires Globrix as it steps up battle against Rightmove|url=http://www.estateagenttoday.co.uk/news_features/Zoopla-acquires-Globrix-as-it-steps-up-battle-against-Rightmove|work=Estate Agent Today|accessdate=8 September 2013}}&lt;/ref&gt;

== References ==
&lt;references/&gt;

==External links==
* [http://www.globrix.com/ Globrix homepage]
* [http://www.ft.com/cms/s/0/bc401968-824e-11dc-8a8f-0000779fd2ac.html News International invests in search engine] - Financial Times
* [http://www.independent.co.uk/news/business/analysis-and-features/home-search-sites-have-a-new-kid-on-the-block-786342.html Home search sites have a new kid on the block] - The Independent

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Online real estate companies]]</text>
      <sha1>rr0f4exteuaatxm1lwflyexre66z4ov</sha1>
    </revision>
  </page>
  <page>
    <title>Figaro Systems</title>
    <ns>0</ns>
    <id>17910258</id>
    <revision>
      <id>647264265</id>
      <parentid>647239744</parentid>
      <timestamp>2015-02-15T17:01:00Z</timestamp>
      <contributor>
        <username>Viva-Verdi</username>
        <id>573321</id>
      </contributor>
      <minor/>
      <comment>make website link visible</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10856">{{Infobox company
|name = Figaro Systems, Inc.
|logo = [[Image:Figaro-logo.png|Figaro logo]]
|type = [[Privately held company|Private]]
|foundation = 1993
|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]
|location_country =[[United States]]
|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]
|homepage = [http://www.figarosystems.com figarosystems.com]
}}

'''Figaro Systems, Inc.''' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 &lt;ref&gt;Andrew Webb, “Opera Subtitle Firm Eyes New Game,” ''New Mexico Business Weekly'', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]&lt;/ref&gt;
by Patrick Markle, [[Geoff Webb]], and Ron Erkman  &lt;ref name=&quot;figaro-systems.com&quot;/&gt; and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.
&lt;ref&gt;[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, “Nothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,”] ''Albuquerque Journal'', June 4, 2006&lt;/ref&gt;

==History==
Figaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.&lt;ref&gt;[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, &quot;Figaro: Eyes translate when ears don't get it&quot;,] ''New Mexico Business Weekly'', April 8, 2005&lt;/ref&gt;

The [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.&lt;ref name=&quot;figaro-systems.com&quot;&gt;[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]&lt;/ref&gt; Markle, Webb, and Erkman were further reinforced by their understanding of technology’s role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.&lt;ref&gt;[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf “[[User-friendly]] art: In-seat text displays that subtitle and translate”, ''Auditoria'', May 2007]&lt;/ref&gt; Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.
&lt;ref&gt;[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=11&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=figaro.ASNM.&amp;OS=AN/figaro&amp;RS=AN/Figaro  United States Patent 5,739,869, &quot;Electronic libretto display apparatus and method,&quot; issued April 14, 1998. [[United States Patent and Trademark Office]] ]&lt;/ref&gt;&lt;ref&gt;[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, ''Daily News Bulletin'', May 7, 2001]&lt;/ref&gt;

Philanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.&lt;ref&gt;[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], &quot;A Knight at the Opera,&quot; ''[[New York Magazine]]'', January 14, 2002]&lt;/ref&gt;&lt;ref&gt;[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  &quot;Alberto Vilar: The Privileges of Wealth,&quot; ''The Free Encyclopedia'']&lt;/ref&gt;  He donated the company's [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.

In 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its &quot;Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met.&quot; &lt;ref&gt;[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, ''Class Action Litigation Information''] on classactionlitigation.com&lt;/ref&gt;

==Products and technology==
The company’s products are known variously as seat back titles, [[surtitles]],
&lt;ref&gt;[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, &quot;Surtitles at the Opera,&quot; ''Public Radio News and Information in Houston, Texas'', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/&lt;/ref&gt; [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.

Opera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]
&lt;ref&gt;[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html &quot;Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program,&quot; Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov&lt;/ref&gt; although the software enables the reading of the libretto in any [[written language]].
&lt;ref name=&quot;entertanmentengineering.com&quot;&gt;[http://www.entertanmentengineering.com/v4.issue04/page.06.html  “Giving the Opera a New Voice,”] ''Entertainment Engineering,&quot; Volume 4, Issue 2, p. 6&lt;/ref&gt; Translation is provided by one screen and delivery system per person.&lt;ref&gt;[http://www.figarosystems.com  Figaro Systems Official Website]&lt;/ref&gt;

Typically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues’ existing systems as well as Figaro Systems' &quot;Simultext&quot; system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.&lt;ref name=&quot;entertanmentengineering.com&quot;/&gt;

The company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens 
&lt;ref&gt;[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=3&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=figaro.ASNM.&amp;OS=AN/figaro&amp;RS=AN/Figaro  United States Patent 6,760,010. &quot;Wireless electronic libretto display apparatus and method,&quot; issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database&lt;/ref&gt; for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.

==Venues==
In the US, the company’s systems are in use in the [[Ellie Caulkins Opera House]] 
&lt;ref&gt;[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, &quot;Opera dialogue shows on seat in front of you,&quot;] ''Rocky Mountain News'' (Denver, Colorado), September 3, 2005 on highbeam.com,&lt;/ref&gt; in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,&lt;ref&gt;[http://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],&lt;/ref&gt; the [[Brooklyn Academy of Music]]&lt;ref&gt;[http://www.appliancemagazine.com/editorial.php?article=1768&amp;zone=210&amp;first=1  “An Operatic Performance,” ''Appliance Magazine'', June 2007],&lt;/ref&gt; the [[Metropolitan Opera]], New York, where it is called &quot;MetTitles&quot;),&lt;ref&gt;[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],&lt;/ref&gt; the [[Roy E. Disney]] Theatre in [[Albuquerque]]'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.&lt;ref name=&quot;figaro-systems.com&quot;/&gt;

In the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].
&lt;ref&gt;[http://www.entertainmentengineering.com/v4.issue04/page.06.html “Giving the Opera a New Voice,” ''Entertainment Engineering.'', Volume 4, Issue 2, p. 6], on entertainmentengineering.com&lt;/ref&gt;

==Awards==
In 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories’ Technology Commercialization Award for its Simultext system.&lt;ref&gt;[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, &quot;Los Alamos announces technology commercialization awards,&quot; ''Los Alamos National Laboratory News''], Los Alamos National Security, LLC, US Department of Energy's NNSA, May 7, 2001 on lanl.gov/news.&lt;/ref&gt;
In 2008, the company’s software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.

==References==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Software companies based in New Mexico]]
[[Category:Assistive technology]]
[[Category:Educational technology]]
[[Category:Companies based in New Mexico]]
[[Category:Privately held companies based in New Mexico]]
[[Category:Companies established in 1993]]</text>
      <sha1>nb5jk9yjtpf2cfg4jleg1nrg93c1e8k</sha1>
    </revision>
  </page>
  <page>
    <title>Text Retrieval Conference</title>
    <ns>0</ns>
    <id>1897206</id>
    <revision>
      <id>632721871</id>
      <parentid>632721206</parentid>
      <timestamp>2014-11-06T18:25:46Z</timestamp>
      <contributor>
        <username>Cdrott</username>
        <id>151835</id>
      </contributor>
      <comment>/* Past tracks */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11574">{{Other uses of|TREC|TREC (disambiguation)}}
The '''Text REtrieval Conference (TREC)''' is an on-going series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or ''tracks.'' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale ''evaluation'' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].

Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.

== Tracks ==

===Current Tracks===
''New tracks are added as new research needs are identified, this list is current for TREC 2014.''
* Contextual Suggestion Track - '''Goal:''' to investigate search techniques for complex information needs that are highly dependent on context and user interests.
* Clinical Decision Support Track - '''Goal:''' to investigate techniques for linking medical cases to information relevant for patient care
* Federated Web Search Track - '''Goal:''' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.
* Knowledge Base Acceleration Track - '''Goal:''' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.
* [[Microblog]] Track - '''Goal:''' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. 
* Session Track - '''Goal:''' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.
* Temporal Summarization Track - '''Goal:''' to develop systems that allow users to efficiently monitor the information associated with an event over time.
* Web Track - '''Goal:''' to explore information seeking behaviors common in general web search.

===''Past tracks''===
* Chemical Track - '''Goal:''' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.
* [[Crowdsourcing]] Track - '''Goal:''' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. 
* [[TREC Genomics|Genomics Track]] - '''Goal:''' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.
* [[Enterprise search|Enterprise Track]] - '''Goal:''' to study search over the data of an organization to complete some task. Last ran on TREC 2008.
* Entity Track - '''Goal:''' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.
* [[Cross-language information retrieval|Cross-Language]] Track - '''Goal:''' to investigate the ability of retrieval systems to find documents topically regardless of source language.
* [[Federated search|FedWeb]] Track - '''Goal:''' to select best resources to forward a query to, and merge the results so that most relevant are on the top.
* Filtering Track - '''Goal:''' to binarily decide retrieval of new incoming documents given a stable [[information need]].
* HARD Track - '''Goal:''' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.
* Interactive Track - '''Goal:''' to study user [[Human-computer interaction|interaction]] with text retrieval systems.
* Legal Track - '''Goal:''' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.
* Medical Records Track - '''Goal:''' to explore methods for searching unstructured information found in patient medical records. 
* Novelty Track - '''Goal:''' to investigate systems' abilities to locate new (i.e., non-redundant) information.
* [[Question answering|Question Answering]] Track - '''Goal:''' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.
* Robust Retrieval Track - '''Goal:''' to focus on individual topic effectiveness.
* [[Relevance feedback|Relevance Feedback]] Track - '''Goal:''' to further deep evaluation of relevance feedback processes.
* [[Spam (electronic)|Spam]] Track - '''Goal:''' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.
* [[Terabyte]] Track - '''Goal:''' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.
* [[Video search engine|Video]] Track - '''Goal:''' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].
:In 2003, this track became its own independent evaluation named [[TRECVID]].

===Related Events===
In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).

== Conference Contributions ==
&lt;!-- contributions of conference to research/IR community --&gt;
NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.&lt;ref&gt;[http://trec.nist.gov/overview.html From TREC homepage: &quot;... effectiveness approximately doubled in the first six years of TREC&quot;]&lt;/ref&gt; The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world's commercial [[search engine]]s.  An independent report by RTII found that &quot;about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia.&quot;
&lt;ref&gt;{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}&lt;/ref&gt;
&lt;ref&gt;http://www.nist.gov/director/planning/upload/report10-1.pdf&lt;/ref&gt;

While one study suggests that the state of the art for ad hoc search  has not advanced substantially in the past decade,&lt;ref&gt;Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don't add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.&lt;/ref&gt; it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.

The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.

TREC systems often provide a baseline for further research.  Examples include:
* [[Hal Varian]], Chief Economist at [[Google]], says ''Better data makes for better science. The history of information retrieval illustrates this principle well,&quot; and describes TREC's contribution.&lt;ref&gt;[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]&lt;/ref&gt;
* TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.&lt;ref&gt;[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]&lt;/ref&gt;
* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world's best [[Jeopardy!]] players,&lt;ref&gt;[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]&lt;/ref&gt; used data and systems from TREC's QA Track as baseline performance measurements.&lt;ref&gt;[http://www.aaai.org/AITopics/articles&amp;columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. '''Building Watson:  An Overview of the DeepQA Project''']&lt;/ref&gt;

== Participation ==
The conference is made up of a varied, international group of researchers and developers.&lt;ref&gt;{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}&lt;/ref&gt;&lt;ref&gt;http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}&lt;/ref&gt; In 2003, there were 93 groups from both academia and industry from 22 countries participating.

==References==
{{reflist}}

== External links ==
*[http://trec.nist.gov/ TREC website at NIST]
*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]
*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]

[[Category:Information retrieval]]
[[Category:Computational linguistics]]
[[Category:Natural language processing]]
[[Category:Computer science competitions]]</text>
      <sha1>sgiyavgwq9urz1ukwhsojpcbc1kx6ia</sha1>
    </revision>
  </page>
  <page>
    <title>Scientific data archiving</title>
    <ns>0</ns>
    <id>10022970</id>
    <revision>
      <id>609509468</id>
      <parentid>609509370</parentid>
      <timestamp>2014-05-21T11:15:44Z</timestamp>
      <contributor>
        <username>Ruecknaj</username>
        <id>20732921</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12758">'''Scientific data archiving''' is  the [[Computer_data_storage#Volatility|long-term storage]] of [[scientific data]] and methods. The various scientific journals have differing policies regarding how much of their data and methods scientists are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.

[[Data archiving]] is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.

The requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].&lt;ref&gt;”Policy on Referencing Data in and Archiving Data for AGU Publications” [http://www.agu.org/pubs/authors/policies/data_policy.shtml]&lt;/ref&gt; This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of &quot;data papers&quot;; and it establishes AGU's role in maintaining data archives. But it makes no requirements on paper authors to archive their data.

Prior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The science community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.

The need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.&lt;ref&gt;&quot;The Case for Due Diligence When Empirical Research is Used in Policy Formation&quot; by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]&lt;/ref&gt;&lt;ref&gt;[http://gking.harvard.edu/replication.shtml &quot;Data Sharing and Replication&quot; a website by Gary King]&lt;/ref&gt;

==Selected policies by journals==

===The American Naturalist===
{{quote|[[The American Naturalist]]'' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR&lt;ref&gt;[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]&lt;/ref&gt;}}

===Journal of Heredity===
{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, ''Journal of Heredity'' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.

The American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org&lt;ref&gt;[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]&lt;/ref&gt;}}

===Molecular Ecology===
{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley&lt;ref&gt;[http://www.wiley.com/bw/submit.asp?ref=0962-1083&amp;site=1 Policy on data archiving]&lt;/ref&gt;}}

===Nature===
{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the ''Nature'' journal at submission, either uploaded via the journal's online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author's personal or institutional web site.&lt;ref&gt;[http://www.nature.com/authors/editorial_policies/availability.html &quot;Availability of Data and Materials: The Policy of Nature Magazine]&lt;/ref&gt;

''Nature'' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: &quot;Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]&lt;ref&gt;{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}&lt;/ref&gt;}}

===''Science''===
{{quote|''Science'' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.&lt;ref&gt;[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep &quot;General Policies of Science Magazine&quot;]&lt;/ref&gt;

&quot;Materials and methods&quot; – ''Science'' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]&lt;ref&gt;[http://www.sciencemag.org/about/authors/prep/prep_online.dtl ”Preparing Your Supporting Online Material”]&lt;/ref&gt;}}

== Royal Society Publishing==
{{quote|As a condition of acceptance authors agree to honour any reasonable request by other researchers for materials, methods, or data necessary to verify the conclusion of the article. Supplementary data up to 10Mb is placed on the Society's website free of charge and is publicly accessible. Large datasets must be deposited in a recognised public domain database by the author prior to submission. The accession number should be provided for inclusion in the published article.|{{citation needed |date=September 2013}}}}

==Policies by funding agencies==
In the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.&lt;ref&gt;[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html ”NSF to Ask Every Grant Applicant for Data Management Plan”]&lt;/ref&gt;

The NSF [[Datanet]] initiative has resulted in funding of the '''Data Observation Network for Earth''' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE's stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.

==Data archives==
The following list refers to scientific data archives. See [[Data archive]] for social science archives. 
* [[CISL Research Data Archive]]
* [[Dryad (repository)|Dryad]]
* [[ESO/ST-ECF Science Archive Facility]]
* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]
* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]
* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]
* [[National Archive of Computerized Data on Aging]]
* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]
* [[National Climatic Data Center]]
* [[National Geophysical Data Center]]
* [[National Snow and Ice Data Center]]
* [[National Oceanographic Data Center]]
* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]
* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth &amp; Environmental Science]]
* [[World Data Center]]
* [[DataONE]]

==References==
{{Reflist}}

==External links==
* [[Registry of Research Data Repositories]] ''re3data.org'' [http://service.re3data.org/search/results?term=]
* Statistical checklist required by ''Nature'' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]
* Policies of ''Proceedings of the National Academy of Sciences (U.S.)'' [http://www.pnas.org/misc/iforc.shtml#policies]
* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]
* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]
* Data sharing and replication – Gary King website [http://gking.harvard.edu/replication.shtml]
* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]
* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]
* “How to encourage the right behaviour” An opinion piece published in ''Nature'',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]
* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]
* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]
* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]
[[Category:Information retrieval]]
[[Category:Knowledge representation]]</text>
      <sha1>ik6j1lta7dolddce7u4owev3cn50w40</sha1>
    </revision>
  </page>
  <page>
    <title>GLIMPSE</title>
    <ns>0</ns>
    <id>465038</id>
    <revision>
      <id>627205860</id>
      <parentid>627205374</parentid>
      <timestamp>2014-09-26T21:03:45Z</timestamp>
      <contributor>
        <username>Derek farn</username>
        <id>752956</id>
      </contributor>
      <comment>Updated category list</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2260">{{Other uses|Glimpse (disambiguation)}}
{{Infobox software
| name = Glimpse
| logo = 
| screenshot =
| caption =
| developer = [[Internet WorkShop]]
| status = 
| latest release version = 4.18.6 (source) / 4.18.5 (binary) 
| latest release date = {{release date|2012|06|09}}
| operating system = [[Cross-platform]]
| programming language = [[C (programming language)|C]]
| genre = [[Search algorithm|Search]] and [[index (search engine)|index]]
| license = 
| website = {{URL|http://webglimpse.net/}}
}}
'''GLIMPSE''' is a text indexing and [[text retrieval|retrieval]] [[software]] program originally developed at the [[University of Arizona]] by [[Udi Manber]], [[Sun Wu]], and [[Burra Gopal]].  It was released under the ISC [[open source]] license in September 2014.

GLIMPSE stands for GLobal IMPlicit SEarch. While many text indexing schemes create quite large indexes (usually around 50% of the size of the original text), a GLIMPSE-created index is only 2-4% of the size of the original text.

GLIMPSE uses and takes a great deal of inspiration from [[Agrep]], which was also developed at the University of Arizona, but GLIMPSE uses a high level index whereas Agrep parses all the text each time.

The basic algorithm is similar to other text indexing and retrieval engines, except that the text records in the index are huge, consisting of multiple files each. This index is searched using a boolean matching algorithm like most other text indexing and retrieval engines. After one or more of these large text records is matched, Agrep is used to actually scan for the exact text desired. While this is slower than traditional totally indexed approaches, the advantage of the smaller index is seen to be advantageous to the individual user. This approach would not work particularly well across websites, but it would work reasonably well for a single site, or a single workstation. In addition, the smaller index can be created more quickly than a full index.

==References==
{{Reflist}}

==External links==
*[http://webglimpse.net/ Glimpse and WebGlimpse home page]
*[http://webglimpse.net/pubs/glimpse.pdf Original Glimpse paper] (PDF)

[[Category:Information retrieval]]
[[Category:Free search engine software]]
[[Category:Search engine software]]</text>
      <sha1>drzto85k5z3eqz08hppsh9i2wsepuc6</sha1>
    </revision>
  </page>
  <page>
    <title>SimRank</title>
    <ns>0</ns>
    <id>19518308</id>
    <revision>
      <id>645598900</id>
      <parentid>622463846</parentid>
      <timestamp>2015-02-04T13:21:26Z</timestamp>
      <contributor>
        <ip>2001:630:12:10C0:8DAE:9FA3:A695:79C</ip>
      </contributor>
      <comment>/* Further research on SimRank */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12176">'''SimRank''' is a general [[Semantic similarity|similarity measure]], based on a simple and intuitive [[Graph theory|graph-theoretic model]].
SimRank is applicable in any [[Domain model|domain]] with object-to-object [[Relation (mathematics)|relationships]], that measures similarity of the structural context in which objects occur, based on their relationships with other objects.
Effectively, SimRank is a measure that says &quot;'''two objects are considered to be similar if they are referenced by similar objects'''.&quot;

== Introduction ==

Many [[Application software|applications]] require a measure of &quot;similarity&quot; between objects.
One obvious example is the &quot;find-similar-document&quot; query,
on traditional text corpora or the [[World Wide Web|World-Wide Web]].
More generally, a similarity measure can be used to [[Cluster analysis|cluster objects]], such as for [[collaborative filtering]] in a [[recommender system]], in which “similar” users and items are grouped based on the users’ preferences.

Various aspects of objects can be used to determine similarity, usually depending on the domain and the appropriate definition of similarity for that domain.
In a [[Text corpus|document corpus]], matching text may be used, and for collaborative filtering, similar users may be identified by common preferences.
SimRank is a general approach that exploits the object-to-object relationships found in many domains of interest.
On the [[World Wide Web|Web]], for example, two pages are related if there are [[hyperlink]]s between them.
A similar approach can be applied to scientific papers and their citations, or to any other document corpus with [[cross-reference]] information.
In the case of recommender systems, a user’s preference for an item constitutes a relationship between the user and the item.
Such domains are naturally modeled as [[Graph (mathematics)|graphs]], with [[Vertex (graph theory)|nodes]] representing objects and [[Edge (graph theory)#Graph|edges]] representing relationships.

The intuition behind the SimRank algorithm is that, in many domains, '''similar objects are referenced by similar objects'''.
More precisely, objects &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are considered to be similar if they are pointed from objects &lt;math&gt;c&lt;/math&gt; and &lt;math&gt;d&lt;/math&gt;, respectively, and &lt;math&gt;c&lt;/math&gt; and &lt;math&gt;d&lt;/math&gt; are themselves similar.
The [[Recursion (computer science)#Recursive programming|base case]] is that objects are maximally similar to themselves
.&lt;ref name=jeh_widom&gt;G. Jeh and J. Widom. SimRank: A Measure of Structural-Context Similarity. In [[SIGKDD|KDD'02]]: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 538-543. [[Association for Computing Machinery|ACM Press]], 2002. [http://www-cs-students.stanford.edu/~glenj/simrank.pdf]&lt;/ref&gt;

It is important to note that SimRank is a general algorithm that determines only the similarity of structural context.
SimRank applies to any domain where there are enough relevant relationships between objects to base at least some notion of similarity on relationships.
Obviously, similarity of other domain-specific aspects are important as well; these can — and should be combined with relational structural-context similarity for an overall similarity measure.
For example, for [[Web page]]s SimRank can be combined with traditional textual similarity; the same idea applies to scientific papers or other document corpora.
For recommendation systems, there may be built-in known similarities between items (e.g., both computers, both clothing, etc.), as well as similarities between users (e.g., same gender, same spending level).
Again, these similarities can be combined with the similarity scores that are computed based on preference patterns, in order to produce an overall similarity measure.

== Basic SimRank equation ==

For a node &lt;math&gt;v&lt;/math&gt; in a directed graph, we denote by &lt;math&gt;I(v)&lt;/math&gt; and &lt;math&gt;O(v)&lt;/math&gt; the set of in-neighbors and out-neighbors of &lt;math&gt;v&lt;/math&gt;, respectively.
Individual in-neighbors are denoted as &lt;math&gt;I_i(v)&lt;/math&gt;, for &lt;math&gt;1 \le i \le \left|I(v)\right|&lt;/math&gt;, and individual
out-neighbors are denoted as &lt;math&gt;O_i(v)&lt;/math&gt;, for &lt;math&gt;1 \le i \le \left|O(v)\right|&lt;/math&gt;.

Let us denote the similarity between objects &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; by &lt;math&gt;s(a, b) \in [0, 1]&lt;/math&gt;. 
Following the earlier motivation, a recursive equation is written for &lt;math&gt;s(a, b)&lt;/math&gt;.
If &lt;math&gt;a = b&lt;/math&gt; then &lt;math&gt;s(a, b)&lt;/math&gt; is defined to be &lt;math&gt;1&lt;/math&gt;.
Otherwise,
:&lt;math&gt;s(a, b) = \frac{C}{\left|I(a)\right| \left|I(b)\right|}
 \sum_{i=1}^{\left|I(a)\right|}\sum_{j=1}^{\left|I(b)\right|}
 s(I_i(a), I_j(b))&lt;/math&gt;
where &lt;math&gt;C&lt;/math&gt; is a constant between &lt;math&gt;0&lt;/math&gt; and &lt;math&gt;1&lt;/math&gt;.
A slight technicality here is that either &lt;math&gt;a&lt;/math&gt; or &lt;math&gt;b&lt;/math&gt; may not have any in-neighbors.
Since there is no way to infer any similarity between &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; in this case, similarity is set to &lt;math&gt;s(a, b) = 0&lt;/math&gt;, so the summation in the above equation is defined to be &lt;math&gt;0&lt;/math&gt; when &lt;math&gt;I(a) = \emptyset&lt;/math&gt; or &lt;math&gt;I(b) = \emptyset&lt;/math&gt;.

== Matrix representation of SimRank ==

Let &lt;math&gt;\mathbf{S}&lt;/math&gt; be the similarity matrix whose entry &lt;math&gt;[\mathbf{S}]_{a,b}&lt;/math&gt; denotes the similarity score &lt;math&gt;s(a,b)&lt;/math&gt;, and &lt;math&gt;\mathbf{A}&lt;/math&gt; be the column normalized adjacency matrix whose entry &lt;math&gt;[\mathbf{A}]_{a,b}=\tfrac{1}{|\mathcal{I}(b)|}&lt;/math&gt; if there is an edge from &lt;math&gt;a&lt;/math&gt; to &lt;math&gt;b&lt;/math&gt;, and 0 otherwise. Then, in matrix notations, SimRank can be formulated as

:&lt;math&gt;
   {{\mathbf{S}}}= \max\{C\cdot (\mathbf{A}^{T} \cdot {{\mathbf{S}}}\cdot {{\mathbf{A}}} ) , {{\mathbf{I}}}\},&lt;/math&gt;

where &lt;math&gt;\mathbf{I}&lt;/math&gt; is an identity matrix.

== Computing SimRank ==

A solution to the SimRank equations for a graph &lt;math&gt;G&lt;/math&gt; can be reached by [[Iterative method|iteration]] to a [[Fixed point (mathematics)|fixed-point]].
Let &lt;math&gt;n&lt;/math&gt; be the number of nodes in &lt;math&gt;G&lt;/math&gt;.
For each iteration &lt;math&gt;k&lt;/math&gt;, we can keep &lt;math&gt;n^2&lt;/math&gt; entries &lt;math&gt;s_k(*, *)&lt;/math&gt;, where &lt;math&gt;s_k(a, b)&lt;/math&gt; gives the score between &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; on iteration &lt;math&gt;k&lt;/math&gt;.
We successively compute &lt;math&gt;s_{k+1}(*, *)&lt;/math&gt; based on &lt;math&gt;s_k(*, *)&lt;/math&gt;.
We start with &lt;math&gt;s_0(*, *)&lt;/math&gt; where each &lt;math&gt;s_0(a, b)&lt;/math&gt; is a lower bound on the actual SimRank score &lt;math&gt;s(a, b)&lt;/math&gt;:
:&lt;math&gt; s_0(a, b) =
 \begin{cases}
  1 \mbox{  } , \mbox{    } \mbox{if } a = b  \mbox{  } , \\
  0 \mbox{  } , \mbox{    } \mbox{if } a \neq b \mbox{  } .
 \end{cases}&lt;/math&gt;

To compute &lt;math&gt;s_{k+1}(a, b)&lt;/math&gt; from &lt;math&gt;s_k(*, *)&lt;/math&gt;, we use the basic SimRank equation to get:
:&lt;math&gt;s_{k + 1}(a, b) = 
 \frac{C}{\left|I(a)\right| \left|I(b)\right|}
 \sum_{i=1}^{\left|I(a)\right|}\sum_{j=1}^{\left|I(b)\right|}
  s_k(I_i(a), I_j(b))&lt;/math&gt;
for &lt;math&gt;a \ne b&lt;/math&gt;, and &lt;math&gt;s_{k+1}(a, b) = 1&lt;/math&gt; for &lt;math&gt;a = b&lt;/math&gt;.
That is, on each iteration &lt;math&gt;k + 1&lt;/math&gt;, we update the similarity of &lt;math&gt;(a, b)&lt;/math&gt; using the similarity scores of the neighbours of &lt;math&gt;(a, b)&lt;/math&gt; from the previous iteration &lt;math&gt;k&lt;/math&gt; according to the basic SimRank equation.
The values &lt;math&gt;s_k(*, *)&lt;/math&gt; are [[Monotonic function|nondecreasing]] as &lt;math&gt;k&lt;/math&gt; increases.
It was shown in &lt;ref name=&quot;jeh_widom&quot;/&gt; that the values [[Limit of a sequence|converge]] to [[Limit of a sequence|limits]] satisfying the basic SimRank equation, the SimRank scores &lt;math&gt;s(*, *)&lt;/math&gt;, i.e., for all &lt;math&gt;a, b \in V&lt;/math&gt;, &lt;math&gt;\lim_{k \to \infty} s_k(a, b) = s(a, b)&lt;/math&gt;.

The original SimRank proposal suggested choosing the decay factor &lt;math&gt;C = 0.8&lt;/math&gt; and a fixed number &lt;math&gt;K = 5&lt;/math&gt; of iterations to perform.
However, the recent research &lt;ref name=&quot;lizorkin&quot;&gt;D. Lizorkin, P. Velikhov, M. Grinev and D. Turdakov. Accuracy Estimate and Optimization Techniques for
SimRank Computation. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 422--433. [http://modis.ispras.ru/Lizorkin/Publications/simrank_accuracy.pdf]&lt;/ref&gt; showed that the given values for &lt;math&gt;C&lt;/math&gt; and &lt;math&gt;K&lt;/math&gt; generally imply relatively low [[Accuracy and precision|accuracy]] of iteratively computed SimRank scores.
For guaranteeing more accurate computation results, the latter paper suggests either using a smaller decay factor (in particular, &lt;math&gt;C = 0.6&lt;/math&gt;) or taking more iterations.

== Partial Sums Memoization ==

The recent work of Lizorkin et al.&lt;ref name=&quot;lizorkin&quot;/&gt; proposed three optimization techniques for speeding up the computation of SimRank:

(1) Essential nodes selection may eliminate the computation of a fraction of node pairs with a-priori zero scores.

(2) Partial sums memoization can effectively reduce repeated calculations of the similarity among different node pairs by caching part of similarity summations for later reuse.

(3) A threshold setting on the similarity enables a further reduction in the number of node pairs to be computed. 

In particular, the second observation of partial sums memoization plays a paramount role in greatly speeding up the computation of SimRank from &lt;math&gt;O(Kd^2n^2)&lt;/math&gt; to &lt;math&gt;O(Kdn^2)&lt;/math&gt;, where &lt;math&gt;K&lt;/math&gt; is the number of iterations, &lt;math&gt;d&lt;/math&gt; is average degree of a graph, and &lt;math&gt;n&lt;/math&gt; is the number of nodes in a graph. The central idea of partial sums memoization consists of two steps:

First, the partial sums over &lt;math&gt;{\mathcal I}(a)&lt;/math&gt; are memoized as
:&lt;math&gt;
Partial_{{\mathcal I}(a)}^{s_{k}}(j)=\sum_{i\in{\mathcal I}(a)}s_{k}(i,j), \qquad (\forall j \in {\mathcal I}(b))
&lt;/math&gt;
and then &lt;math&gt;s_{k+1} (a,b)&lt;/math&gt; is iteratively computed from &lt;math&gt;Partial_{{\mathcal I}(a)}^{s_{k}}(j)&lt;/math&gt; as
:&lt;math&gt;
s_{k+1}( a,b )=\tfrac{C}{| \mathsf{\mathcal{I}}( a ) | | \mathsf{\mathcal{I}}( b ) |}\sum_{j \in \mathsf{\mathcal{I}}( b ) } Partial_{{\mathcal I}(a)}^{s_{k}}(j).
&lt;/math&gt;
Consequently, the results of &lt;math&gt;Partial_{{\mathcal I}(a)}^{s_{k}}(j)&lt;/math&gt;, &lt;math&gt;\forall j \in {\mathcal I}(b)&lt;/math&gt;,
can be reused later when we compute the similarities &lt;math&gt;s_{k+1}(a,*)&lt;/math&gt; for a given vertex &lt;math&gt;a&lt;/math&gt; as the first argument.

== Further research on SimRank ==

* Fogaras and Racz &lt;ref name=&quot;fogaras_racz&quot;&gt;D. Fogaras and B. Racz. Scaling link-based similarity search. In [[World Wide Web Conference|WWW '05]]: Proceedings of the 14th international conference on World Wide Web, pages 641--650, New York, NY, USA, 2005. [[Association for Computing Machinery|ACM]]. [http://www2005.org/docs/p641.pdf]&lt;/ref&gt; suggested speeding up SimRank computation through [[Probability theory|probabilistic]] computation using the [[Monte Carlo method]].

* Antonellis et al.&lt;ref name=&quot;simrank_plusplus&quot;&gt;I. Antonellis, H. Garcia-Molina and C.-C. Chang. Simrank++: Query Rewriting through Link Analysis of the Click Graph. In [[Very large database|VLDB '08]]: Proceedings of the 34th International Conference on Very Large Data Bases, pages 408--421. [http://dbpubs.stanford.edu/pub/showDoc.Fulltext?lang=en&amp;doc=2008-17&amp;format=pdf&amp;compression=&amp;name=2008-17.pdf]&lt;/ref&gt; extended SimRank equations to take into consideration (i) evidence factor for [[Graph (mathematics)#Properties of graphs|incident nodes]] and (ii) link weights.

* Lizorkin et al.&lt;ref name=&quot;lizorkin&quot;/&gt; proposed several [[Optimization (computer science)|optimization]] techniques for speeding up SimRank iterative computation.

* Yu et al.&lt;ref name=&quot;yu_icde13&quot;&gt;W. Yu, X. Lin, W. Zhang. Towards Efficient SimRank Computation on Large Networks. In [[International Conference on Data Engineering|ICDE '13]]: Proceedings of the 29th IEEE International Conference on Data Engineering, pages 601--612. [http://www.cse.unsw.edu.au/~weirenyu/pubs/icde13.pdf]&lt;/ref&gt; further improved SimRank computation via a fine-grained [[memoization]] method to share small common parts among different partial sums.

== See also ==

* [[PageRank]]

== Citations ==
{{reflist|colwidth=30em}}

[[Category:Information retrieval]]</text>
      <sha1>a81jv6kwtx8vkkaq99fbdu6lnhgyzpd</sha1>
    </revision>
  </page>
  <page>
    <title>Discounted cumulative gain</title>
    <ns>0</ns>
    <id>19542049</id>
    <revision>
      <id>632411909</id>
      <parentid>617452002</parentid>
      <timestamp>2014-11-04T12:44:40Z</timestamp>
      <contributor>
        <username>Gwnbu2012</username>
        <id>21878202</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11025">'''Discounted cumulative gain''' ('''DCG''') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or ''gain'', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.&lt;ref&gt;Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422–446 (2002)&lt;/ref&gt;

== Overview ==

Two assumptions are made in using DCG and its related measures.

# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)
# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than irrelevant documents.

DCG originates from an earlier, more primitive, measure called Cumulative Gain.

=== Cumulative Gain ===

Cumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{CG_{p}} = \sum_{i=1}^{p} rel_{i} &lt;/math&gt;

Where &lt;math&gt;rel_{i}&lt;/math&gt; is the graded relevance of the result at position &lt;math&gt;i&lt;/math&gt;.

The value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document &lt;math&gt;d_{i}&lt;/math&gt; above a higher ranked, less relevant, document &lt;math&gt;d_{j}&lt;/math&gt; does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.

=== Discounted Cumulative Gain ===

The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:&lt;ref name=&quot;stanfordireval&quot;&gt;{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}&lt;/ref&gt;

:&lt;math&gt; \mathrm{DCG_{p}} = rel_1 + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}(i)} &lt;/math&gt;

Previously there has not been shown any theoretically sound justification for using a [[logarithm]]ic reduction factor&lt;ref&gt;{{cite book | title=Search Engines: Information Retrieval in Practice | author=B. Croft, D. Metzler, and T. Strohman |year=2009 | publisher=''Addison Wesley&quot;}}&lt;/ref&gt; other than the fact that it produces a smooth reduction.

An alternative formulation of DCG&lt;ref&gt;Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML '05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363&lt;/ref&gt; places stronger emphasis on retrieving relevant documents:

:&lt;math&gt; \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} &lt;/math&gt;

The latter formula is commonly used in industry including major web search companies&lt;ref name=&quot;stanfordireval&quot;/&gt; and data science competition platform such as Kaggle.&lt;ref&gt;{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}&lt;/ref&gt;

In Croft, Metzler and Strohman (page 320, 2010), the authors mistakenly claim that these two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]]; &lt;math&gt;rel_{i} \in \{0,1\}&lt;/math&gt;.  To see that they are not the same, let there be one relevant document and that relevant document is at rank 2.  The first version of DCG equals 1 / log2(2) = 1.  The second version of DCG equals 1 / log2(2+1) = 0.631.  The way that the two formulations of DCG are the same for binary judgments is in the way gain in the numerator is calculated.  For both formulations of DCG, binary relevance produces gain at rank i of 0 or 1.  No matter the number of relevance grades, the two formulations differ in their discount of gain.

Note that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.

Recently, Wang et al.(2013)&lt;ref&gt;Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of NDCG Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).&lt;/ref&gt; give theoretical guarantee for using the logarithmic reduction factor in NDCG. Specifically, the authors prove for every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets.

=== Normalized DCG ===

Search result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of &lt;math&gt;p&lt;/math&gt; should be normalized across queries. This is done by sorting documents of a result list by relevance, producing the maximum possible DCG till position &lt;math&gt;p&lt;/math&gt;, also called Ideal DCG (IDCG) till that position. For a query, the ''normalized discounted cumulative gain'', or nDCG, is computed as:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG_{p}} &lt;/math&gt;

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.

== Example ==

Presented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning irrelevant, 3 meaning completely relevant, and 1 and 2 meaning &quot;somewhere in between&quot;. For the documents ordered by the ranking algorithm as

:&lt;math&gt; D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} &lt;/math&gt;

the user provides the following relevance scores:

:&lt;math&gt; 3, 2, 3, 0, 1, 2 &lt;/math&gt;

That is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:

:&lt;math&gt; \mathrm{CG_{6}} = \sum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11&lt;/math&gt;

Changing the order of any two documents does not affect the CG measure. If &lt;math&gt;D_3&lt;/math&gt; and &lt;math&gt;D_4&lt;/math&gt; are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:

{| class=&quot;wikitable&quot; border=&quot;1&quot;
|-
! &lt;math&gt;i&lt;/math&gt;
! &lt;math&gt;rel_{i}&lt;/math&gt;
! &lt;math&gt;\log_{2}i&lt;/math&gt;
! &lt;math&gt; \frac{rel_{i}}{\log_{2}i} &lt;/math&gt;
|-
| 1
| 3
| 0
| N/A
|-
| 2
| 2
| 1
| 2
|-
| 3
| 3
| 1.585
| 1.892
|-
| 4
| 0
| 2.0
| 0
|-
| 5
| 1
| 2.322
| 0.431
|-
| 6
| 2
| 2.584
| 0.774
|}

So the &lt;math&gt;DCG_{6}&lt;/math&gt; of this ranking is:

:&lt;math&gt; \mathrm{DCG_{6}} = rel_{1} + \sum_{i=2}^{6} \frac{rel_{i}}{\log_{2}i} = 3 + (2 + 1.892 + 0 + 0.431 + 0.774) = 8.10&lt;/math&gt;

Now a switch of &lt;math&gt;D_3&lt;/math&gt; and &lt;math&gt;D_4&lt;/math&gt; results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.

The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.

To normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:

:&lt;math&gt; 3, 3, 2, 2, 1, 0 &lt;/math&gt;

The DCG of this ideal ordering, or ''IDCG'', is then:

:&lt;math&gt; \mathrm{IDCG_{6}} = 8.69 &lt;/math&gt;

And so the nDCG for this query is given as:

:&lt;math&gt; \mathrm{nDCG_{6}} = \frac{DCG_{6}}{IDCG_{6}} = \frac{8.10}{8.69} = 0.932 &lt;/math&gt;

== Limitations ==
# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores &lt;math&gt; 1,1,1 &lt;/math&gt; and &lt;math&gt; 1,1,1,0 &lt;/math&gt; respectively, both would be considered equally good even if later contains a bad result. One way to take into account this limitation is use &lt;math&gt;1 - 2^{rel_{i}}&lt;/math&gt; in numerator for scores for which we want to penalize and &lt;math&gt;2^{rel_{i}} - 1&lt;/math&gt; for all others. For example, for the ranking judgments &lt;math&gt;Excellent, Fair, Bad&lt;/math&gt; one might use numerical scores &lt;math&gt;1,0,-1&lt;/math&gt; instead of &lt;math&gt;2,1,0&lt;/math&gt;.
# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores &lt;math&gt; 1,1,1 &lt;/math&gt; and &lt;math&gt; 1,1,1,1,1 &lt;/math&gt; respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores &lt;math&gt; 1,1,1,0,0 &lt;/math&gt; and &lt;math&gt; 1,1,1,1,1 &lt;/math&gt; and quote nDCG as nDCG@5.
# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as &quot;restaurants&quot; nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.

== References ==
{{Reflist|1}}

[[Category:Information retrieval|*]]</text>
      <sha1>djqdnigpnv6movlh1dq27kfwot1fmtj</sha1>
    </revision>
  </page>
  <page>
    <title>Expertise finding</title>
    <ns>0</ns>
    <id>20227676</id>
    <revision>
      <id>625287212</id>
      <parentid>618327325</parentid>
      <timestamp>2014-09-12T20:41:02Z</timestamp>
      <contributor>
        <username>DexDor</username>
        <id>11025703</id>
      </contributor>
      <comment>/* References */ rm portal category tag from article (not portal page)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10938">{{cleanup|date=November 2010}}{{External links|date=January 2012}}

'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.

== Importance of expertise ==

It can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and “licensing” expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.

Until very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one’s judgment about those individuals is justified and that their answers are thoughtful.

In the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed “expertise locating systems”.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|“recommender systems”]].

At the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.

Still other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed “gated objects”, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.

Examples of the systems outlined above are listed in Table 1.

'''Table 1: A classification of expertise location systems'''

{| class=&quot;wikitable&quot; border=&quot;1&quot;
|-
! Type
! Application domain
! Data source
! Examples
|-
| Social networking
| Professional networking
| User-generated
|
* [[LinkedIn]]
|-
| [[Scientific literature]]
| Identifying publications with strongest research impact
| Third-party generated
|
* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]
|-
| [[Scientific literature]]
| Expertise search
| Software
|
* [[Arnetminer]][http://arnetminer.org]
|-
| Knowledge base
| Private expertise database
| User-Generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* Decisiv Search Matters &amp; Expertise ([[Recommind (software company)|Recommind]], Inc.)
* [[Tacit Software]] (Oracle Corporation)
|-
| Knowledge base
| Publicly accessible expertise database
| User-generated
|
* [[Community of Science]] Expertise [http://expertise.cos.com]
* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]
|-
| Knowledge base
| Private expertise database
| Third party-generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* MindServer Expertise ([[Recommind]], Inc.)
* Tacit Software
|-
| Knowledge base
| Publicly accessible expertise database
| Third party-generated
|
* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)
* [http://authoratory.com/ authoratory.com]
* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)
* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)
* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)
* [http://researchcrossroads.org ResearchCrossroads.org] (Innolyst, Inc.)
|-
| Blog [[search engine]]s
|
| Third party-generated
|
* [[Technorati]] [http://technorati.com/]
|}

== Technical problems ==
A number of interesting problems follow from the use of expertise finding systems:

* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.
* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).
* Finding ways to avoid “gaming” of the system to reap unjustified expertise [[credibility]].

== Expertise ranking ==

Means of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:

* How can expertise be assessed objectively? Is that even possible?
* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?
* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?
* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?

== Sources of data for assessing expertise ==
Many types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure &quot;raw&quot; contributions provided by the expert, or whether some sort of filter is applied to these contributions.

Unfiltered data sources that have been used to assess expertise, in no particular ranking order:

* user recommendations
* help desk tickets: what the problem was and who fixed it
* e-mail traffic between users
* documents, whether private or on the web, particularly publications
* user-maintained web pages
* reports (technical, marketing, etc.)

Filtered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:

* [[patent]]s, particularly if issued
* scientific publications
* issued grants (failed grant proposals are rarely know beyond the authors)
* [[clinical trial]]s
* product launches
* pharmaceutical drugs

== Approaches for creating expertise content ==
* Manual, either by experts themselves (e.g., LinkedIn) or by a curator
* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard])

== Interesting expertise systems over the years ==
In no particular order...

* Autonomy's IDOL
* AskMe
* Tacit Knowledge Systems' ActiveNet
* Triviumsoft's SEE-K
* MIT’s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)
* MITRE’s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]
* MITRE’s XpertNet
* Arnetminer (ref 2)
* Dataware II Knowledge Directory
* Thomson’s tool
* Hewlett-Packard’s CONNEX
* Microsoft’s SPUD project
* [http://www.xperscore.com Xperscore]
* [http://intunex.fi/skillhive/ Skillhive]

== Conferences ==
# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]

== References ==

# Ackerman, Mark and McDonald, David (1998) &quot;Just Talk to Me: A Field Study of Expertise Location&quot; ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.
# Hughes, Gareth and Crowder, Richard (2003) &quot;Experiences in designing highly adaptable expertise finder systems&quot;  ''Proceedings of the DETC Conference 2003''.
# Maybury, M., D’Amore, R., House, D. (2002). &quot;Awareness of organizational expertise.&quot; ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.
# Maybury, M., D’Amore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.
# Maybury, M., D’Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.
# Maybury, M., D’Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.
# Mattox, D., M. Maybury, ''et al.'' (1999). &quot;Enterprise expert and knowledge discovery&quot;. ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.
# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) &quot;ArnetMiner: extraction and mining of academic social networks&quot; ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.
# Viavacqua, A. (1999). &quot;Agents for expertise location&quot;. ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.

[[Category:Evaluation methods]]
[[Category:Metrics]]
[[Category:Analysis]]
[[Category:Impact assessment]]
[[Category:Intellectual works]]
[[Category:Knowledge sharing]]
[[Category:Library science]]
[[Category:Information retrieval]]
[[Category:Science studies]]</text>
      <sha1>8n0yjv4y6sstuqy01xff92htwhb5ri4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Sound production technology</title>
    <ns>14</ns>
    <id>2028879</id>
    <revision>
      <id>588963136</id>
      <parentid>544039036</parentid>
      <timestamp>2014-01-03T11:45:58Z</timestamp>
      <contributor>
        <username>BotMultichill</username>
        <id>4080734</id>
      </contributor>
      <minor/>
      <comment>Adding Commons category link to [[:Commons:Category:Sound production technology|category with the same name]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="166">{{Commons category|Sound production technology}}
[[Category:Audio electronics|Production]]
[[Category:Information retrieval]]
[[Category:Sound production|Technology]]</text>
      <sha1>lg7zan1jrter1w2ruydjx7zewp59kjw</sha1>
    </revision>
  </page>
  <page>
    <title>Cranfield Experiments</title>
    <ns>0</ns>
    <id>20289869</id>
    <revision>
      <id>545498473</id>
      <parentid>496678222</parentid>
      <timestamp>2013-03-19T19:41:43Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q5182087]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1048">The '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.&lt;ref&gt;Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.&lt;/ref&gt;&lt;ref&gt;Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.&lt;/ref&gt;&lt;ref&gt;Cleverdon, C. W., &amp; Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. 
&lt;/ref&gt; 

They represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).

==See also==
*[[ASLIB]]
*[[Information history]]

==References==
{{Reflist}}

[[Category:Experiments]]
[[Category:Information retrieval]]


{{database-stub}}</text>
      <sha1>lxzp4cy4m9h0misvcy11msk3t47g3hz</sha1>
    </revision>
  </page>
  <page>
    <title>Communication engine</title>
    <ns>0</ns>
    <id>20895417</id>
    <revision>
      <id>601817015</id>
      <parentid>572075831</parentid>
      <timestamp>2014-03-29T15:39:12Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10067)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="777">{{Orphan|date=February 2009}}
A '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.

[[Category:Information retrieval]]
[[Category:Computing terminology]]


{{Web-stub}}</text>
      <sha1>ej2f7z4f351uelw7iav6pk37iohq64g</sha1>
    </revision>
  </page>
  <page>
    <title>Subject indexing</title>
    <ns>0</ns>
    <id>13200719</id>
    <revision>
      <id>646588178</id>
      <parentid>622419779</parentid>
      <timestamp>2015-02-11T02:00:55Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17088">'''Subject indexing''' is the act of describing or [[Document classification|classifying]] a [[document]] by [[keyword (search)|index terms]] or other symbols in order to indicate what the document is '''[[Aboutness|about]],''' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]].  In other words, it is about identifying and describing the '''[[Subject (documents)|subject]]''' of documents.  Indexes are constructed, separately, on three distinct levels:  terms in a document such as a book;  objects in a collection such as a library;  and documents (such as books and articles) within a field of knowledge.

Subject indexing is used in [[information retrieval]] especially to create [[bibliographic database]]s to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.

The process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].&lt;ref name=&quot;Lancaster2003a&quot;&gt;F. W. Lancaster (2003): &quot;Indexing and abstracting in theory and practise&quot;. Third edition. London, Facet ISBN 1-85604-482-3. page 6&lt;/ref&gt; The terms in the index are then presented in a systematic order.

Indexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.

== Subject analysis ==
The first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as &quot;Does the document deal with a specific product, condition or phenomenon?&quot;.&lt;ref name=&quot;Chowdhury2004&quot;&gt;G.G. Chowdhury (2004): &quot;Introduction to modern information retrieval&quot;. Third Edition. London, Facet. ISBN 1-85604-480-7. page 71&lt;/ref&gt; As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyse the content differently and so come up with different index terms. This will impact on the success of retrieval.

=== Automatic vs. manual subject analysis ===
Automatic indexing follows set processes of analysing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analysing the full text in depth is costly and time consuming &lt;ref name=&quot;Lancaster2003b&quot;&gt;F. W. Lancaster (2003): &quot;Indexing and abstracting in theory and practise&quot;. Third edition. London, Facet ISBN 1-85604-482-3. page 24&lt;/ref&gt; An automated system takes away the time limit and allows the entire document to be analysed, but also has the option to be directed to particular parts of the document.

== Term selection ==
The second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular.  Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval.  These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]].  The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials.  With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.&lt;ref name=&quot;Voss2007&quot;&gt;
{{cite conference
  | first= Jakob | last = Voss
  | title = Tagging, Folksonomy &amp; Co - Renaissance of Manual Indexing?
  | booktitle = Proceedings of the International Symposium of Information Science
  | pages = 234–254
  | year = 2007
  | arxiv = cs/0701072
}}&lt;/ref&gt;

One application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.

=== Extraction/Derived indexing ===
Extraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words such as the, and would be referred to and such [[stop words]] would be excluded as index terms. Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases.
Automated extraction indexing also has the problem that even with use of a stop-list to remove common words such as “the,” some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques  would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded. Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.&lt;ref name=&quot;Lamb2008&quot;&gt;J. Lamb (2008): ''[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]'' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.&lt;/ref&gt;

=== Assignment indexing ===
An alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.&lt;ref name=&quot;Tenopir&quot;&gt;C. Tenopir (1999): &quot;Human or automated, indexing is important&quot;. ''Library Journal'' '''124'''(18) pages 34-38.&lt;/ref&gt; It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.&lt;ref name=&quot;Chowdhury2004&quot; /&gt;

== Index presentation ==
The final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination &lt;ref name=&quot;Bodoff1998&quot;&gt;D. Bodoff and A. Kambil, (1998): &quot;Partial coordination. I. The best of pre-coordination and post-coordination.&quot; ''Journal of the American Society for Information Science'', '''49'''(14), 1254-1269.&lt;/ref&gt;

== Depth of Indexing ==
Indexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity &lt;ref name=&quot;Cleveland2001&quot;&gt;D.B. Cleveland and A.D. Cleveland (2001): &quot;Introduction to indexing and abstracting&quot;. 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105&lt;/ref&gt;

=== Exhaustivity ===
An exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.&lt;ref name=&quot;Weinberg1999&quot;&gt;B.H. Weinberg (1990): &quot;Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference&quot;. ''Key Words'', '''7'''(5), pages 1+.&lt;/ref&gt; Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.

=== Specificity ===
The specificity describes how closely the index terms match the topics they represent &lt;ref name=&quot;Anderson1997&quot;&gt;J.D. Anderson (1997): ''[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]'' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.&lt;/ref&gt; An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.&lt;ref name=&quot;Cleveland2001b&quot;&gt;D.B. Cleveland and A.D. Cleveland (2001): &quot;Introduction to indexing and abstracting&quot;. 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106&lt;/ref&gt; Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.

==Indexing theory==
[[Birger Hjørland|Hjørland]] (2011)&lt;ref&gt;Hjørland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. ''Journal of the American Society for Information Science and Technology'', 62(1,), 72-77.&lt;/ref&gt; found that theories of indexing is at the deepest level connected to different theories of knowledge:

'''Rationalist theories of indexing''' (such as Ranganathan's theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then &quot;analytic-synthetic&quot;, to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). '''Empiricist theories of indexing''' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques.  '''Historicist and hermeneutical theories of indexing''' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of “relevant” documents by knowing about those different horizons. '''Pragmatic and critical theories of indexing''' (such as Hjørland, 1997)&lt;ref&gt;Hjørland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport &amp; London: Greenwood Press.&lt;/ref&gt; is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by Ørom (2003)&lt;ref&gt;Ørom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.&lt;/ref&gt; and in music by Abrahamsen (2003).&lt;ref&gt;Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169. 
&lt;/ref&gt;

The core of indexing is, as stated by Rowley &amp; Farrow&lt;ref name=rowley2000&gt;Rowley, J. E. &amp; Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company&lt;/ref&gt; to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hjørland (1992,&lt;ref&gt;Hjørland, Birger (1992). The Concept of &quot;Subject&quot; in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF&lt;/ref&gt; 1997) to index its informative potentials.

&quot;In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject  and the nature of the contribution that the document is making to the advancement of knowledge.&quot; (Rowley &amp; Farrow, 2000,&lt;ref name=rowley2000/&gt; p.&amp;nbsp;99).

== See also ==
* [[Indexing and abstracting service]]
* [[Document classification]]
* [[Metadata]]
* [[Overcategorization]]
* [[Thomas of Ireland]], a medieval pioneer in subject indexing

== References ==
&lt;references/&gt;
*{{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}
*{{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81–101|doi=10.1108/eb026855}}

[[Category:Library science]]
[[Category:Information science]]
[[Category:Information retrieval]]</text>
      <sha1>snny34ijmc2m6pmla0ci255y2e392sv</sha1>
    </revision>
  </page>
  <page>
    <title>Relevance feedback</title>
    <ns>0</ns>
    <id>5818361</id>
    <revision>
      <id>646021284</id>
      <parentid>645416806</parentid>
      <timestamp>2015-02-07T11:00:42Z</timestamp>
      <contributor>
        <ip>188.98.197.14</ip>
      </contributor>
      <comment>Undo citation SPAM.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7085">'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or &quot;pseudo&quot; feedback.

== Explicit feedback ==

Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.

Users may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as &quot;not relevant&quot;, &quot;somewhat relevant&quot;, &quot;relevant&quot;, or &quot;very relevant&quot;). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.

The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

A performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].

== Implicit feedback ==

Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf].

The key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:

# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and
# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback

An example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.

== Blind feedback ==

Pseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top &quot;k&quot; ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:

# Take the results returned by initial query as relevant results (only top k with k being between 10 to 50 in most experiments).
# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.
# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.

Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.

This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.&lt;ref&gt;Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.&lt;/ref&gt; Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.&lt;ref&gt;Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.&lt;/ref&gt; 
Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.

Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.

== Using relevance information ==

Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

==Further reading==
*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's
*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''
*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

== References ==
{{reflist|2}}

[[Category:Information retrieval]]

[[zh:相关反馈]]</text>
      <sha1>pqft1gootq77p1hckyhpv8u2prt83c8</sha1>
    </revision>
  </page>
  <page>
    <title>Index term</title>
    <ns>0</ns>
    <id>6118940</id>
    <revision>
      <id>631096551</id>
      <parentid>630389931</parentid>
      <timestamp>2014-10-25T20:06:31Z</timestamp>
      <contributor>
        <username>Qwfp</username>
        <id>6032993</id>
      </contributor>
      <comment>/* Examples */ per [[MOS:ACRO]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3204">An '''index term''', '''subject term''', '''subject heading''', or '''descriptor''', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts also. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.

Keywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it is inefficient to do so. Almost every English-language site on the Internet has the article &quot;''the''&quot;, and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as &quot;the&quot; and &quot;a&quot; from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.

The term &quot;descriptor&quot; was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[thesaurus]]. 

The [[Simple Knowledge Organisation System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].

==Author keywords==
Many journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document ''relative'' to the other documents in the database. Author keywords are an integral part of literature.

==Examples==
*[[Canadian Subject Headings]] (CSH)
*[[Library of Congress Subject Headings]] (LCSH)
*[[Medical Subject Headings]] (MeSH)
*[[Polythematic Structured Subject Heading System]] (PSH)

==See also==
*[[Dynamic keyword insertion]]
&lt;!-- *Key-objects --&gt;
*[[Keyword cloud]]
*[[Keyword density]]
*[[Keyword optimization]]
*[[knowledge tags|Keyword tagging]]
*[[Subject (documents)]]

== References ==
{{commonscat|Information retrieval}}
*{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}

[[Category:Information retrieval]]

{{Library-stub}}</text>
      <sha1>h1lylrrdfxj8fnnlhopx2ytxi2edrtz</sha1>
    </revision>
  </page>
  <page>
    <title>Collaborative search engine</title>
    <ns>0</ns>
    <id>22101925</id>
    <revision>
      <id>637089098</id>
      <parentid>634590898</parentid>
      <timestamp>2014-12-07T23:35:19Z</timestamp>
      <contributor>
        <username>Gilo1969</username>
        <id>8271180</id>
      </contributor>
      <comment>fix citation with wikilinks embedded in URL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14231">{{Recommender systems}}
'''Collaborative search engines''' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].

== Models of collaboration ==

Collaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization
&lt;ref name=Golo2007&gt;{{citation
 | title = Collaborative Exploratory Search
 | year = 2007
 | author = Golovchinsky Gene, Pickens Jeremy
 | journal = Proceedings of HCIR 2007 workshop
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf
}}&lt;/ref&gt; and depth of mediation 
,&lt;ref name=Pickens2008&gt;{{citation
 | title = Collaborative Exploratory Search
 | year = 2008
 | author = Pickens Jeremy, Golovchinsky Gene, Shah Chirag, Qvarfordt Pernilla, Back Maribeth
 | booktitle = SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval
 | pages = 315–322
 | volume = 
 | issue = 
 | doi = 10.1145/1390334.1390389
 | isbn = 
 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389
| chapter = Algorithmic mediation for collaborative exploratory search
 }}&lt;/ref&gt; task vs. trait,&lt;ref name=Morris2008&gt;{{citation
 | contribution = Understanding Groups’ Properties as a Means of Improving Collaborative Search Systems
 | year = 2008
 | author = Morris Meredith, Teevan Jaime
 | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf
}}&lt;/ref&gt; and division of labor and sharing of knowledge.&lt;ref name=Foley2008&gt;{{citation
 | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval
 | year = 2008
 | author = Foley Colum
 | booktitle = PhD Thesis, Dublin City University
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf
}}&lt;/ref&gt;

=== Explicit vs. implicit collaboration ===

Implicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,&lt;ref name=Smith2003&gt;{{citation
 | title = Collaborative Web Search
 | year = 2003
 | author = Barry Smyth, Evelyn Balfe, Peter Briggs, Maurice Coyle, Jill Freyne
 | journal = IJCAI
 | pages = 1417–1419
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,&lt;ref name=Glance2001&gt;{{citation
 | title = Community search assistant
 | year = 2001
 | author = Natalie S. Glance
 | journal = Workshop on AI for Web Search AAAI'02
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; the CSE of Burghardt et al.,&lt;ref name=BurghardtWI2008&gt;{{citation
 | title = Discovering the Scope of Privacy Needs in Collaborative Search
 | year = 2008
 | author = Thorben Burghardt, Erik Buchmann, Klemens Böhm
 | journal = Web Intelligence (WI)
 | pages = 
 910| volume = 
 | issue = 
 | doi = 10.1109/WIIAT.2008.165
 | isbn = 
 978-0-7695-3496-1}}&lt;/ref&gt; and the works of Longo et al.
&lt;ref name=Longo2009a&gt;{{citation
 | title = Toward Social Search - From Explicit to Implicit Collaboration
               to Predict Users' Interests
 | year = 2009
 | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo
 | journal = WEBIST 2009 - Proceedings of the Fifth International Conference
               on Web Information Systems and Technologies, Lisbon, Portugal,
               March 23–26, 2009
 | pages = 693–696
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-989-8111-81-4
 | url = 
}}&lt;/ref&gt; 
&lt;ref name=Longo2010&gt;{{citation
 | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time
 | year = 2010
 | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo
 | journal = Transaction Computational Collective Intelligence II
 | pages = 46–69
 | volume = 2
 | issue = 
 | doi = 10.1007/978-3-642-17155-0_3
 | isbn = 
 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/
| series = Lecture Notes in Computer Science
 }}&lt;/ref&gt; 
&lt;ref name=Longo2009b&gt;{{citation
 | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search
 | year = 2009
 | author = Longo Luca, Barrett Stephen, Dondio Pierpaolo
 | journal = Computational Collective Intelligence. Semantic Web, Social
               Networks and Multiagent Systems, First International Conference,
               ICCCI 2009, Wroclaw, Poland, October 5–7, 2009. Proceedings
 | pages = 63–74
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-3-642-04440-3
 | url = http://dl.acm.org/citation.cfm?id=1692026
}}&lt;/ref&gt; 
all represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.

Explicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether&lt;ref name=Morris2007&gt;{{citation
 | title = SearchTogether: An Interface for Collaborative Web Search
 | year = 2007
 | author = Meredith Ringel Morris, Eric Horvitz
 | journal = UIST
| url = http://portal.acm.org/citation.cfm?id=1294211.1294215
}}&lt;/ref&gt; published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.&lt;ref name=Redy2008&gt;{{citation
 | title = The Role of Communication in Collaborative Information Searching
 | year = 2008
 | author = Madhu C. Reddy, Bernhard J. Jansen, Rashmi Krishnappa
 | journal = ASTIS
}}&lt;/ref&gt; (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,&lt;ref name=&quot;Smith2003&quot;/&gt; the Community Search Assistant,&lt;ref name=&quot;Glance2001&quot;/&gt; and the CSE of Burghardt et al.&lt;ref name=&quot;BurghardtWI2008&quot; /&gt; Cerciamo &lt;ref name=Pickens2008 /&gt; supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.

However, in Papagelis et al.&lt;ref name=Papagelis2007&gt;{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis, Christos Zaroliagis| journal = ENC '07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88–98| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}&lt;/ref&gt; terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.

=== Community of practice  ===

Recent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.&lt;ref name=Rohini&amp;Ambati&gt;{{citation
 | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries
 | year = 2002
 | author = Rohini U, Vamshi Ambati
 | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}&lt;/ref&gt;

Collaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.&lt;ref name=Coyle2008&gt;{{citation
 | title = Social Aspects of a Collaborative, Community-Based Search Network
 | editor4-first = Eelco
 | editor3-first = Pearl
 | editor2-first = Judy
 | editor1-first = Wolfgang
 | year = 2008
 | editor1-last = Nejdl
 | author = Maurice Coyle and Barry Smyth
 | journal = Adaptive Hypermedia and Adaptive Web-Based Systems
 | pages =  103–112  
 | volume = 5149/2008
 | issue = 
 
 | series = Volume| doi = 10.1007/978-3-540-70987-9
 | isbn = 978-3-540-70984-8
 | url = http://portal.acm.org/citation.cfm?id=1485050
 | editor2-last = Kay
 | editor4-last = Herder
 | editor3-last = Pu}}&lt;/ref&gt; The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project Jumper 2.0.&lt;ref name=Jumper2010&gt;{{citation
 | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features
 | year = 2010
 | author = Jumper Networks Inc
 | journal = Press release
 | pages = 
 | volume =
 | issue = 
 | doi =
 | isbn =
 | url = http://www.trilexnet.com/labs/jumper}}&lt;/ref&gt;

=== Depth of mediation ===

This refers to the degree that the CSE mediates search.&lt;ref name=Pickens2008 /&gt; SearchTogether&lt;ref name=Morris2007 /&gt; is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo&lt;ref name=Pickens2008 /&gt; and recommendation systems such as I-Spy&lt;ref name=Smith2003 /&gt; keep track of each person's search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.

=== Task vs. trait ===

This model classifies people's membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.&lt;ref name=Morris2008 /&gt;

== Privacy-aware collaborative search engines ==

Search terms and links clicked that are shared among users reveal their interests, habits, social
relations and intentions.&lt;ref name=EUArticle29&gt;{{citation
 | title = Article 29 EU Data Protection Working Party
 | year = 2008
 | author = Data Protection Working Party
 | journal = EU
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. 
&lt;ref name=&quot;Morris2007&quot;/&gt;&lt;ref name=Smith2005&gt;{{citation
 | title = A Live-User Evaluation of Collaborative Web Search
 | year = 2005
 | author = Barry Smyth, Evelyn Balfe, Oisin Boydell, Keith Bradley, Peter Briggs, Maurice Coyle, Jill Freyne
 | journal = IJCAI
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt;
&lt;ref name=Smith2006&gt;{{citation
 | title = Anonymous personalization in collaborative web search
 | year = 2005
 | author = Smyth,, Barry and Balfe,, Evelyn
 | journal = Inf. Retr.
 | pages = 165–190
 | volume = 9
 | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = 
 | url = 
}}&lt;/ref&gt;
&lt;ref name=Jung2004&gt;{{citation
 | title = Applying Collaborative Filtering for Efficient Document Search
 | year = 2004
 | author = Seikyung Jung, Juntae Kim, Herlocker, J.L.
 | journal = Inf. Retr.
 | pages = 640–643
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.

As CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.&lt;ref name=BurghardtCC2008&gt;{{citation
 | title = Collaborative Search And User Privacy: How Can They Be Reconciled?
 | year = 2008
 | author = Thorben Burghardt, Erik Buchmann, Klemens Böhm, Chris Clifton
 | journal = CollaborateCom
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://dbis.ipd.uni-karlsruhe.de/1184.php
}}&lt;/ref&gt; They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.

== References ==
{{reflist|2}}
{{Internet search}}

[[Category:Information retrieval]]</text>
      <sha1>k6r1ovbwpppy13bw4mm67srz4dj4138</sha1>
    </revision>
  </page>
  <page>
    <title>Negative search</title>
    <ns>0</ns>
    <id>21692300</id>
    <revision>
      <id>641411527</id>
      <parentid>574328815</parentid>
      <timestamp>2015-01-07T13:16:11Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging using [[Project:AWB|AWB]] (10703)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3460">{{Multiple issues|
{{unreferenced|date=March 2009}}
{{orphan|date=February 2009}}
{{confusing|date=March 2009}}
}}

'''Negative Search''' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.

Negative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.

Negative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.

Negative Search can also apply to searches where the user has a clear understanding of '''Negative Intent''' (what they don't want) rather than what they do.

Examples of Negative Intent are:

- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don't want.

- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don't want.

- An investigator is looking for a car but has no other information on that car on which to base a search.

==Negative Search Classifiers==

If there are two forms of search (positive and negative) it follows that there are two forms of classifier models: '''Inclusive Classifiers''' and '''Exclusive Classifiers'''.

[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.

==Irrelevancy as a Desirable Construct==

Positive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.

It follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.

==Degrees of Passivity==

Positive Search involves an active search by a user with no degree of passivity (or openness). For example: &quot;I am only interested in the Hilton Hotel in Vientiane on [[New Year\'s Eve|New Years Eve]].&quot;

Discovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: &quot;I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there's a better hotel, let me know&quot;

Negative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: &quot;I need a holiday and really don't care where as long as its good.&quot;

Searchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don't know they're looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.

==References==
{{Reflist}}

[[Category:Information retrieval]]</text>
      <sha1>mhy2rmyjwgszy10af7wctozl8o3gzgu</sha1>
    </revision>
  </page>
  <page>
    <title>List of enterprise search vendors</title>
    <ns>0</ns>
    <id>19501639</id>
    <revision>
      <id>646577483</id>
      <parentid>646558062</parentid>
      <timestamp>2015-02-11T00:34:41Z</timestamp>
      <contributor>
        <username>Kuru</username>
        <id>764407</id>
      </contributor>
      <comment>rmv external link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3316">== Free and open source [[enterprise search]] software ==
&lt;!--
################# READ THIS

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.
Also, read the definition of enterprise search before adding a new search engine
--&gt;
*[[Apache Solr]]
*[[DataparkSearch]]
*[[ElasticSearch]]
*[[Htdig|ht://Dig]]
*[[ApexKB]]
*[[mnoGoSearch]]
*[[OpenSearchServer]]
*[[Searchdaimon]]
*[[Sphinx_(search_engine)|Sphinx]]
*[[Zettair]]

== Vendors of open source enterprise search software ==
* [[30 Digits]] - Implementation, consulting, support, and value-add components for [[Lucene]] and [[Solr]]
* [[Apache Software Foundation]] - The foundation is the entity behind the [[Lucene]] family of products
* [[LucidWorks]] (former Lucid Imagination) - Commercial support, training and services for [[Lucene]] and [[Solr]]
* Customermatrix (acquired Polyspot, CRM development and products for [[Lucene]]) 
* [[Searchblox]] - Commercial product for [[Lucene]] and [[ElasticSearch]]
* [[Sematext]] - Consulting, development and products for [[Lucene]], [[Solr]], [[Nutch]], and [[Hadoop]]
* [[FlaxUK|Flax]] - Architecture, development and support for [[Lucene]], [[Solr]] and [[Xapian]]

== Vendors of proprietary enterprise search software ==
&lt;!--
################# READ THIS

Please do not add web links or companies which do not have Wikipedia articles. They will be summarily deleted.

--&gt;
*[[AskMeNow]]
*[[Attivio]]
*[[Concept Searching Limited]]
*[[Content Analyst Company|Content Analyst Company LLC]]
*[[Coveo]]
*[[Dassault Systèmes]] (acquired [[Exalead]])
*[[Denodo]]
*[[Dieselpoint, Inc.]]
*[[dtSearch Corp.]]
*[[EMC Corp.]]
*[[Exorbyte GmbH]]
*[[Expert System S.p.A.]]
*[[Exterro, Inc.]]
*[[Fabasoft Mindbreeze|Fabasoft]]
*[[Funnelback]]
*[[Google Search Appliance]]
*[[HP]] (acquired [[Autonomy Corporation]] which in turn acquired [[Verity]] K2 and Ultraseek)
*[[IBM]] (acquired [[Vivisimo]], rebranded &quot;[[Watson (computer)|Watson]]&quot;)
*[[Inbenta]]
*[[inter:gator Enterprise Search]]
*[[ISYS Search Software]]
*[[Lookeen]]
*[[Mark Logic|MarkLogic]]
*[[Microsoft]] (includes [[Microsoft Search Server]], [[Fast Search &amp; Transfer]])
*[[Fabasoft Mindbreeze|Mindbreeze]] 
*[[Neofonie]] (includes WeFind)
*[[Omniture]] (acquired by [[Adobe Systems]])
*[[Open Text Corporation]]
*[[Oracle Corporation]] (includes [[Oracle_Corporation#Oracle_Secure_Enterprise_Search|Secure Enterprise Search]] and [[Endeca Technologies Inc.]])
*[[Q-go]]
*[[Q-Sensei]]
*[[Recommind (software company)|Recommind]]
*[[SAP AG|SAP]] (includes SAP NetWeaver Enterprise Search, Search Services in SAP NetWeaver AS ABAP, and Search and Classification TREX)
*[[Silent Eight]]
*[[Sinequa]]
*[[SLI_Systems]]
*[[Sophia Search Limited]]
* [[Swiftype]]
*[[TeraText]]
*[[Thunderstone Software]]
*[[X1 Technologies, Inc.]]
*[[ZyLAB Technologies]]
*[[ZL Technologies]]

== External links ==
* [http://www.dmoz.org/Computers/Software/Information_Retrieval/Fulltext/ DMOZ category Information Retrieval/Fulltext]
{{Companies by industry}}

{{DEFAULTSORT:Enterprise search vendors}}
[[Category:Information retrieval]]
[[Category:Searching]]
[[Category:Search engine software|*Enterprise search vendors]]
[[Category:Lists of software]]
[[Category:Lists of companies by industry|Enterprise search vendors]]</text>
      <sha1>re648vmdao4w0tyusucma4njaacqhml</sha1>
    </revision>
  </page>
  <page>
    <title>Compound term processing</title>
    <ns>0</ns>
    <id>18046649</id>
    <revision>
      <id>648003633</id>
      <parentid>647896308</parentid>
      <timestamp>2015-02-20T07:04:30Z</timestamp>
      <contributor>
        <username>Bgwhite</username>
        <id>264323</id>
      </contributor>
      <comment>[[WP:CHECKWIKI]] error fix #26. Convert HTML to wikicode. Do [[Wikipedia:GENFIXES|general fixes]] and cleanup if needed. - using [[Project:AWB|AWB]] (10822)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5833">{{copy edit|for=Use of references (both inline, and ref tags)|date=February 2015}}

'''Compound term processing''' refers to a category of techniques used in [[information retrieval]] applications that perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, &quot;triple&quot; is a single word term, but &quot;triple heart bypass&quot; is a compound term.

In August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing &lt;ref&gt;{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&amp;ETOC=RN]&lt;/ref&gt;

CLAMOUR&lt;ref&gt;[http://www.statistics.gov.uk/methods_quality/clamour/coordination/wp03.asp] National Statistics CLAMOUR project&lt;/ref&gt;&lt;ref&gt;[http://www.statistics.gov.uk/methods_quality/clamour/downloads/Clamour_march2002_final_reportAO.pdf] CLAMOUR Final Report&lt;/ref&gt; is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information &amp; statistics. In contrast to the techniques discussed by Concept Searching Limited, CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.

Compound Term Processing allows information retrieval applications, such as search engines, to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.

Most [[search engine]]s simply look for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, &quot;Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen&quot; uses the operators &quot;NEAR&quot;, &quot;AND&quot;, &quot;OR&quot; and &quot;NOT&quot; to specify that these words must follow certain requirements. [[Phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.

Techniques for probabilistic weighting of single word terms dates back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Spärck Jones]] entitled &quot;Relevance weighting of search terms&quot;, originally published in the ''Journal of the American Society for Information Science''.&lt;ref&gt;{{cite doi | 10.1002/asi.4630270302}}&lt;/ref&gt;  Robertson stated that the assumption of word independence is not justified and exists simply as a matter of mathematical convenience. His objection to term independence is not a new idea, dating back to at least 1964 when H. H. Williams expressed that &quot;[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience&quot;.&lt;ref&gt;WILLIAMS, J.H., 'Results of classifying documents with multiple discriminant functions', In : Statistical Association Methods for Mechanized Documentation, National Bureau of Standards, Washington, 217-224 (1965).&lt;/ref&gt;

Compound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? By forming compound terms and placing these terms in a search engine's index, searches can be performed with a higher degree of accuracy, as the ambiguity inherent in single words is no longer a problem. Using this technique, a search for ''survival rates following a triple heart bypass in elderly people'' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case &quot;survival rates&quot;, &quot;triple heart bypass&quot; and &quot;elderly people&quot;) and use these concepts to select the most relevant documents.

In 2004, Anna Lynn Patterson filed a number of patents on &quot;phrase-based searching in an information retrieval system&quot;&lt;ref&gt;[http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=1&amp;f=G&amp;l=50&amp;s1=%2220060031195%22.PGNR.&amp;OS=DN/20060031195&amp;RS=DN/20060031195] US Patent: 20060031195&lt;/ref&gt; to which Google subsequently acquired the rights. A full discussion of the patents can be found at [http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]{{dead link|date=February 2015}}.

Statistical compound term processing is a method more adaptive than the process described by Patterson in her patent applications. Her process is targeted at searching the World Wide Web where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.

Statistical compound term processing is also more adaptive than the linguistic approach taken by the CLAMOUR project, which must take into consideration the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.

==See also==
* [[Enterprise search]]
* [[Information retrieval]]

== References ==
{{Reflist}}

==  External links ==
*[http://www.conceptsearching.com/ Concept Searching Limited]
*[http://www.webmasterwoman.com/search-engines/phrase-based-indexing.html Webmaster Woman]

{{Natural Language Processing}}

{{DEFAULTSORT:Compound Term Processing}}
[[Category:Information retrieval]]</text>
      <sha1>iy78y4qbacn2wajlta6uhd8cqcqmnqf</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise search</title>
    <ns>0</ns>
    <id>12101316</id>
    <revision>
      <id>643278482</id>
      <parentid>636151618</parentid>
      <timestamp>2015-01-19T23:02:03Z</timestamp>
      <contributor>
        <ip>173.72.197.85</ip>
      </contributor>
      <comment>/* Components of an enterprise search system */ Corrected number of noun.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9926">'''Definition:''' '''Enterprise search''' is the organized retrieval of '''structured''' and '''unstructured''' data within an organization. 


'''Enterprise search''' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.

==Enterprise search summary==
&quot;Enterprise Search&quot; is used to describe the software of search information within an enterprise (though the search function and its results may still be public).&lt;ref&gt;[http://www.aiim.org/What-is-Enterprise-Search What is Enterprise Search?]&lt;/ref&gt; Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.

Enterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.&lt;ref&gt;[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]&lt;/ref&gt; Enterprise search systems also use access controls to enforce a security policy on their users.&lt;ref&gt;[http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search - Part 1: Defining Specific Security Requirements]&lt;/ref&gt;

Enterprise search can be seen as a type of [[vertical search]] of an enterprise.

==Components of an enterprise search system==
In an enterprise search system, content goes through various phases from source repository to search results:

=== Content awareness ===
Content awareness (or &quot;content collection&quot;) is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.&lt;ref&gt;[http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html Understanding Content Collection and Indexing]&lt;/ref&gt;

=== Content processing and analysis ===
Content from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.

As part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.&lt;ref&gt;[http://packages.python.org/Whoosh/stemming.html Stemming, Variations, and Accent Folding]&lt;/ref&gt;

=== Indexing ===
The resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].

=== Query Processing ===
Using a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.

=== Matching ===
The processed query is then compared to the stored index, and the search system returns results (or &quot;hits&quot;) referencing source documents that match. Some systems are able to present the document as it was indexed.

==Differences from web search==
Beyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:
*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].
*[[Federated search]], which consists of
# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,
# merging the results collected from the databases,
# presenting them in a succinct and unified format with minimal duplication, and
# providing a means, performed either automatically or by the portal user, to sort the merged result set.
*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.
*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.
*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).
*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.
*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.

==Relevance factors for enterprise search==
The factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web's [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document's recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user's attention.

==Access Control - early binding vs late binding==
Security and restricted access to documents is an important matter in Enteprise Search. There are two main approaches to apply restricted access: early binding vs late binding.&lt;ref&gt;[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]&lt;/ref&gt;

===Late binding===
Permissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).

===Early binding===
Permissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).

==Search Relevance Testing options==
Search application relevance can be determined by following relevance testing options like&lt;ref&gt;[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]&lt;/ref&gt;
*Focus groups
*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)
*Empirical testing
*[[A/B testing]]
*Log analysis on a Beta production site
*Online ratings

==See also==
*[[Comparison of enterprise search software]]
*[[List of enterprise search vendors]]
*[[List of Search Engines]]
*[[Collaborative search engine]]
*[[Data Defined Storage]] 
*[[Enterprise bookmarking]]
*[[Enterprise information access]]
*[[Knowledge management]]
*[[Text mining]]
*[[Faceted search]]
*[[Information Extraction]]
*[[Vertical search|Vertical Search]]

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise Search}}
[[Category:Information retrieval]]
[[Category:Searching]]</text>
      <sha1>p6j25vizs3d12q03kxw1xzf21fki0h2</sha1>
    </revision>
  </page>
  <page>
    <title>Clairlib</title>
    <ns>0</ns>
    <id>22641668</id>
    <revision>
      <id>542406090</id>
      <parentid>495811756</parentid>
      <timestamp>2013-03-06T18:05:24Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>Converted {{Multiple issues}} to new format to fix expert parameter &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]] (8853)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2672">{{Multiple issues|
{{unreferenced|date=May 2009}}
{{expert-subject|date=May 2009}}
{{orphan|date=February 2011}}
}}

{{Infobox software
|name                       = Clairlib
|logo                       = [[Image:Clair logo.jpg]]
|screenshot                 = 
|caption                    = 
|collapsible                = 
|author                     = 
|developer                  = CLAIR [[University of Michigan]]
|released                   = 
|latest release version     = 1.0.8
|latest release date        = {{release date and age|2009|08|1}}
|latest preview version     = 
|latest preview date        = 
|frequently updated         = yes
|programming language       = [[Perl]]
|operating system           = 
|platform                   = Cross-platform
|size                       = 
|language                   = Perl
|status                     = Active
|genre                      = [[Natural Language Processing]], [[Network theory|Network Analysis]], [[Information Retrieval]]
|license                    = [[GNU General Public License]], [[Artistic License]]
|website                    = [http://www.clairlib.org/ www.clairlib.org]
}}
'''Clairlib''' is a suite of open-source [[Perl]] modules developed and maintained by the Computational Linguistics And Information Retrieval (CLAIR) group at the [[University of Michigan]]. Clairlib is intended to simplify a number of generic tasks in [[natural language processing]] (NLP), [[information retrieval]] (IR), and network analysis (NA). The latest version of clairlib is 1.06 which was released on March 2009 and includes about 130 modules implementing a wide range of functionalities.

==Functionality==

Clairlib is distributed in two forms: Clairlib-core, which has essential functionality and minimal dependence on external software, and Clairlib-ext, which has extended functionality that may be of interest to a smaller audience. Much can be done using Clairlib on its own. Some of the things that Clairlib can do are: Tokenization, Summarization, Document Clustering, Document Indexing, Web Graph Analysis, Network Generation,  [[Power law distribution]] Analysis, [[Network theory|Network Analysis]], [[Random walk]]s on graphs, [[Tf-idf]], [[Perceptron]] learning  and classification, and [[Compound term processing|Phrase Based Retrieval]] and [[Fuzzy logic|Fuzzy OR Queries]].

==References==
{{reflist}}

==External links==
*[http://www.clairlib.org Homepage]
*[http://tangra.si.umich.edu/clair/ Computational Linguistics And Information Retrieval (CLAIR) group]

[[Category:Free computer libraries]]
[[Category:Perl modules]]
[[Category:University of Michigan]]
[[Category:Information retrieval]]</text>
      <sha1>ofhdkr6qr31ih265ty1o2i8j5qjcv3m</sha1>
    </revision>
  </page>
  <page>
    <title>Web query classification</title>
    <ns>0</ns>
    <id>16350490</id>
    <revision>
      <id>637344574</id>
      <parentid>636587359</parentid>
      <timestamp>2014-12-09T17:23:41Z</timestamp>
      <contributor>
        <ip>24.57.222.62</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9156">{{Cleanup|date=March 2011}}
''' 
A Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query “''apple''” might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.

== KDDCUP 2005 ==

KDDCUP 2005 competition&lt;ref&gt;[http://www.sigkdd.org/kdd2005/kddcup.html KDDCUP 2005 dataset]&lt;/ref&gt; highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query “''apple''”, it should be classified into ranked categories: “''Computers \ Hardware''; ''Living \ Food &amp; Cooking''”.

{| class=&quot;wikitable&quot;
|-
! Query
! Categories
|-
| apple
| Computers \ Hardware&lt;br /&gt;Living \ Food &amp; Cooking
|-
| FIFA 2006
| Sports \ Soccer&lt;br /&gt;Sports \ Schedules &amp; Tickets&lt;br /&gt;Entertainment \ Games &amp; Toys
|-
| cheesecake recipes
| Living \ Food &amp; Cooking&lt;br /&gt;Information \ Arts &amp; Humanities
|-
| friendships poem
| Information \ Arts &amp; Humanities&lt;br /&gt;Living \ Dating &amp; Relationships
|}

[[Image:Web query length.gif]]
[[Image:Web query meaning.gif]]

== Difficulties ==

Web query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:

=== How to derive an appropriate feature representation for Web queries? ===

Many queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, &quot;''apple''&quot; can mean a kind of fruit or a computer company. &quot;''Java''&quot; can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.

* Query-enrichment based methods&lt;ref&gt;Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf &quot;Q2C@UST: Our Winning Solution to Query Classification&quot;]. ''ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2''.&lt;/ref&gt;&lt;ref&gt;Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 &quot;Query Enrichment for Web-query Classification&quot;]. ''ACM TOIS, Vol. 24, No. 3, July 2006''.&lt;/ref&gt; start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).

How about disadvantages and advantages??
give the answers:

=== How to adapt the changes of the queries and categories over time? ===

The meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word &quot;''Barcelona''&quot; has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.

* Intermediate taxonomy based method&lt;ref&gt;Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 &quot;Building bridges for web query classification&quot;]. ''ACM SIGIR, 2006''.&lt;/ref&gt; first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.

=== How to use the unlabeled query logs to help with query classification? ===

Since the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users' knowledge about the World Wide Web.

* Query clustering method&lt;ref&gt;Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 &quot;Query Clustering Using User Logs&quot;], ''ACM TOIS, Volume 20, Issue 1, January 2002''.&lt;/ref&gt; tries to associate related queries by clustering “session data”, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.

* Selectional preference based method&lt;ref&gt;Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 &quot;Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs&quot;], ''ACM TOIS, Volume 25, Issue 2, April 2007''.&lt;/ref&gt; tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.

== Applications ==

* '''[[metasearch|Metasearch engines]]''' send a user's query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users' navigation.
* '''[[Vertical search]]''', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.
* '''[[Online advertising]]'''&lt;ref&gt;[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD'07)], KDD workshop 2007&lt;/ref&gt;&lt;ref&gt;[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA'08)], WWW workshop 2008&lt;/ref&gt; aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.
All these services rely on the understanding Web users' search intents through their Web queries.

== See also ==

* [[Document classification]]
* [[Web search query]]
* [[Information retrieval]]
* [[Query expansion]]
* [[Naive Bayes classifier]]
* [[Support vector machines]]
* [[Meta search]]
* [[Vertical search]]
* [[Online advertising]]

== References ==

{{reflist}}

== Further reading ==
* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&amp;skeywords=CSED%202007%20Shen &quot;Learning-based Web Query Understanding&quot;]. ''Phd Thesis'', ''HKUST'', June 2007.
{{Internet search}}

{{DEFAULTSORT:Web Query Classification}}
[[Category:Information retrieval]]
[[Category:Internet search]]</text>
      <sha1>bq7kabwr13gafp9cd5uomze5otrfqxc</sha1>
    </revision>
  </page>
  <page>
    <title>Search-based application</title>
    <ns>0</ns>
    <id>23266481</id>
    <revision>
      <id>630659757</id>
      <parentid>630658632</parentid>
      <timestamp>2014-10-22T14:55:39Z</timestamp>
      <contributor>
        <username>Remocrevo</username>
        <id>4578454</id>
      </contributor>
      <comment>/* Further Reading */ Fix broken article link, add author and accessed date.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6517">'''Search-based applications''' ('''SBA''') are [[software applications]] in which a [[Search engine|search engine platform]] is used as the core infrastructure for information access and reporting. SBAs use [[Semantic technology|semantic technologies]] to aggregate, normalize and classify [[Unstructured data|unstructured]], [[Semi-structured data|semi-structured]] and/or [[Structured data|structured content]] across multiple repositories, and employ [[Natural language processing|natural language technologies]] for accessing the aggregated information.

== Pre-Conditions ==

Search based applications are fully packaged applications that:&lt;ref&gt;Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.&lt;/ref&gt;
* Are built on a search backbone to enable sub-second access to information in multiple formats and from multiple sources
* Are delivered as a unified work environment to support a specific task or workflow, for example: eDiscovery, financial services regulatory compliance, fraud detection, voice of the customer, sales prospecting, pharmaceutical research, anti-terrorism intelligence, or customer support.
* Integrate all the tools that are commonly needed for that specific task or workflow, including:
** Multi-source information access
** Authoring
** Collaboration
** Business process
** Reporting and analysis
** Alerting
** Visualization
* Provide pre-configured data integration with multiple repositories of information in multiple formats as appropriate for the application domain.
* Integrate domain knowledge to support the particular task, including industry taxonomies and vocabularies, internal processes, workflow for the task, connectors to specialized collections of information, and decision heuristics typical of the field.
* Provide a compelling user interface and interaction design that eliminates the need for users to “pogo stick” or continually jump from one application to another. This buffers the user from the complexity of operating separate applications and enables them to focus on getting work done.
* Are quick to deploy, easy to customize or extend, and economical to administer

== Practical Uses ==

SBAs are used for a variety of purposes, including:

* ''' Enterprise Business Applications:''' For example, [[Customer Relationship Management]] (CRM), [[Enterprise Resource Planning]] (ERP), [[Supply Chain Management]] (SCM), Compliance &amp; Discovery, and [[Business Intelligence]] (BI)

* ''' Web Applications:''' Typically, B2B, B2C and C2B applications that [[Mashup (digital)|mash-up]] data and functionality from diverse sources (databases, Web content, user-generated content, mapping data and functions, etc.)

The use of a search platform as the core infrastructure for software applications has been enabled largely by two search engine features:  1) Scalability 2) Ad hoc access to multiple heterogeneous sources from a single point of access.

Search based applications have proven popular and effective because they provide a dynamic, scalable access infrastructure that can be integrated with other features that information workers need:  task-specific, and easy to use work environments that integrate features that are usually designed to be used as separate applications, collaborative features, domain knowledge, and security.

Search engines are not a replacement for database systems; they are a complement. They have been optimally engineered to facilitate access to information, not to record and store transactions. In addition, the mathematical and statistical processors integrated to date into search engines remain relatively simple. At present, therefore, databases still provide a more effective structure for complex analytical functions.Search applications also focus on providing quality results considering search relevancy.

==References==
{{Reflist}}

==Further Reading==
* Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.
* Butler Group [http://www.butlergroup.com/webinarIntroduction.asp?mcr=EXA190509&amp;scr=EXA190509 Webinar on Search Based Applications] explaining SBA and how they work
* Presentation on [http://www.informationbuilders.com/support/developers/presentations/?109 Search Based Applications] by   [[Information Builders]]
* IDC Executive Brief [http://www.exalead.com/software/forms/download.php?resourceid=69 &quot;The Information Advantage: Information Access in Tomorrow's Enterprise,&quot;] October 2009, downloadable from the [[Exalead|Exalead.com]] website. Adapted from [http://www.idc.com/getdoc.jsp?containerId=217936 Hidden Costs of Information Work: A Progress Report] and [http://www.idc.com/getdoc.jsp?containerId=219883 Worldwide Search and Discovery Software 2009–2013 Forecast Update and 2008 Vendor Shares] by Susan Feldman, IDC.
* IDC [http://www.kmworld.com/downloads/66062/Search_Market_Map_Chart.pdf Search and Discovery Software: 2009 Market Map]
* KMWorld article [http://www.kmworld.com/Articles/Editorial/Feature/Search-based-applications-support-critical-decision-making-66062.aspx Search-based applications support critical decision making]
* Kellblog post [http://www.kellblog.com/2010/02/11/idcs-definiton-of-search-based-applications/ IDC's Definition of Search-Based Applications]
* Steve-Kearns' [http://www.basistech.com/knowledge-center/search/2010-05-building-multilingual-search-based-applications.pdf Building Multilingual Search Based Applications] presentation at Apache Lucene EuroCon 2010 conference
* Information Today article [http://newsbreaks.infotoday.com/NewsBreaks/Attivio-Upgrades-Its-Active-Intelligence-Engine-67608.asp Attivio Upgrades Its Active Intelligence Engine]
* [http://lucidworks.com/blog/debugging-search-application-relevance-issues/ Debugging Search Application Relevance Issues] by Grant Ingersoll. Accessed October 22, 2014.
* [http://www.mind7.fr/en/information_intelligence.html Explanatory video on SBA's and Content Analysis]

== See also ==
{{col-begin}}
{{col-2}}
* [[Agile application]]
* [[Agile development]]
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Enterprise Search]]
* [[Search oriented architecture]]
* [[Software as a service]]
* [[Lookeen]]
* [[Lucene]]
* [[Exalead]]
{{col-end}}

&lt;!-- Categories --&gt;
[[Category:Enterprise application integration]]
[[Category:Information retrieval]]
[[Category:Internet search engines| ]]
[[Category:Internet terminology]]</text>
      <sha1>5uezbzczmy43rarp7xbknwr9346febn</sha1>
    </revision>
  </page>
  <page>
    <title>European Summer School in Information Retrieval</title>
    <ns>0</ns>
    <id>22254915</id>
    <revision>
      <id>596243155</id>
      <parentid>555365118</parentid>
      <timestamp>2014-02-19T21:35:59Z</timestamp>
      <contributor>
        <ip>109.113.212.31</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4749">The '''European Summer School in Information Retrieval''' (ESSIR) is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.
The aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: “''The term IR identifies the activities that a person – the user – has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''”&lt;ref&gt;Agosti, M.: “Information Access using the Guide of User Requirements”. In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).&lt;/ref&gt;

IR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[Computer Science]] to [[Information Science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].

ESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the [[Internet]].

Two books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval''
,&lt;ref&gt;Agosti, M., Crestani, F. and Pasi, G. (Eds): “Lectures on Information Retrieval“. Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11–15, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.&lt;/ref&gt; the second one is ''Advanced Topics in Information Retrieval''.&lt;ref&gt;Melucci, M., and Baeza-Yates, R. (Eds): “Advanced Topics in Information Retrieval“. The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.&lt;/ref&gt;

== ESSIR Editions ==
ESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by Nick Belkin, [[Rutgers University]], U.S.A., and Maristella Agosti, [[University of Padua]], Italy, for an Italian audience in 1989.

{| class=&quot;wikitable&quot; border=&quot;1&quot;
|-
! Edition
! Web Site
! Location
! Organiser(s)
|-
|  9th
|  [http://www.ugr.es/~essir2013/ ESSIR 2013]
|  Granada, Spain
|  Juan M. Fernadez-Luna and Juan F. Huete
|-
|  8th
|  [http://essir.uni-koblenz.de/ ESSIR 2011]
|  Koblenz, Germany
|  Sergej Sizov and Steffen Staab
|-
|  7th
|  [http://essir2009.dei.unipd.it/ ESSIR 2009]
|  Padua, Italy
|  Massimo Melucci and Ricardo Baeza-Yates
|-
|  6th
|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]
|  Glasgow, Scotland, United Kingdom
|  Iadh Ounis and Keith van Rijsbergen
|-
|  5th
|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]
|  Dublin, Ireland
|  Alan Smeaton
|-
|  4th
|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]
|  Aussois (Savoie), France
|  Catherine Berrut and Yves Chiaramella
|-
|  3rd
|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]
|  Varenna, Italy
|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi
|-
|  2nd
|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]
|  Glasgow, United Kingdom
|  Keith van Rijsbergen
|-
|  1st
|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]
|  Brixen, Italy
|  Maristella Agosti
|}

==Notes==
{{reflist}}

==External links==
* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]
* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering - University of Padua, Italy]
* [http://www.dei.unipd.it/ Department of Information Engineering - University of Padua, Italy]
* [http://www.unipd.it/en/index.htm University of Padua, Italy]

[[Category:Information retrieval]]
[[Category:Summer schools]]</text>
      <sha1>g1hj0xmq6tt0bg38rf9zo0180v4pbp1</sha1>
    </revision>
  </page>
  <page>
    <title>Matthews correlation coefficient</title>
    <ns>0</ns>
    <id>12306500</id>
    <revision>
      <id>637534226</id>
      <parentid>633446297</parentid>
      <timestamp>2014-12-10T22:16:36Z</timestamp>
      <contributor>
        <ip>77.101.208.183</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7725">The '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]]. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &amp;minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &amp;minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson's chi-square test|chi-square statistic]] for a 2×2 [[contingency table]]

: &lt;math&gt;|\text{MCC}| = \sqrt{\frac{\chi^2}{n}}&lt;/math&gt;

where ''n'' is the total number of observations.

While there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures{{Citation needed|reason=Source needed for being 'best'|date=December 2014}}. Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.

The MCC can be calculated directly from the [[confusion matrix]] using the formula:

: &lt;math&gt;
\text{MCC} = \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
&lt;/math&gt;

In this equation, ''TP'' is the number of [[true positive]]s, ''TN'' the number of [[true negative]]s, ''FP'' the number of [[false positive]]s and ''FN'' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.

The measure was introduced in 1975 by Matthews.&lt;ref&gt;{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442-451|doi=10.1016/0005-2795(75)90109-9}}&lt;/ref&gt; The original formula equal to above was:
: &lt;math&gt;
\text{N} = TN + TP + FN + FP
&lt;/math&gt;
: &lt;math&gt;
\text{S} = \frac{ TP + FN } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{P} = \frac{ TP + FP } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{MCC} = \frac{ TP / N - S \times P } {\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }
&lt;/math&gt;

As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[markedness]] (deltap) and informedness (deltap').&lt;ref name=&quot;Perruchet2004&quot;&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}&lt;/ref&gt;&lt;ref name=&quot;Powers2007&quot;&gt;{{cite journal |first=David M W |last=Powers |date=2007/2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness &amp; Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}&lt;/ref&gt;

== Confusion Matrix ==
{{main|Confusion matrix}}

{| class=&quot;wikitable&quot; align=&quot;right&quot; width=35% style=&quot;font-size:98%; margin-left:0.5em; padding:0.25em; background:#f1f5fc;&quot;
|+ Terminology and derivations&lt;br 
/&gt;from a confusion matrix
|- valign=top
|
; true positive (TP)
:eqv. with hit
; true negative (TN)
:eqv. with correct rejection
; false positive (FP)
:eqv. with [[false alarm]], [[Type I error]]
; false negative (FN)
:eqv. with miss, [[Type II error]]
----
; [[sensitivity (test)|sensitivity]] or true positive rate (TPR)
:eqv. with [[hit rate]], [[Information retrieval#Recall|recall]]
:&lt;math&gt;\mathit{TPR} = \mathit{TP} / P = \mathit{TP} / (\mathit{TP}+\mathit{FN})&lt;/math&gt;
; [[Specificity (tests)|specificity]] (SPC) or True Negative Rate
:&lt;math&gt;\mathit{SPC} = \mathit{TN} / N = \mathit{TN} / (\mathit{FP} + \mathit{TN}) &lt;/math&gt;
; [[Information retrieval#Precision|precision]] or [[positive predictive value]] (PPV)
:&lt;math&gt;\mathit{PPV} = \mathit{TP} / (\mathit{TP} + \mathit{FP})&lt;/math&gt;
; [[negative predictive value]] (NPV)
:&lt;math&gt;\mathit{NPV} = \mathit{TN} / (\mathit{TN} + \mathit{FN})&lt;/math&gt;
; [[Information retrieval#Fall-out|fall-out]] or false positive rate (FPR)
:&lt;math&gt;\mathit{FPR} = \mathit{FP} / N = \mathit{FP} / (\mathit{FP} + \mathit{TN})&lt;/math&gt;
; [[false discovery rate]] (FDR)
:&lt;math&gt;\mathit{FDR} = \mathit{FP} / (\mathit{FP} + \mathit{TP}) = 1 - \mathit{PPV} &lt;/math&gt;
; Miss Rate or [[Type I and type II errors#False positive and false negative rates|False Negative Rate]] (FNR)
:&lt;math&gt;\mathit{FNR} = \mathit{FN} / (\mathit{FN} + \mathit{TP}) &lt;/math&gt;
----
; [[accuracy]] (ACC)
:&lt;math&gt;\mathit{ACC} = (\mathit{TP} + \mathit{TN}) / (P + N)&lt;/math&gt;
;[[F1 score]]
: is the [[Harmonic mean#Harmonic mean of two numbers|harmonic mean]] of [[Information retrieval#Precision|precision]] and [[sensitivity (test)|sensitivity]]
:&lt;math&gt;\mathit{F1} = 2 \mathit{TP} / (2 \mathit{TP} + \mathit{FP} + \mathit{FN})&lt;/math&gt;
; Matthews correlation coefficient (MCC)
:&lt;math&gt; \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP+FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
&lt;/math&gt;

;Informedness
:&lt;math&gt;TPR + SPC - 1&lt;/math&gt;
;Markedness
:&lt;math&gt;PPV + NPV - 1&lt;/math&gt;
;
&lt;span style=&quot;font-size:90%;&quot;&gt;''Source: Fawcett (2006).''&lt;ref name=Fawcelt2006&gt;{{cite journal|last=Fawcelt|first=Tom|title=An Introduction to ROC Analysis|journal=Pattern Recognition Letters|date=2006|volume=27|issue=8|pages=861 - 874|doi=10.1016/j.patrec.2005.10.010}}&lt;/ref&gt;&lt;/span&gt;
|}

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

== See also ==
* [[Phi coefficient]]
* [[F1 score]]
* [[Cramér's V (statistics)|Cramér's V]], a similar measure of association between nominal variables.
* [[Cohen's kappa]]♙

== References ==

{{Reflist}}

=== General References ===
* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview&quot; ''Bioinformatics'' 2000, 16, 412&amp;ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]
* Matthews, B.W., Comparison of the predicted and observed secondary structure of T4 phage lysozyme&quot; ''Biochim. Biophys. Acta'' 1975, 405, 442&amp;ndash;451
* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]

{{DEFAULTSORT:Matthews Correlation Coefficient}}
[[Category:Machine learning]]
[[Category:Information retrieval]]
[[Category:Statistical classification]]
[[Category:Computational chemistry]]
[[Category:Cheminformatics]]
[[Category:Bioinformatics]]
[[Category:Statistical ratios]]
[[Category:Summary statistics for contingency tables]]</text>
      <sha1>03pah4nu9ba8bcqq25t0ui0db2nfily</sha1>
    </revision>
  </page>
  <page>
    <title>Isearch</title>
    <ns>0</ns>
    <id>3844607</id>
    <revision>
      <id>577612206</id>
      <parentid>577465625</parentid>
      <timestamp>2013-10-17T19:17:38Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor/>
      <comment>Fixing article with multiple categories on one line (CHECKWIKI error 9) using [[Project:AWB|AWB]] (9543)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6268">{{for|the adware|Isearch (malware)}}

'''Isearch''' is [[open-source software|open-source]] [[text retrieval]] software first developed in 1994 by Nassib Nassar as part of the Isite [[Z39.50]] information framework. The project started at the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) of the North Carolina supercomputing center MCNC and funded by the [[National Science Foundation]] to follow in the track of [[Wide Area Information Server|WAIS]] and develop prototype systems for distributed information networks encompassing Internet applications, library catalogs and other information resources.

The main features of Isearch include full text and field searching, relevance ranking, Boolean queries, and support for many document types such as HTML, mail folders, list digests, MEDLINE, BibTeX, SGML/XML, FGDC Metadata, NASA DIF, ANZLIC metadata, ISO 19115 metadata and many other resource types and document formats.

It was the first search engine to be designed from the ground up to support [[SGML]] and ISO [[Z39.50]] search and retrieval. It included many innovations including the &quot;document type&quot; model—which is simply a (object oriented) method of associating each document with a class of functions providing a standard interface for accessing the document. It was one of the first engines (if not the first) to ever support XML.

The Isearch search/indexing text algorithms were based on [[Gaston Gonnet]]'s seminal work into PAT arrays and trees for text retrieval--- ideas that were developed for the New Oxford English Dictionary Project at the Univ. of Waterloo, and provided the seeds for [[Tim Bray]]'s PAT SGML engine that formed the basis of [[Open Text]]. One of the limiting factors, however, of the  Isearch design was that it was not well suited to handle the extremely large data sets that became popular in the mid to late 1990s. In many cases Isearch was adapted or modified to use different algorithms but usually retained the document type model and the architectural relationship with Isite.

Isearch was widely adopted and used in hundreds of public search sites, including  many high profile projects such as the [http://patft1.uspto.gov/ U.S. Patent and Trademark Office (USPTO) patent search],[http://clearinghouse3.fgdc.gov/  the Federal Geographic Data Clearinghouse (FGDC)], the NASA Global Change Master Directory, the NASA EOS Guide System, the NASA Catalog Interoperability Project, the Astronomical pre-print service based at the Space Telescope Science Institute, The PCT Electronic Gazette at the World Intellectual Property Organization (WIPO), Linsearch (a search engine for Open Source Software designed by Miles Efron), the SAGE Project of the Special Collections Department at Emory University, Eco Companion Australasia (an environmental geospatial resources catalog), Australian National Genomic Information Service (ANGIS), the [[Open Directory Project]] and numerous governmental portals in the context of the Government Information Locator Service (GILS) [[United States Government Printing Office|GPO]] mandate (ended in 2005?).

From 1994 to 1998 most of the development was centered around the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) in North Carolina (Engine core) and BSn in Germany (Doctypes). By 1998 much of the open-source Isearch core developers re-focused development into several spin-offs. In 1998 it became part of the Advanced Search Facility reference software platform funded by the U.S. Department of Commerce.

A/WWW Enterprises now maintains the open source version for public usage, supported by paying government clients, such as the U.S. Patent and Trademark Office, NASA, and the FGDC who have provided support to enhance the functionality and reliability of the software. The software suite is considered a reference implementation of catalog service software.

As of 2010, the open source version of Isearch is still used on 250+ nodes of FGDC, and by ANZLIC in Australia and selected Geospatial OneStop contributors to facilitate harvesting by GOS, including NOAA, Census Bureau and the Tenn. Field Office of the US Fish and Wildlife Service, among others.

==References==
*[http://www.springerlink.com/content/g5e2wfd0lekygvut/ Application of Metadata Concepts to Discovery of Internet Resources]
*[http://www.springerlink.com/content/b5chmkgx8akg4m2h/ An Operational Metadata Framework for Searching, Indexing, and Retrieving Distributed Geographic Information Services on the Internet]
* The UNIX Web Server Book, Second Edition, by R. Douglas Matthews et al. (Ventana Press, 1997).
* [http://www.webtechniques.com/archives/1997/05/nassar/  &quot;Searching With Isearch&quot;. May 1997, Web Techniques]
* [http://www.itl.nist.gov/fipspubs/fip192.htm FIPS-192: APPLICATION PROFILE FOR THE GOVERNMENT INFORMATION LOCATOR SERVICE (GILS)]
* [http://www.uneca.org/awich/AWICH%20Workshop/YaoundeWorkshop/Clearinghouse%20Yaounde.pdf Clearinghouse and Metadata Concepts, Danel Behanu, U.N. Economic Commission for Africa,  2004]
* [http://web.archive.org/web/19991006225226/http://www.whitehouse.gov/OMB/memoranda/m9805.html M-98-05 Guidance on the Government Information Locator Service] published by the [[Office of Management and Budget|OMB]]
* [http://www.hpcwire.com/archives/3149.html 01/1995 Press Release: Patent Office Launch Internet AIDS Patent Library]

==External links==
*[http://www.fgdc.gov/dataandservices/isite U.S. Federal Geographic Data Committee Isite]
*[http://isite.awcubed.com/ Isite/Isearch2 Documentation Site]
*[ftp://ftp.awcubed.com/pub/Software Current Isearch download site]
*[http://www.etymon.com/tr.html Etymon: Isearch]
*[http://www.ibu.de/node/52 BSn/NONMONOTONIC Lab: IB Search Engine], embeddable search engine. A commercial spin-off from the Isearch project.

===Comparisons===
* [http://www.ukoln.ac.uk/metadata/roads/product-comparison/  Product Comparison: Information Gateway Software]
* [http://wrg.upf.edu/WRG/dctos/Middleton-Baeza.pdf  A Comparison of Open Source Search Engines, Christian Middleton, Ricardo Baeza-Yates]
* [http://www.infomotions.com/musings/opensource-indexers/ Comparing Open Source Indexers]

[[Category:Information retrieval]]
[[Category:Free search engine software]]</text>
      <sha1>d4wi4yvpemff1be8t6kyamhee63031s</sha1>
    </revision>
  </page>
  <page>
    <title>Overlap coefficient</title>
    <ns>0</ns>
    <id>22049756</id>
    <revision>
      <id>638854995</id>
      <parentid>638854895</parentid>
      <timestamp>2014-12-20T01:47:16Z</timestamp>
      <contributor>
        <ip>46.162.232.132</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="882">The '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the intersection divided by the smaller of the size of the two sets:

:&lt;math&gt;\mathrm{overlap}(X,Y) = \frac{| X \cap Y | }{\min(|X|,|Y|)}&lt;/math&gt;

If set ''X'' is a subset of ''Y'' or the converse then the overlap coefficient is equal to one.

== External links==
* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/OverlapMetric.scala Overlap] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]

[[Category:Information retrieval]]
[[Category:String similarity measures]]
[[Category:Measure theory]]</text>
      <sha1>00paf9hdzs57nyxt6vgxcy1u9wazldy</sha1>
    </revision>
  </page>
  <page>
    <title>Gain (information retrieval)</title>
    <ns>0</ns>
    <id>11259135</id>
    <revision>
      <id>565408916</id>
      <parentid>544548995</parentid>
      <timestamp>2013-07-23T01:12:16Z</timestamp>
      <contributor>
        <username>New Children of the Almighty</username>
        <id>19038423</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2663">{{other uses2|Gain}}
{{Unreferenced|date=August 2009}}
The '''gain''', also called '''improvement over random''' {{cn|date=March 2013}} can be specified for a [[classifier (mathematics)|classifier]] and is an important measure {{dubious|date=March 2013}} to describe the performance of it.

== Definition ==
In the following a random classifier is defined such that it randomly predicts the same amount of either class.

The gain is defined as described in the following:

=== Gain in Precision ===

The random [[positive predictive value|precision]] of a classifier is defined as

&lt;math&gt;
r = \frac{TP+FN}{TP+TN+FP+FN} = \frac{\textit{Positives}}{N}
&lt;/math&gt;

where TP, TN, FP and FN are the numbers of true positives, true negatives, false positives and false negatives respectively, positives is the number of positive instances in the target dataset and N is the size of the dataset.

The random precision defines the lowest baseline of a classifier.

And '''Gain''' is defined as 

&lt;math&gt;
G = \frac{\textit{precision}}{r}
&lt;/math&gt;

which gives a factor by which a classifier is better when compared to its random counterpart. A Gain of 1 would indicate a classifier that is not better than random. The larger the gain, the better.

=== Gain in Overall Accuracy ===

The [[accuracy]] of a classifier in general is defined as

&lt;math&gt;
Acc = \frac{TP+TN}{TP+TN+FP+FN} = \frac{\textit{Corrects}}{N}
&lt;/math&gt;

Here, the random accuracy of a classifier can be defined as

&lt;math&gt;
r = \left ( \frac{\textit{Positives}}{N} \right ) ^2+ \left ( \frac{\textit{Negatives}}{N} \right ) ^2=f(\textit{Positives})^2 + f(\textit{Negatives})^2
&lt;/math&gt;

f(Positives) and f(Negatives) is the fraction of positive and negative classes in the dataset.

And again '''gain''' is

&lt;math&gt;
G = \frac{\textit{Acc}}{r}
&lt;/math&gt;

This time the gain is measured not only with respect to the prediction of a so-called positive class, but with respect to the overall classifier ability to distinguish the two equally important classes.

== Application ==
In [[Bioinformatics]] as an example, the gain is measured for methods that predict residue contacts in proteins.

== See also ==
* [[Accuracy and precision]]
* [[Binary classification]]
* [[Brier score]]
* [[Confusion matrix]]
* [[Detection theory]]
* [[F-score]]
* [[Information retrieval]]
* [[Matthews correlation coefficient]]
* [[Receiver operating characteristic]] or ROC curve
* [[Selectivity (electronic)|Selectivity]]
* [[Sensitivity and specificity]]
* [[Sensitivity index]]
* [[Statistical significance]]
* [[Youden's J statistic]]

{{DEFAULTSORT:Gain (Information Retrieval)}}
[[Category:Logic]]
[[Category:Information retrieval]]</text>
      <sha1>awxwgw347vcddumkobzeckv7nuw2447</sha1>
    </revision>
  </page>
  <page>
    <title>TeLQAS</title>
    <ns>0</ns>
    <id>21727808</id>
    <revision>
      <id>554506388</id>
      <parentid>551990586</parentid>
      <timestamp>2013-05-10T21:41:23Z</timestamp>
      <contributor>
        <username>Jac16888</username>
        <id>1211616</id>
      </contributor>
      <comment>rm userlink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1738">'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.&lt;ref&gt;Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.&lt;/ref&gt;

==Architecture==
TeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.&lt;ref&gt;Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).&lt;/ref&gt; The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.

==References==
&lt;references/&gt;

[[Category:Computational linguistics]]
[[Category:Information retrieval]]
[[Category:Natural language processing software]]</text>
      <sha1>8zjyxc3gnqwooqdnugxh8gmtxcrcvnh</sha1>
    </revision>
  </page>
  <page>
    <title>Instant indexing</title>
    <ns>0</ns>
    <id>6111052</id>
    <revision>
      <id>637541353</id>
      <parentid>601714721</parentid>
      <timestamp>2014-12-10T23:06:55Z</timestamp>
      <contributor>
        <username>Bhny</username>
        <id>285109</id>
      </contributor>
      <comment>/* top */ simplified</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3029">{{multiple issues|
{{Orphan|date=February 2009}}
{{Refimprove|article|date=November 2006}}
}}

'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].

==Delayed inclusion==
Certain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]]{{Citation needed|date=February 2007}}.

Delayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index{{Citation needed|date=February 2007}}.

==Criticisms==
A criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}

Instant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.

Select search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.

==External links==
* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html &lt;!-- Bot retrieved archive --&gt; |archivedate = 2006-04-27}}
* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories — A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}

== See also ==
* [[Search engine]]
* [[Search engine indexing]]
* [[Web crawling]]

[[Category:Internet terminology]]
[[Category:Information retrieval]]


{{website-stub}}</text>
      <sha1>dkxjn9x50yuvh1glstt6rbjwizq0bnn</sha1>
    </revision>
  </page>
  <page>
    <title>Full text search</title>
    <ns>0</ns>
    <id>1315248</id>
    <revision>
      <id>641403197</id>
      <parentid>633118310</parentid>
      <timestamp>2015-01-07T12:18:48Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging using [[Project:AWB|AWB]] (10703)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12485">{{Multiple issues|
{{refimprove|date=August 2012}}
{{cleanup|date=September 2009}}
}}

In [[text retrieval]], '''full-text search''' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).

In a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.&lt;ref&gt;In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.&lt;/ref&gt;

==Indexing==
When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called &quot;serial scanning.&quot; This is what some tools, such as [[grep]], do when searching.

However, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.&lt;ref name=&quot;Capabilities of Full Text Search System &quot;&gt;[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{Dead link |date=October 2012}}&lt;/ref&gt;

The indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as &quot;the&quot; and &quot;and&quot;) that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words &quot;drives&quot;, &quot;drove&quot;, and &quot;driven&quot; will be recorded in the index under the single concept word &quot;drive.&quot;

==The precision vs. recall tradeoff==
[[Image:Full-text-search-results.png|150px|thumb|right|This diagram represents a low-precision, low-recall search as described in the text.]]
Recall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned divided by all relevant results. Precision is the number of relevant results returned divided by the total number of results returned.

The diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only one relevant result of three possible relevant results was returned, so the recall is a very low ratio of 1/3 or 33%. The precision for the example is a very low 1/4 or 25%, since only one of the four results returned was relevant.&lt;ref name=&quot;isbn1430215941&quot;&gt;{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}&lt;/ref&gt;

Due to the ambiguities of [[natural language]], full text search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall while an increase in recall lowers precision.&lt;ref name=&quot;YuwonoLee&quot;&gt;{{Cite conference | first = Yuwono | last = B. |author2=Lee, D.L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE'96) | year = 1996}}&lt;/ref&gt;

{{See also|Precision and recall}}

==False-positive problem==

Free text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the ''intended'' search question. Such documents are called ''false positives'' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).

Clustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of &quot;football&quot;, clustering can be used to categorize the document/data universe into &quot;American football&quot;, &quot;corporate football&quot;, etc. Depending on the occurrences of words relevant to the categories, search terms a search result can be placed in one or more of the categories. This technique is being extensively deployed in the e-discovery domain.{{clarify|date=January 2012}}

==Performance improvements==

The deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.

===Improved querying tools===

*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.
* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as &quot;Title&quot; or &quot;Author.&quot;
* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, &quot;encyclopedia&quot; AND &quot;online&quot; NOT &quot;Encarta&quot;) can dramatically increase the precision of a free text search. The AND operator says, in effect, &quot;Do not retrieve any document unless it contains both of these terms.&quot; The NOT operator says, in effect, &quot;Do not retrieve any document that contains this word.&quot; If the retrieval list retrieves too few documents, the OR operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, &quot;encyclopedia&quot; AND &quot;online&quot; OR &quot;Internet&quot; NOT &quot;Encarta&quot;. This search will retrieve documents about online encyclopedias that use the term &quot;Internet&quot; instead of &quot;online.&quot; This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.&lt;ref&gt;Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]&lt;/ref&gt;
* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as &quot;Wikipedia, the free encyclopedia.&quot;
* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.
* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.
* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for &quot;Wikipedia&quot; WITHIN2 &quot;free&quot; would retrieve only those documents in which the words &quot;Wikipedia&quot; and &quot;free&quot; occur within two words of each other.
* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.
* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)
* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example using the asterisk in a search query &quot;s*n&quot; will find &quot;sin&quot;, &quot;son&quot;, &quot;sun&quot;, etc. in a text.

===Improved search algorithms===
The [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.&lt;ref&gt;{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt; See [[Search engine]] for additional examples.

==Software==

The following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.

=== Free and open source software ===
&lt;!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

--&gt;
* [[Apache Solr]]
* [[BaseX]]
* [[DataparkSearch]]
* [[ElasticSearch]]
* [[Ht-//Dig|ht://Dig]]
* [[KinoSearch]]
* [[Lemur Project|Lemur/Indri]]
* [[Lucene]]
* [[mnoGoSearch]]
* [[Searchdaimon]]
* [[Sphinx (search engine)|Sphinx]]
* [[Swish-e]]
* [[Xapian]]

=== Proprietary software ===
&lt;!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

--&gt;
* [[Attivio]]
* [[Autonomy Corporation]]
* [[Bar Ilan Responsa Project]]
* [[Brainware]]
* [[BRS/Search]] 
* [[Clusterpoint|Clusterpoint Server]]
* [[Concept Searching Limited]]
* [[Dieselpoint]]
* [[dtSearch]]
* [[Endeca]]
* [[Exalead]]
* [[Fast Search &amp; Transfer]]
* [[Inktomi (company)|Inktomi]]
* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)
* [[Lookeen]]
* [[Lucid Imagination]]
* [[MarkLogic]]
* [[Swiftype]]
* [[Thunderstone Software LLC.]]
* [[Vivísimo]]

==Notes==
{{Reflist}}

==See also==
*[[Pattern matching]] and [[string matching]]
*[[Compound term processing]]
*[[Controlled vocabulary]]
*[[Enterprise search]]
*[[Information Extraction]]
*[[Information retrieval]]
*[[Faceted search]]
*[[Full text database]]
*[[List of enterprise search vendors]]
*[[Search engine]]
*[[WebCrawler]], first FTS engine
*[[Search engine indexing]] - how search engines generate indices to support full text searching
*[[SQL Server Full Text Search|SQL Server Full Text Search (implementation of)]]

{{DEFAULTSORT:Full Text Search}}
[[Category:Searching]]
[[Category:Text editor features]]
[[Category:Information retrieval]]</text>
      <sha1>d0h92z3crgo5q9zbw8hx003qfoqa0p5</sha1>
    </revision>
  </page>
  <page>
    <title>Faceted search</title>
    <ns>0</ns>
    <id>10715937</id>
    <revision>
      <id>644014397</id>
      <parentid>644009529</parentid>
      <timestamp>2015-01-24T21:49:52Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>/* Technology */ add references, remove one I couldn't confirm; improve internal linking</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7002">{{mergeto|Faceted classification|date=January 2015}}
'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.&lt;ref name=&quot;Faceted Search&quot;&gt;[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan &amp; Claypool, 2009&lt;/ref&gt;

Facets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.

Within the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].{{fact|date=May 2014}}

==Development==

The [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:
&lt;blockquote&gt;
The web search world, since its very beginning, has offered two paradigms:
*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.
*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. 
Over the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].&lt;ref name=&quot;sigir06&quot;&gt;[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]&lt;/ref&gt;
&lt;/blockquote&gt;

==Technology==

Various search engine software supports faceted classification.

* [[Apache Lucene]] and derived software:
**  [[Apache Solr]]
** [[Swiftype]]
** [[Elasticsearch]]
* A number of major vendors listed at [[Comparison of enterprise search software#Faceted_Navigation]]
* [[Dieselpoint]]
* [[Endeca]]
* iSeek, search engine for general web and education&lt;ref&gt;[http://www.iseek.com iSeek]&lt;/ref&gt;
* [[SpeedTrack]]&lt;ref&gt;http://www.speedtrack.com/technology&lt;/ref&gt;
* XSEARCH&lt;ref&gt;[http://www.weitkamper.com]&lt;/ref&gt;

==Mass market use==

Faceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] such as [[Swiftype]] provide software for implementing faceted search applications.

Online retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. &lt;ref name=&quot;Smashing Magazine: The Current State of E-Commerce Search (2014)&quot;&gt;[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.&lt;/ref&gt; Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.

==Libraries and information science==




In 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.&lt;ref name=&quot;Major classification systems : the Dewey Centennial&quot;&gt;[http://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]&lt;/ref&gt;

Modern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system.

InfoHarness&lt;ref&gt;{{cite journal|last1=Shklar|first1=Leon|last2=Thatte|first2=Satish|last3=Marcus|first3=Howard|last4=Sheth|first4=Amit|title=The &quot;InfoHarness&quot; Information Integration Platform|journal=Proceedings of the Second International Conference on the World Wide Web|date=1994|url=http://citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.43.4042|accessdate=5 January 2015}}&lt;/ref&gt; &lt;ref&gt;{{cite journal|last1=Shklar|first1=Leon|last2=Sheth|first2=Amit|last3=Kashyap|first3=Vipul|last4=Shah|first4=Kshitij|title=InfoHarness: Use of automatically generated metadata for search and retrieval of heterogeneous information|journal=Advanced Information Systems Engineering|date=20 July 2005|doi=10.1007/3-540-59498-1_248|url=http://link.springer.com/chapter/10.1007/3-540-59498-1_248|accessdate=5 January 2015|ref=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.7442}}&lt;/ref&gt; is one of the first Web System (developed in 1994) that provided faceted search over heterogeneous information artifacts such as Web pages, images, videos and documents. The [[CiteSeerX]] project&lt;ref&gt;[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.&lt;/ref&gt; at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.

==See also==
* [[Enterprise Search]]
* [[Exploratory search]]
* [[Faceted classification]]
* [[Human–computer information retrieval]]
* [[Information Extraction]]
* [[NoSQL]]
* [[Trove (website)]]

==References==
&lt;References/&gt;

{{DEFAULTSORT:Faceted Search}}
[[Category:Information retrieval]]</text>
      <sha1>krwzqnki477ths3k9d3yuud8l1fnn9u</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Citation indices</title>
    <ns>14</ns>
    <id>24447333</id>
    <revision>
      <id>646584444</id>
      <parentid>641841810</parentid>
      <timestamp>2015-02-11T01:30:06Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>added [[Category:Bibliographic databases]] using [[WP:HC|HotCat]] From [[citation index]]: &quot;A citation index is a kind of bibliographic database...&quot;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="242">{{Cat main|Citation index}}
{{cat see also|Bibliographic databases|Bibliographic indexes}}

[[Category:Bibliometrics]]
[[Category:Reference works]]
[[Category:Indexes]]
[[Category:Information retrieval]]
[[Category:Bibliographic databases| ]]</text>
      <sha1>4kgeeipohvjsgqqg56nhjk8wosiijog</sha1>
    </revision>
  </page>
  <page>
    <title>DtSearch</title>
    <ns>0</ns>
    <id>14388058</id>
    <revision>
      <id>647089470</id>
      <parentid>616880015</parentid>
      <timestamp>2015-02-14T11:59:42Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>clean up using [[Project:AWB|AWB]] (10823)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5252">{{Lowercase}}

{{Infobox company |
  name   = dtSearch Corp. |
  slogan = &quot;The Smart Choice for Text Retrieval since 1991&quot; |
  type   =  Private company |
  foundation     = 1991 |
  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]] |
  key_people     = David Thede, President |
  industry       = [[Software]] |

  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]
}}

'''dtSearch Corp.''' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.

==History==
dtSearch Corp was founded by David Thede&lt;ref&gt;[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm; Lets talk computers - Interview May 31, 2003]&lt;/ref&gt;&lt;ref&gt;[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]&lt;/ref&gt;&lt;ref&gt;[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]&lt;/ref&gt; the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled &quot;Text Retrieval Software&quot; in an early edition of ''[[PC Magazine]]''&lt;ref&gt;&quot;Text Retrieval Software&quot;. (July 1992). [[PC Magazine]] (UK ed)&lt;/ref&gt; as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[AskSAM]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.

In the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft’s initial release of its 32-bit Windows operating system, [[Windows 95]].&lt;ref&gt;[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]&lt;/ref&gt;

In 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.&lt;ref&gt;[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&amp;PageNum=22007 EContent 100 list]&lt;/ref&gt;

==Products==
The current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.

*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)
*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)
*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)
*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)
*dtSearch Engine for Linux - SDK with C++ and Java APIs
*dtSearch Publish &lt;ref&gt;[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&amp;slreturn=1&amp;hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]&lt;/ref&gt; - a search front-end for CD\DVD publishing (32 and 64 bit indexers)

==See also==
* [[Enterprise search]]
* [[List of enterprise search vendors]]

==References==
{{Reflist}}

==External links==
*[http://www.dtsearch.com/ Company Website]
*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]
*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]
*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]
*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan–Feb]
*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233–1246] 
*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]

{{DEFAULTSORT:Dtsearch Corp.}}
[[Category:Desktop search engines]]
[[Category:Information retrieval]]
[[Category:Software companies based in Maryland]]</text>
      <sha1>sn93tzflzf6lzayx1a66opjhnhi2exp</sha1>
    </revision>
  </page>
  <page>
    <title>30 Digits</title>
    <ns>0</ns>
    <id>24605090</id>
    <revision>
      <id>641227856</id>
      <parentid>629621476</parentid>
      <timestamp>2015-01-06T11:31:48Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor/>
      <comment>cleanup using [[Project:AWB|AWB]] (10691)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3702">{{Use dmy dates|date=July 2013}}
{{multiple issues|
{{notability|Companies|date=August 2012}}
{{refimprove|date=August 2012}}
{{peacock|date=August 2012}}
{{advert|date=August 2012}}
}}
{{Infobox company
| logo = [[Image:30 Digits Logo.jpg|center]]
| name = 30 Digits GmbH
| type = [[Private company|Private]]
| foundation = 2008
| location = [[Munich]], Germany
| area_served = [[Europe]] &lt;br/&gt; [[North America]] &lt;br/&gt; [[South America]] &lt;br/&gt; [[Asia]]
| key_people = Justin Gilbreath (Managing Director) &lt;br/&gt; Mathis Koblin (Director of R&amp;D)
| industry = [[Information access]] &lt;br/&gt; [[Information retrieval]] &lt;br/&gt; [[Web mining]] &lt;br/&gt; [[Open Source software]]
| products = [[Search Engines]] &lt;br/&gt; Information Discovery Suite &lt;br/&gt; [[Apache Lucene]] &lt;br/&gt; [[Apache Solr]] &lt;br/&gt; Web Extractor
| company_slogan = Linking People to Content
| homepage = {{url|http://www.30digits.com}}
}}

'''30 Digits''' is a privately held information access and retrieval company with headquarters in [[Munich, Germany]]&lt;ref&gt;{{cite web |url=http://www.digitalpublic.de/web-20-suchmaschinen-holen-auf |title=Web 2.0 – Suchmaschinen holen auf}}&lt;/ref&gt; located in the &quot;Münchner Technologie Zentrum&quot;.&lt;ref&gt;{{cite web |url=http://www.mtz.de/index.php?id=12 |title=List of companies located in the MTZ (Münchner Technologie Zentrum)}}&lt;/ref&gt; The company was founded in 2008 and offers software that is a mix of privately developed code and leading [[Open Source]] technology primarily from the [[Apache Software Foundation]].

The company focuses on [[enterprise information access]] solutions from areas ranging from call-center applications to [[enterprise search]] to database offloading. The company also focuses on solutions created out of unstructured content on the web being structured for analysis,&lt;ref&gt;{{cite web |url=http://www.crmmanager.de/magazin/artikel_2165_enterprise_20_wahlkampf.html |title=Enterprise 2.0: Was ein Unternehmen von Obamas Wahlkampf lernen kann}}&lt;/ref&gt; often referred to as [[web harvesting]].  This can be for monitoring security threats or observing customer reactions to products.  It can even be used to gather complex address and other details about entities like properties.&lt;ref&gt;{{cite web |url=http://www.prweb.com/releases/2011/03/prweb5186764.htm |title=viewr Selects 30 Digits as Primary Property Data Provider }}&lt;/ref&gt;  Sometimes the focuses blend together in areas like Market or Business Intelligence where both internal and external information needs extraction, analysis, and retrieval capabilities.

In addition to the software solutions, 30 Digits Professional Services offers services to assist customer in designing and deploying the correct solutions for the challenge at hand. {{citation needed|date=August 2012}} Trainings, support, and consulting are available both on 30 Digits software and the Open Source software they work with like [[Lucene]] and [[Solr]].&lt;ref&gt;{{cite web |url=http://www.aktiv-verzeichnis.de/details/30-digits-gmbh.html |title=Company description from the Aktiv Verzeichnis}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://www.30digits.com  Company website]
* [http://www.imittelstand.de/mittelstandsliste/webcode/ww1213 Article (in German) placing 30 Digits Web Extractor in Top20 Business Intelligence tools for the &quot;Initiative Mittelstand&quot; 2009]
* [http://lucene.apache.org/  Lucene website]
* [http://lucene.apache.org/solr/ Solr website]

[[Category:Information retrieval]]
[[Category:Search engine software]]
[[Category:Software industry]]
[[Category:Software companies of Germany]]
[[Category:Business software]]
[[Category:Companies based in Munich]]
[[Category:Companies of Germany]]
[[Category:Companies of Europe]]</text>
      <sha1>tfbl3x4vdk754c0d7jva73t0sy23ood</sha1>
    </revision>
  </page>
  <page>
    <title>Search engine (computing)</title>
    <ns>0</ns>
    <id>27804</id>
    <revision>
      <id>641824878</id>
      <parentid>638463025</parentid>
      <timestamp>2015-01-10T03:51:42Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5094">{{more footnotes|date=August 2014}}
{{one source|date=August 2014}}
A '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}

The most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].

==How search engines work==
Search engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.&lt;ref&gt;Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.&lt;/ref&gt; There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].

[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]

The list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive_or|XOR]]) in a probabilistic context.

To provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[serp|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.

Other types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.

==See also==
{{Portal|Computer Science}}
{{div col|colwidth=30em}}
*[[Automatic summarization]]
*[[Bibliographic database]]
*[[Desktop search]]
*[[Emanuel Goldberg]] (inventor of early search engine)
*[[Enterprise search]]
*[[Federated search]]
*[[Full text search]]
*[[Human search engine]]
*[[Image search]]
*[[Index (search engine)]]
*[[Inverted index]]
*[[List of search engines]]
*[[List of enterprise search vendors]]
*[[Medical literature retrieval]]
*[[Metasearch engine]]
*[[Search engine optimization]]
*[[Search suggest drop-down list]]
*[[Selection-based search]]
*[[Semantic search]]
* [[Solver (computer science)]]
*[[Spamdexing]]
*[[SQL]]
*[[Text mining]]
*[[Vertical search]]
*[[Video search engine]]
*[[Web search engine]]
{{div col end}}

==References==
{{Reflist}}
{{Internet search}}

{{DEFAULTSORT:Search Engine (Computing)}}
[[Category:Information retrieval]]
[[Category:Data search engines| Search engine]]</text>
      <sha1>dvwijce3verv262kutre1gt6lvz7tl2</sha1>
    </revision>
  </page>
  <page>
    <title>Cognitive models of information retrieval</title>
    <ns>0</ns>
    <id>24963841</id>
    <revision>
      <id>537851127</id>
      <parentid>526955353</parentid>
      <timestamp>2013-02-12T10:22:37Z</timestamp>
      <contributor>
        <username>㓟</username>
        <id>17458824</id>
      </contributor>
      <comment>removed [[Category:Cognitive science]]; added [[Category:Cognitive modeling]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4990">{{Orphan|date=September 2012}}

'''Cognitive models of information retrieval''' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person's cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.

==Berrypicking==
One way of understanding how users search for information has been described by [[Marcia Bates]]&lt;ref&gt;[[Marcia Bates]] (1989). &quot;The Design of Browsing and Berrypicking Techniques for the Online Search Interface.&quot; http://www.gseis.ucla.edu/faculty/bates/berrypicking.html&lt;/ref&gt; at the [[University of California at Los Angeles]]. Bates argues that &quot;berrypicking&quot; better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.&lt;ref&gt;[[Gerard Salton]] (1968). ''Automatic Information and Retrieval'' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.&lt;/ref&gt;

Bates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user's question may change and thus the search evolves.

==Exploratory Search==
Researchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.&lt;ref&gt;Qu, Yan &amp; Furnas, George. &quot;Model-driven formative evaluation of exploratory search: A study under a sensemaking framework&quot;&lt;/ref&gt; Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.

==Natural language searching==

Another way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type &quot;&quot;I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?&quot; into your browser, and you will receive a useful and relevant response.&lt;ref&gt;Strickland, J. (n.d.). HowStuffWorks &quot;How Web 3.0 Will Work&quot;. Howstuffworks &quot;Computer&quot;. Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm&lt;/ref&gt;  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.

==Notes==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Cognitive modeling]]</text>
      <sha1>jtm4omkt8vt8xv59kk01r89ynjpavpx</sha1>
    </revision>
  </page>
  <page>
    <title>MAREC</title>
    <ns>0</ns>
    <id>24979660</id>
    <revision>
      <id>525865765</id>
      <parentid>494185738</parentid>
      <timestamp>2012-12-01T14:48:41Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>clean up /fixed checkwiki error 18 using [[Project:AWB|AWB]] (8717)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3393">{{other uses}}
The '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.&lt;ref&gt;Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland&lt;/ref&gt;&lt;ref&gt;Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition&lt;/ref&gt; It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.

MAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.&lt;ref&gt;Manning, C. D. and Schütze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.&lt;/ref&gt; The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.

In MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.&lt;ref&gt;European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)&lt;/ref&gt;

MAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics – news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.&lt;ref&gt;Järvelin A. , Talvensaari T. , Järvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)&lt;/ref&gt; Since the patent document refers to the same “invention” or “concept of idea” the text is a translation of the invention, but it does not have to be a direct translation of the text itself – text parts could have been removed or added for clarification reasons.

The 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.

== Use Cases ==
* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.

== References ==
{{Reflist}}

== External links ==
* [http://www.ir-facility.org/prototypes/marec User guide and statistics]
* [http://ir-facility.org Information Retrieval Facility]

[[Category:Corpora]]
[[Category:Information retrieval]]
[[Category:Machine translation]]
[[Category:Natural language processing]]
[[Category:XML]]</text>
      <sha1>5a77aobnrof5sndhltwufso0quas06w</sha1>
    </revision>
  </page>
  <page>
    <title>Learning to rank</title>
    <ns>0</ns>
    <id>25050663</id>
    <revision>
      <id>634286702</id>
      <parentid>633783916</parentid>
      <timestamp>2014-11-17T22:24:04Z</timestamp>
      <contributor>
        <username>Nonlogic a</username>
        <id>23212263</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27552">'''Learning to rank'''&lt;ref name=&quot;liu&quot;&gt;{{citation
|author=Tie-Yan Liu
|title=Learning to Rank for Information Retrieval
|series=Foundations and Trends in Information Retrieval: Vol. 3: No 3
|year=2009
|isbn=978-1-60198-244-5
|doi=10.1561/1500000016
|pages=225–331
|journal=Foundations and Trends® in Information Retrieval
|volume=3
|issue=3
}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]
&lt;/ref&gt; or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.&lt;/ref&gt; Training data consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. &quot;relevant&quot; or &quot;not relevant&quot;) for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is &quot;similar&quot; to rankings in the training data in some sense.

Learning to rank is a relatively new research area which has emerged in the past decade.

== Applications ==

=== In information retrieval ===
[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]
Ranking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], [[computational advertising]] (online ad placement).

A possible architecture of a machine-learned search engine is shown in the figure to the right.

Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),
&lt;!-- &quot;assessor&quot; is the more standard term, used e.g. by TREC conference --&gt;
who check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used — only the top few documents, retrieved by some existing ranking models are checked. &lt;!--
  TODO: write something about selection bias caused by pooling
--&gt; Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),&lt;ref name=&quot;Joachims2002&quot;&gt;{{citation
 | author=Joachims, T.
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
 | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf
 | title=Optimizing Search Engines using Clickthrough Data
 | year=2002
}}&lt;/ref&gt; ''query chains'',&lt;ref&gt;{{citation
 | author=Joachims T., Radlinski F.
 | title=Query Chains: Learning to Rank from Implicit Feedback
 | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf
 | year=2005
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
}}&lt;/ref&gt; or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].

Training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.

Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.&lt;ref&gt;{{citation
 | author=B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt.
 | title=Early exit optimizations for additive machine learned ranking systems
 | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. (to appear)
 | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf
}}&lt;/ref&gt; First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,&lt;ref&gt;{{citation
 | author=Broder A., Carmel D., Herscovici M., Soffer A., Zien J.
 | title=Efficient query evaluation using a two-level retrieval process
 | journal=Proceedings of the twelfth international conference on Information and knowledge management
 | year=2003
 | pages=426–434
 | isbn=1-58113-723-0
 | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 }}&lt;/ref&gt; [[Okapi BM25|BM25]]. This phase is called ''top-&lt;math&gt;k&lt;/math&gt; document retrieval'' and many good heuristics were proposed in the literature to accelerate it, such as using document's static quality score and tiered indexes.&lt;ref name=&quot;manning-q-eval&quot;&gt;{{citation
 | author=Manning C.,  Raghavan P. and Schütze H.
 | title=Introduction to Information Retrieval
 | publisher=Cambridge University Press
 | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]&lt;/ref&gt; In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===
Learning to rank algorithms have been applied in areas other than information retrieval:
* In [[machine translation]] for ranking a set of hypothesized translations;&lt;ref name=&quot;Duh09&quot;&gt;{{citation
 | author=Kevin K. Duh
 | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data
 | year=2009
 | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf
}}&lt;/ref&gt;
* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.&lt;ref name=&quot;Duh09&quot; /&gt;
* In [[proteomics]] for the identification of frequent top scoring peptides.&lt;ref name=&quot;Hen09&quot;&gt;{{citation
 | author=Henneges C., Hinselmann G., Jung S., Madlung J., Schütz W., Nordheim A., Zell A.
 | title=Ranking Methods for the Prediction of Frequent Top Scoring Peptides from Proteomics Data
 | year=2009
 | url=http://www.omicsonline.com/ArchiveJPB/2009/May/01/JPB2.226.pdf
}}&lt;/ref&gt;
* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.&lt;ref&gt;Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.&lt;/ref&gt;

== Feature vectors ==
For convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such approach is sometimes called ''bag of features'' and is analogous to [[bag of words]] and [[vector space model]] used in information retrieval for representation of documents.

Components of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):
* ''Query-independent'' or ''static'' features — those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.&lt;ref name=&quot;manning-q-eval&quot; /&gt;&lt;ref&gt;
{{cite conference
 | first=M. |last=Richardson
 | coauthors=Prakash, A. and Brill, E.
 | title=Beyond PageRank: Machine Learning for Static Ranking
 | booktitle=Proceedings of the 15th International World Wide Web Conference
 | pages=707–715
 | publisher=
 | year=2006
 | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf
 | accessdate=
 }}&lt;/ref&gt;
* ''Query-dependent'' or ''dynamic'' features — those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.
* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''

Some examples of features, which were used in the well-known [[LETOR]] dataset:&lt;ref name=&quot;letor3&quot;&gt;[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]&lt;/ref&gt;
* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;
* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;
* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.

Selecting and designing good features is an important area in machine learning, which is called [[feature engineering]].

== Evaluation measures ==
There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.

Examples of ranking quality measures:
* [[Mean average precision]] (MAP);
* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];
* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where &quot;@''n''&quot; denotes that the metrics are evaluated only on top ''n'' documents;
* [[Mean reciprocal rank]];
* [[Kendall's tau]]
* [[Spearman's rank correlation coefficient|Spearman's Rho]]

DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.&lt;ref&gt;http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt&lt;/ref&gt; Other metrics such as MAP, MRR and precision, are defined only for binary judgements.

Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* [[Expected reciprocal rank]] (ERR);&lt;ref&gt;{{citation
|author=Olivier Chapelle, Donald Metzler, Ya Zhang, Pierre Grinspan
|title=Expected Reciprocal Rank for Graded Relevance
|url=http://research.yahoo.com/files/err.pdf
|journal=CIKM
|year=2009
|pages=
}}&lt;/ref&gt;
* [[Yandex]]'s pfound.&lt;ref&gt;{{citation
|author=Gulin A., Karpovich P., Raskovalov D., Segalovich I.
|title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods
|url=http://romip.ru/romip2009/15_yandex.pdf
|journal=Proceedings of ROMIP'2009
|year=2009
|pages=163–168
}} (in Russian)&lt;/ref&gt;
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==
{{Expand section|date=December 2009}}
Tie-Yan Liu of [[Microsoft Research Asia]] in his paper &quot;Learning to Rank for Information Retrieval&quot;&lt;ref name=&quot;liu&quot; /&gt; and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and [[loss function]]:

=== Pointwise approach ===
In this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.

A number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===
In this case learning-to-rank problem is approximated by a classification problem — learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.

=== Listwise approach ===
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class=&quot;wikitable sortable&quot;
! Year || Name || Type || Notes
|-
| 1989 || OPRF &lt;ref name=&quot;Fuhr1989&quot;&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=ACM Transactions on Information Systems
 | title=Optimum polynomial retrieval functions based on the probability ranking principle
 | volume=7
 | number=3
 | pages=183–204 
 | year=1989
 | doi=10.1145/65943.65944
}}&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || SLR &lt;ref name=&quot;Cooperetal1992&quot;&gt;{{citation
 | author=Cooper, William S.; Gey, Frederic C.; Dabney, Daniel P.
 | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval 
 | title=Probabilistic retrieval based on staged logistic regression
 | pages=198–210 
 | year=1992
 | doi=10.1145/133160.133199
}}&lt;/ref&gt;   || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pointwise || Staged logistic regression
|-
| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||  A more recent exposition is in,&lt;ref name=&quot;Joachims2002&quot; /&gt; which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking&lt;ref&gt;{{cite paper | id = {{citeseerx|10.1.1.20.378}} | title = Pranking }}&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;1&lt;/span&gt; pointwise || Ordinal regression.
|-
| 2003 &lt;!-- or 1998? --&gt; || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
|-
| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
|-
| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; pairwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || 
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || &lt;span style=&quot;display:none&quot;&gt;1&lt;/span&gt; pointwise ||
|-
| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
|-
| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || RankGP&lt;ref&gt;{{cite paper | id = {{citeseerx|10.1.1.90.220}} | title = RankGP }}&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise ||
Regularized least-squares based ranking. The work is extended in
&lt;ref name=pahikkala2009efficient&gt;{{Citation|last=Pahikkala|first=Tapio|coauthors=Tsivtsivadze, Evgeni, Airola, Antti, Järvinen, Jouni, Boberg, Jorma|title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129–165|doi=10.1007/s10994-008-5097-z|postscript=.}}&lt;/ref&gt; to learning to rank from general preference graphs.
|-
| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM&lt;sup&gt;map&lt;/sup&gt;] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.&lt;ref&gt;C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].&lt;/ref&gt;
|-
| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || Based on ListNet.
|-
| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]&lt;ref&gt;Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]&lt;ref&gt;Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.&lt;/ref&gt;  || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]&lt;ref&gt;Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF &quot;SortNet: learning to rank by a neural-based sorting algorithm&quot;], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. 
|-
| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || Based on ListNet.
|-
| 2010 || [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]&lt;ref&gt;Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.&lt;/ref&gt; || &lt;span style=&quot;display:none&quot;&gt;3&lt;/span&gt; listwise || A boosting approach to optimize NDCG.
|-
| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pairwise &amp; listwise || 
|-
| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || &lt;span style=&quot;display:none&quot;&gt;2&lt;/span&gt; pointwise &amp; pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|}

Note: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==
[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;&lt;ref name=&quot;Fuhr1992&quot;&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=Computer Journal
 | title=Probabilistic Models in Information Retrieval
 | volume=35
 | number=3
 | pages=243–255
 | year=1992
 | doi=10.1093/comjnl/35.3.243
}}&lt;/ref&gt; a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.&lt;ref name=&quot;Fuhr1989&quot; /&gt; Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 &lt;ref name=&quot;Cooperetal1992&quot; /&gt; and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.&lt;ref&gt;{{citation |author=Manning C.,  Raghavan P. and Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]&lt;/ref&gt;  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.

Several conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

=== Practical usage by search engines ===
Commercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.&lt;ref&gt;Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]&lt;/ref&gt;&lt;ref&gt;{{US Patent|7197497}}&lt;/ref&gt;

[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,&lt;ref&gt;[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]&lt;/ref&gt;{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.

In November 2009 a Russian search engine [[Yandex]] announced&lt;ref name=&quot;snezhinsk&quot;&gt;[http://webmaster.ya.ru/replies.xml?item_no=5707&amp;ncrnd=5118 Yandex corporate blog entry about new ranking model &quot;Snezhinsk&quot;] (in Russian)&lt;/ref&gt; that it had significantly increased its [[search quality]] due to deployment of a new proprietary MatrixNet algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.&lt;ref&gt;The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].&lt;/ref&gt; Recently they have also sponsored a machine-learned ranking competition &quot;Internet Mathematics 2009&quot;&lt;ref&gt;[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]&lt;/ref&gt; based on their own search engine's production data. Yahoo has announced a similar competition in 2010.&lt;ref&gt;[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]&lt;/ref&gt;

As of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.&lt;ref&gt;{{cite web
  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
  | archiveurl = http://www.webcitation.org/5sq8irWNM
  | archivedate = 2010-09-18
  | title = Are Machine-Learned Models Prone to Catastrophic Errors?
  | date = 2008-05-24
  | last = Rajaraman
  | first = Anand
  | authorlink = Anand Rajaraman}}&lt;/ref&gt; [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models &quot;learn what people say they like, not what people actually like&quot;.&lt;ref&gt;{{cite web
  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archiveurl = http://www.webcitation.org/5sq7DX3Pj
  | archivedate = 2010-09-15
  | title = Cuil Blog: So how is Bing doing?
  | date = 2009-06-26
  | last = Costello
  | first = Tom}}&lt;/ref&gt;

== References ==
{{reflist|2}}

== External links ==
; Competitions and public datasets
* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]
* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]
* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]
* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]

; Open Source code
* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]
* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]
* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]

[[Category:Information retrieval]]
[[Category:Machine learning]]
[[Category:Ranking functions]]</text>
      <sha1>latxe64wduvwmzrugp2jtio9jcsy2hl</sha1>
    </revision>
  </page>
  <page>
    <title>Rocchio algorithm</title>
    <ns>0</ns>
    <id>25130414</id>
    <revision>
      <id>623687272</id>
      <parentid>572330307</parentid>
      <timestamp>2014-09-01T09:31:48Z</timestamp>
      <contributor>
        <username>Stylor</username>
        <id>19000158</id>
      </contributor>
      <minor/>
      <comment>/* Time complexity */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7270">The '''Rocchio algorithm''' is based on a method of [[relevance feedback]] found in [[information retrieval]] systems which stemmed from the [[SMART Information Retrieval System]] around the year 1970. Like many other retrieval systems, the Rocchio feedback approach was developed using the [[Vector Space Model]].  The [[algorithm]] is based on the assumption that most users have a general conception of which documents should be denoted as  [[Relevance (information retrieval)|relevant]] or non-relevant.&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 181. Cambridge University Press, 2009.&lt;/ref&gt;  Therefore, the user's search query is revised to include an arbitrary percentage of  relevant and non-relevant documents as a means of increasing the [[search engine]]'s [[Information_retrieval#Recall|recall]], and possibly the precision as well.  The number of  relevant and non-relevant documents allowed to enter a [[Information retrieval|query]] is dictated by the weights of the a, b, c variables listed below in the [[Rocchio_Classification#Algorithm|Algorithm section]].&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 292. Cambridge University Press, 2009.&lt;/ref&gt;

==Algorithm==
The [[Formula (mathematical logic)|formula]] and variable definitions for Rocchio relevance feedback is as follows:&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 182. Cambridge University Press, 2009.&lt;/ref&gt;

&lt;math&gt; \overrightarrow{Q_m} = \bigl(a \cdot \overrightarrow{Q_o}\bigr) + \biggl(b \cdot {\tfrac{1}{|D_r|}} \cdot \sum_{\overrightarrow{D_j} \in D_r} \overrightarrow{D_j}\biggr)
- \biggl(c \cdot {\tfrac{1}{|D_{nr}|}} \cdot \sum_{\overrightarrow{D_k} \in D_{nr}} \overrightarrow{D_k}\biggr) &lt;/math&gt;

{| class=&quot;wikitable&quot;
|-
! Variable
! Value
|-
| &lt;math&gt; \overrightarrow{Q_m} &lt;/math&gt;
| Modified Query Vector
|-
| &lt;math&gt; \overrightarrow{Q_o} &lt;/math&gt;
| Original Query Vector
|-
| &lt;math&gt; \overrightarrow{D_j} &lt;/math&gt;
| Related Document Vector
|-
| &lt;math&gt; \overrightarrow{D_k} &lt;/math&gt;
| Non-Related Document Vector
|-
| &lt;math&gt; a &lt;/math&gt;
| Original Query Weight
|-
| &lt;math&gt; b &lt;/math&gt;
| Related Documents Weight
|-
| &lt;math&gt; c &lt;/math&gt;
| Non-Related Documents Weight
|-
| &lt;math&gt; D_r &lt;/math&gt;
| Set of Related Documents
|-
| &lt;math&gt; D_{nr} &lt;/math&gt;
| Set of Non-Related Documents
|}
[[Image:Rocchioclassgraph.jpg|thumb|right|250px|Rocchio Classification]]

As demonstrated in the Rocchio formula, the associated weights ('''a''', '''b''', '''c''') are responsible for shaping the modified [[vector space|vector]] in a direction closer, or farther away, from the original query, related documents, and non-related documents.  In particular, the values for '''b''' and '''c''' should be incremented or decremented proportionally to the set of documents classified by the user.  If the user decides that the modified query should not contain terms from either the original query, related documents, or non-related documents, then the corresponding weight ('''a''', '''b''', '''c''') value for the category should be set to 0.

In the later part of the algorithm, the variables '''Dr''', and '''Dnr''' are presented to be sets of [[Tuple|vectors]] containing the coordinates of related documents and non-related documents.  Though '''Dr''' and '''Dnr''' are not  vectors themselves, &lt;math&gt; \overrightarrow{Dj} &lt;/math&gt; and &lt;math&gt; \overrightarrow{Dk} &lt;/math&gt; are the vectors used to iterate through the two sets and form vector [[summation]]s. These summations will be multiplied against the [[Multiplicative inverse]] of their respective document set ('''Dr''', '''Dnr''') to complete the addition or subtraction of related or non-related documents.

In order to visualize the changes taking place on the modified vector, please refer to the image below.&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 293. Cambridge University Press, 2009.&lt;/ref&gt; As the weights are increased or decreased for a particular category of documents, the coordinates for the modified vector begin to move either closer, or farther away, from the [[centroid]] of the document collection. Thus if the weight is increased for related documents, then the modified vectors [[coordinate]]s will reflect being closer to the centroid of related documents.

==Time complexity==
The [[time complexity]] for training and testing the algorithm are listed below and followed by the definition of each [[variable (mathematics)|variable]]. Note that when in testing phase, the time complexity can be reduced to that of calculating the [[euclidean distance]] between a class [[centroid]] and the respective document.  As shown by: &lt;math&gt;\Theta(\vert\mathbb{C}\vert M_{a})&lt;/math&gt;.

Training = &lt;math&gt;\Theta(\vert\mathbb{D}\vert L_{ave}+\vert\mathbb{C}\vert\vert V\vert)&lt;/math&gt; &lt;br&gt;
Testing = &lt;math&gt;\Theta( L_{a}+\vert\mathbb{C}\vert M_{a})= \Theta(\vert\mathbb{C}\vert M_{a})&lt;/math&gt; &lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 296. Cambridge University Press, 2009.&lt;/ref&gt;

{| class=&quot;wikitable&quot;
|-
! Variable
! Value
|-
| &lt;math&gt; \mathbb{D} &lt;/math&gt;
| Labeled Document Set
|-
| &lt;math&gt; L_{ave} &lt;/math&gt;
| Average Tokens Per Document
|-
| &lt;math&gt; \mathbb{C} &lt;/math&gt;
| Class Set
|-
| &lt;math&gt; V &lt;/math&gt;
| Vocabulary/Term Set
|-
| &lt;math&gt; L_{a} &lt;/math&gt;
| Number of Tokens in Document
|-
| &lt;math&gt; M_{a} &lt;/math&gt;
| Number of Types in Document
|}

==Usage==
Though there are benefits to ranking documents as not-relevant, a [[relevant]] document ranking will result in more precise documents being made available to the user. Therefore, traditional values for the algorithm's weights ('''a''', '''b''', '''c''') in Rocchio Classification are typically around '''a = 1''', '''b = 0.8''', and ''' c = 0.1'''. Modern [[information retrieval]] systems have moved towards eliminating the non-related documents by setting '''c = 0''' and thus only accounting for related documents. Although not all [[Information retrieval|retrieval systems]] have eliminated the need for non-related documents, most have limited the effects on modified query by only accounting for strongest non-related documents in the '''Dnr''' set.

==Limitations==
The Rocchio algorithm often fails to classify multimodal classes and relationships. For instance, the country of [[Burma]] was renamed to [[Myanmar]] in 1989. Therefore the two queries of &quot;Burma&quot; and &quot;Myanmar&quot; will appear much farther apart in the [[vector space model]], though they both contain similar origins.&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: ''An Introduction to Information Retrieval'', page 296. Cambridge University Press, 2009.&lt;/ref&gt;

== See also ==
*  [[Nearest centroid classifier]], aka Rocchio classifier

==References==
{{reflist}}
* [http://nlp.stanford.edu/IR-book/pdf/09expand.pdf Relevance Feedback and Query Expansion]
* [http://nlp.stanford.edu/IR-book/pdf/14vcat.pdf Vector Space Classification]
* [http://cs.nyu.edu/courses/fall07/G22.2580-001/lec7.html Data Classification]

[[Category:Information retrieval]]</text>
      <sha1>5hvdh8dg6wms4qkpxvubwcm0pt1kigj</sha1>
    </revision>
  </page>
  <page>
    <title>Taganode Local Search Engine</title>
    <ns>0</ns>
    <id>25256397</id>
    <revision>
      <id>532120594</id>
      <parentid>508642280</parentid>
      <timestamp>2013-01-09T05:12:12Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Removing Orphan Tag (Nolonger an Orphan) ([[User_talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1782">'''The Open Local Search Engine from Taganode''' is a [[search engine]] specifically targeting [[mobile phone]]s. It is based on local [[search algorithm]]s to find new places of interest within a specified distance.

The Taganode search engine offers an Open Developer [[Application programming interface|API]] that any one can use freely when writing new applications for [[iPhone]]s, [[Android (operating system)|Android phones]] and other platforms.

The search engine is optimized for mobile phones by low [[Bandwidth (computing)|bandwidth]] usage and only makes the simplest service calls to try to be compatible with as many mobile phones as possible. The Taganode search service is at this moment present in [[London]], [[Rome]], [[Venice]], [[Amsterdam]], [[Berlin]], [[Sweden]] and in [[Denmark]].

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
*{{cite news|url=http://www.webfinanser.com/nyheter/164362/taganode-gratis-reseguide-finns-nu-aven-i-venedig/|title=Taganode – Gratis reseguide finns nu även i Venedig |date=2009-10-12|work=Webfinanser|language=Swedish|accessdate=18 December 2009}}
*{{cite news|url=http://www.webfinanser.com/nyheter/159761/en-ny-och-innovativ-soktjanst-for-resenarer-i-europa/|title=En ny och innovativ söktjänst för resenärer i Europa|date=September 19, 2009|work=Webfinanser |language=Swedish|accessdate=18 December 2009}}

== External links ==
* [http://www.taganode.com Official site]
* [http://www.mynewsdesk.com/se/view/pressrelease/taganode-free-guide-now-available-in-rome-325701/  Taganode Service in Venice] (press release)

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Mobile phones]]</text>
      <sha1>oxig1jkmvv7ckr01umdgc49yarbuuw1</sha1>
    </revision>
  </page>
  <page>
    <title>Information Retrieval Specialist Group</title>
    <ns>0</ns>
    <id>10218640</id>
    <revision>
      <id>383629057</id>
      <parentid>337601902</parentid>
      <timestamp>2010-09-08T11:58:46Z</timestamp>
      <contributor>
        <username>SmackBot</username>
        <id>433328</id>
      </contributor>
      <minor/>
      <comment>Date maintenance tags and general fixes: build 510:</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1469">{{Unreferenced|date=January 2010}}

The '''Information Retrieval Specialist Group''' ('''IRSG''') or '''BCS-IRSG''' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called ''The Informer'', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}

==European Conference on Information Retrieval==
Organising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name &quot;Annual Colloquium on Information Retrieval Research&quot;, and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.

== External links ==
* [http://irsg.bcs.org/ IRSG website]

[[Category:Information retrieval|Specialist Group]]
[[Category:BCS Specialist Groups]]</text>
      <sha1>obmki3tf9mx6wexolmqzfgi7sad1jx5</sha1>
    </revision>
  </page>
  <page>
    <title>Query expansion</title>
    <ns>0</ns>
    <id>7872152</id>
    <revision>
      <id>618192608</id>
      <parentid>600301525</parentid>
      <timestamp>2014-07-23T23:43:29Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6447">'''Query expansion''' ('''QE''') is the process of reformulating a seed query to improve retrieval performance in [[information retrieval]] operations.&lt;ref&gt;{{cite journal
 | last = Vectomova | first = Olga |author2=Wang, Ying  | year = 2006
 | title = A study of the effect of term proximity on query expansion | journal = [[Journal of Information Science]]
 | volume = 32 | issue = 4 | pages = 324&amp;ndash;333
 | doi = 10.1177/0165551506065787 | id =  | url = http://jis.sagepub.com/cgi/content/abstract/32/4/324
 | format = Abstract | accessdate = 2006-12-09
 }}&lt;/ref&gt;
In the context of web [[search engine]]s, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of [[data]]) and expanding the search query to match additional documents.  Query expansion involves techniques such as:

* Finding [[synonym]]s of words, and searching for the synonyms as well
* Finding all the various [[Morphology (linguistics)|morphological]] forms of words by [[stemming]] each word in the [[search query]]
* Fixing [[Typographical error|spelling errors]] and automatically searching for the corrected form or suggesting it in the results
* Re-weighting the terms in the original query

Query expansion is a methodology studied in the field of [[computer science]], particularly within the realm of [[natural language processing]] and [[information retrieval]].

== Precision and recall tradeoffs ==

Search engines invoke query expansion to increase the quality of user search results.  It is assumed that users do not always formulate search queries using the best terms. Best in this case may be because the database does not contain the user entered terms.  

By [[stemming]] a user-entered term, more documents are matched, as the alternate word forms for a user entered term are matched as well, increasing the total [[recall (information retrieval)|recall]]. This comes at the expense of reducing the [[precision (information retrieval)|precision]].  By expanding a search query to search for the synonyms of a user entered term, the recall is also increased at the expense of precision.  This is due to the nature of the equation of how precision is calculated, in that a larger recall implicitly causes a decrease in precision, given that factors of recall are part of the denominator. It is also inferred that a larger recall negatively impacts overall search result quality, given that many users do not want more results to comb through, regardless of the precision.

The goal of query expansion in this regard is by increasing recall, precision can potentially increase (rather than decrease as mathematically equated), by including in the result set pages which are more relevant (of higher quality), or at least equally relevant. Pages which would not be included in the result set, which have the potential to be more relevant to the user's desired query, are included, and without query expansion would not have, regardless of relevance.  At the same time, many of the current commercial search engines use word frequency ([[Tf-idf]]) to assist in ranking.  By ranking the occurrences of both the user entered words and synonyms and alternate morphological forms, documents with a higher density (high frequency and close proximity) tend to migrate higher up in the search results, leading to a higher quality of the search results near the top of the results, despite the larger recall.

This tradeoff is one of the defining problems in query expansion, regarding whether it is worthwhile to perform given the questionable effects on precision and recall. Critics{{Who|date=March 2009}} state one of the problems is that the dictionaries and [[thesauri]], and the stemming algorithm, are driven by human bias and while this is implicitly handled by the query expansion algorithm, this explicitly affects the results in a non-automated manner (similar to how statisticians can 'lie' with statistics){{Citation needed|date=July 2013}}. Other critics{{Who|date=March 2009}} point out potential for corporate influence on the dictionaries, promoting advertising of online web pages in the case of [[web search engine]]s. {{Citation needed|date=December 2007}}

==See also==

* [[Search engine]]
* [[Search engine indexing]]
* [[Information retrieval]]
* [[Document retrieval]]
* [[Linguistics]]
* [[Natural language processing]]
* [[Stemming]]
* [[Morphology (linguistics)]]

== Software libraries ==
*[http://qtanalyzer.codeplex.com/ QueryTermAnalyzer] open-source, C#. Machine learning based query term weight and synonym analyzer for query expansion.
*[http://lucene-qe.sourceforge.net/ LucQE] - open-source, Java.  Provides a framework along with several implementations that allow to perform query expansion with the use of Apache [[Lucene]].
*[[Xapian]] is an open-source search library which includes support for query expansion

== References ==

* D. Abberley, D. Kirby, S. Renals, and T. Robinson, The THISL broadcast news  retrieval system. In ''Proc. ESCA ETRW Workshop Accessing Information in Spoken Audio'', (Cambridge), pp.&amp;nbsp;14–19, 1999. Section on [http://homepages.inf.ed.ac.uk/srenals/pubs/1999/esca99-thisl/node6.html Query Expansion] - Concise, mathematical overview.
* R. Navigli, P. Velardi. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&amp;nbsp;42–49 - An analysis of query expansion methods relying on WordNet as the reference ontology.
* Y. Qiu and H.P. Frei. [http://citeseer.ist.psu.edu/qiu93concept.html Concept Based Query Expansion]. In ''Proceedings of SIGIR-93, 16th ACM International Conference on Research and Development in Information Retrieval'', Pittsburgh, SIGIR Forum, ACM Press, June 1993 - Academic document on a specific method of query expansion
* Efthimis N. Efthimiadis. [http://faculty.washington.edu/efthimis/pubs/Pubs/qe-arist/QE-arist.html Query Expansion]. In: Martha E. Williams (ed.), ''Annual Review of Information Systems and Technology (ARIST)'', v31, pp 121–187, 1996 - An introduction for less-technical viewers.

=== Notes ===
{{reflist}}

{{DEFAULTSORT:Query Expansion}}
[[Category:Information retrieval]]
[[Category:Searching]]</text>
      <sha1>5ns0mq85p9vdtlry1vc53dg9uucy43e</sha1>
    </revision>
  </page>
  <page>
    <title>Binary Independence Model</title>
    <ns>0</ns>
    <id>25957127</id>
    <revision>
      <id>646025306</id>
      <parentid>646025188</parentid>
      <timestamp>2015-02-07T11:45:23Z</timestamp>
      <contributor>
        <username>HelpUsStopSpam</username>
        <id>24038232</id>
      </contributor>
      <comment>Also link [[Stephen Robertson (computer scientist)]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5009">{{context|date=June 2012}}
The '''Binary Independence Model''' (BIM)&lt;ref name=&quot;cyu76&quot; /&gt;&lt;ref name=&quot;jones77&quot;/&gt; is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.

==Definitions==
The Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.
The representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x&lt;sub&gt;1&lt;/sub&gt;, ..., x&lt;sub&gt;m&lt;/sub&gt;)'' where ''x&lt;sub&gt;t&lt;/sub&gt;=1'' if term ''t'' is present in the document ''d'' and ''x&lt;sub&gt;t&lt;/sub&gt;=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.
&quot;Independence&quot; signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the &quot;naive&quot; assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.

The probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:

&lt;math&gt;P(R|x,q) = \frac{P(x|R,q)*P(R|q)}{P(x|q)}&lt;/math&gt;

where ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.
The exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.

''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.
Since a document is either relevant or nonrelevant to a query we have that:

&lt;math&gt;P(R=1|x,q) + P(R=0|x,q) = 1&lt;/math&gt;

=== Query Terms Weighting ===
Given a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the
terms in the query such that the retrieval effectiveness will be high. Let &lt;math&gt;p_i&lt;/math&gt; and &lt;math&gt;q_i&lt;/math&gt; be the probability that a relevant document and an irrelevant document has the &lt;math&gt;i^{th}&lt;/math&gt; term respectively. Yu and [[Gerard Salton|Salton]],&lt;ref name=&quot;cyu76&quot; /&gt; who first introduce BIM, propose that the weight of the &lt;math&gt;i^{th}&lt;/math&gt; term is an increasing function of &lt;math&gt;Y_i =  \frac{p_i *(1-q_i)}{(1-p_i)*q_i}&lt;/math&gt;. Thus, if &lt;math&gt;Y_i&lt;/math&gt; is higher than &lt;math&gt;Y_j&lt;/math&gt;, the weight
of term &lt;math&gt;i&lt;/math&gt; will be higher than that of term &lt;math&gt;j&lt;/math&gt;. Yu and Salton&lt;ref name=&quot;cyu76&quot; /&gt; showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Spärck Jones|Spärck Jones]]&lt;ref name=&quot;jones77&quot;/&gt; later showed that if the &lt;math&gt;i^{th}&lt;/math&gt; term is assigned the weight of &lt;math&gt;log Y_i&lt;/math&gt;, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.

The Binary Independence Model was introduced by Yu and Salton.&lt;ref name=&quot;cyu76&quot; /&gt; The name Binary Independence Model was coined by Robertson and Spärck Jones.&lt;ref name=&quot;jones77&quot;/&gt;

== See also ==

* [[Bag of words model]]

==Further reading==
* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | coauthors=Prabhakar Raghavan &amp; Hinrich Schütze | publisher=Cambridge University Press | year=2008}}
* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&amp;uuml;ttcher | coauthors=Charles L. A. Clarke &amp; Gordon V. Cormack | publisher=MIT Press | year=2010}}

==References==
{{Reflist|refs=
&lt;ref name=&quot;cyu76&quot;&gt;{{cite doi | 10.1145/321921.321930 }}&lt;/ref&gt;
&lt;ref name=&quot;jones77&quot;&gt;{{cite doi | 10.1002/asi.4630270302 }}&lt;/ref&gt; 
}}

[[Category:Information retrieval]]
[[Category:Probabilistic models]]</text>
      <sha1>t3l80yjzh0t40lzowszpiht1iy9ymne</sha1>
    </revision>
  </page>
  <page>
    <title>Probabilistic relevance model</title>
    <ns>0</ns>
    <id>25959000</id>
    <revision>
      <id>619372510</id>
      <parentid>594893241</parentid>
      <timestamp>2014-08-01T02:19:38Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>fixed [[Help:CS1 errors#bad_date|CS1 errors: dates]] to meet [[MOS:DATEFORMAT]] (also [[WP:AWB/GF|General fixes]]), added [[CAT:UL|underlinked]] tag using [[Project:AWB|AWB]] (10331)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1787">{{Underlinked|date=August 2014}}

The '''probabilistic relevance model'''&lt;ref&gt;{{citation | author=S. E. Robertson | coauthors=K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129–146 | date=May–June 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}&lt;/ref&gt; was devised by Robertson and Jones as a framework for probabilistic models to come.
 
It makes an estimation of the probability of finding if a document ''d&lt;sub&gt;j&lt;/sub&gt;'' is relevant to a query ''q''. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query ''q''. Such an ideal answer set is called ''R'' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set ''R'' are relevant to the query, while documents not present in the set are non-relevant.

&lt;math&gt;sim(d_{j},q) = \frac{P(R|\vec{d}_j)}{P(\bar{R}|\vec{d}_j)}&lt;/math&gt;

==Related models==
There are some limitations to this framework that need to be addressed by further development:
* There is no accurate estimate for the first run probabilities
* Index terms are not weighted
* Terms are assumed mutually independent

To address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and it's BM25F brother.

==References==
{{reflist}}

[[Category:Information retrieval]]
[[Category:Probabilistic models]]</text>
      <sha1>hk89pc0av1r73lqvcnzxwxds1qlx5b1</sha1>
    </revision>
  </page>
  <page>
    <title>Uncertain inference</title>
    <ns>0</ns>
    <id>25962276</id>
    <revision>
      <id>648199328</id>
      <parentid>604151084</parentid>
      <timestamp>2015-02-21T16:04:55Z</timestamp>
      <contributor>
        <username>Krishnachandranvn</username>
        <id>2657296</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4180">'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]&lt;ref&gt;{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481–485 | year=1986}}&lt;/ref&gt; as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.

==Definitions==
Rijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:

&lt;math&gt;P(d \to q)&lt;/math&gt;

A user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.
In many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference &lt;math&gt;d \to q&lt;/math&gt; is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.
Since ''d'' and ''q'' are both generated by users, they are error prone; thus &lt;math&gt;d \to q&lt;/math&gt; is uncertain. This will affect the plausibility of a given query.

By doing this it accomplishes two things:
* Separate the processes of revising probabilities from the logic
* Separate the treatment of relevance from the treatment of requests

[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.

Uncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.

==Example==
If we have a query of the form:

&lt;math&gt;q = A \wedge B \wedge C&lt;/math&gt;

where A, B and C are query assertions, then for a document D we want the probability:

&lt;math&gt;P (D \to (A \wedge B \wedge C))&lt;/math&gt;

If we transform this into the [[conditional probability]] &lt;math&gt;P ((A \wedge B \wedge C) | D)&lt;/math&gt; and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.

==Further work==
Croft and Krovetz&lt;ref&gt;{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author=W. B. Croft | coauthors=R. Krovetz | year=1988 }}&lt;/ref&gt; applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.

[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.

[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.

== See also ==
* [[Fuzzy logic]]
* [[Probabilistic logic]]
* [[Plausible reasoning]]
* [[Imprecise probability]]

==References==
{{reflist}}

[[Category:Fuzzy logic]]
[[Category:Information retrieval]]
[[Category:Inference]]</text>
      <sha1>8358l8w55tkd16i3grt4467v5wigdce</sha1>
    </revision>
  </page>
  <page>
    <title>Champion list</title>
    <ns>0</ns>
    <id>26304039</id>
    <revision>
      <id>506334321</id>
      <parentid>449540057</parentid>
      <timestamp>2012-08-08T03:25:17Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>[[WP:TSN|Typo fixing]], typos fixed: heighest → highest using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="568">{{orphan|date=January 2011}}

A '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].

[[Category:Information retrieval]]


{{computing-stub}}</text>
      <sha1>0cvw8eeoxdzsrvegkv9vroi41pbwugn</sha1>
    </revision>
  </page>
  <page>
    <title>Queries per second</title>
    <ns>0</ns>
    <id>26039201</id>
    <revision>
      <id>545860858</id>
      <parentid>528771979</parentid>
      <timestamp>2013-03-21T03:07:20Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q3414243]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="639">{{refimprove|date=February 2010}}
'''Queries Per Second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.&lt;ref&gt;[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]&lt;/ref&gt;&lt;ref&gt;[http://www.answers.com/topic/qps QPS definition at answers.com]&lt;/ref&gt;

High-traffic systems must watch their QPS in order to know when to scale the system to handle more load.

== References ==
{{reflist}}

[[Category:Units of measurement]]
[[Category:Information retrieval]]

{{computer-stub}}</text>
      <sha1>dpmtatv0bak24ktp7bmahczvz3w3zpt</sha1>
    </revision>
  </page>
  <page>
    <title>Database search engine</title>
    <ns>0</ns>
    <id>7330158</id>
    <revision>
      <id>557399077</id>
      <parentid>552231884</parentid>
      <timestamp>2013-05-29T19:47:58Z</timestamp>
      <contributor>
        <ip>78.250.114.241</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2630">There are several categories of search engine software:  Web search or full-text search (example: [[Lucene]]), database or structured data search (example: [[Dieselpoint]]), and mixed or [[enterprise search]] (example: [[Google Search Appliance]]).  The largest web search engines such as [[Google]] and [[Yahoo!]] utilize tens or hundreds of thousands of computers to process billions of web pages and return results for thousands of searches per second. High volume of queries and text processing requires the software to run in highly distributed environment with high degree of redundancy. Modern search engines have the following main components:

Searching for text-based content in [[databases]] or other [[structured data]] formats ([[XML]], [[Comma-separated values|CSV]], etc.) presents some special challenges and opportunities which a number of specialized search engines resolve.  Databases are slow when solving complex queries (with multiple logical or [[string matching]] arguments.  Databases allow logical queries which full-text search doesn't (use of multi-field boolean logic for instance).  There is no crawling necessary for a database since the data is already structured but it is often necessary to index the data in a more compact form designed to allow for faster search.

Database search engines were initially (and still usually are) included with major database software products.  As such, they are usually called indexing engines.  However, these indexing engines are relatively limited in their ability to customize indexing formats (compounding, normalization, transformation, [[transliteration]], etc.)   Usually they do not provide sophisticated data matching technology ([[string matching]], [[boolean logic]], algorithmic methods, search scripting, etc.).

In more advanced Database search systems relational databases are indexed by compounding multiple tables into a single table containing only the fields that need to be queried (or displayed in search results).  The actual data matching engines can include any functions from basic string matching, normalization, transformation,  Database search technology is heavily used by government database services, e-commerce companies, web advertising platforms, telecommunications service providers, etc.

==See also==

*[[Search engine]]
*[[Web crawler]]
*[[Search engine indexing]]
*[[Enterprise search]]

==External links==
* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]

{{DEFAULTSORT:Search Engine Technology}}
[[Category:Internet search engines]]
[[Category:Information retrieval]]</text>
      <sha1>29x9qhz3n1z6atz0ectkwx1kvlj66rg</sha1>
    </revision>
  </page>
  <page>
    <title>Greenpilot</title>
    <ns>0</ns>
    <id>26926858</id>
    <revision>
      <id>622945390</id>
      <parentid>545830725</parentid>
      <timestamp>2014-08-26T22:31:18Z</timestamp>
      <contributor>
        <username>SemanticMining</username>
        <id>22339547</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6894">{{COI|date=April 2010}}
The online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.

The project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst Köhler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgewählter Ort 2009).&lt;ref&gt;[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]&lt;/ref&gt;

==Objective==
The Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,&lt;ref&gt;[http://www.medpilot.de/ Medpilot portal]&lt;/ref&gt; also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.

==Technical Background==
Greenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:

* semantic search optimised for the fields of Medicine and Life Sciences
* a contextual analysis of texts taking synonyms and compounds into account
* multilingual and cross-language search
* linking of lay and expert vocabulary
The search results are generated from a search index.

Additionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.

==Contents==
The Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.
The following is a list of sources from November 2009:&lt;ref&gt;[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]&lt;/ref&gt;

===Library Catalogues===
* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)
* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)
* Catalogue of the Bonn University Library
* Library catalogues of scientifically relevant departments within the collective library network (GBV)
* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)
* Catalogue of the Johann Heinrich von Thünen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries
* Catalogue of the Julius Kühn-Institut, Federal Research Centre for Cultivated Plants
* Catalogue of the Friedrich Löffler-Institut, Federal Research Institute for Animal Health
* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food
* Catalogue of the Federal Institute for Risk Assessment
* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)
* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)
* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)
* Catalogue of the special collection inshore and deep-sea fishery
* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)
* Catalogue of the German National Library of Economics (ZBW)

===Bibliographic databases===
* AGRIS (1975–2008), FAO ( Food and Agriculture Organization of the United Nations)
* VITIS-VEA, Viticulture and Enology Abstracts
* Medline (2004–2009)
* UFORDAT, Environmental Research Database (UBA)
* ULIDAT, Environmental Literature Database (UBA)
* ELFIS, International Information System for the Agricultural Sciences and Technology

===Relevant Internet Sources===
* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture
* Open Access journals with full text documents

===Metasearch===
* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.
* ECONIS, Catalogue of the German National Library of Economics (ZBW).

==Other Features==

===Search and results page===
* Search and advanced search
* Context sensitive help function
* [[Truncation]] and [[Boolean function]]s
* Personalised refining of search results by filtering for a specific document type, language or database
* [[Bookmark]]s

===Document ordering===
* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).

===Personalisation===
* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.

==See also==
*[[List of digital library projects]]
*[[vascoda]]

==References==
&lt;references /&gt;

==External links==
* [http://www.greenpilot.de Greenpilot website]
* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]
* [http://www.land-of-ideas.org Germany - Land of Ideas website]

{{coord missing|Germany}}

[[Category:Libraries in Germany]]
[[Category:Information retrieval]]
[[Category:Internet search engines]]</text>
      <sha1>0oe5655hah28mwt7pe61zm4ri21pum3</sha1>
    </revision>
  </page>
  <page>
    <title>Anchor text</title>
    <ns>0</ns>
    <id>1225632</id>
    <revision>
      <id>639925658</id>
      <parentid>639817224</parentid>
      <timestamp>2014-12-28T10:00:28Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10538)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7183">{{Use dmy dates|date=February 2013}}
The '''anchor text''', '''link label''', '''link text''', or '''link title''' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, Web Search Engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],&lt;ref&gt;{{cite web|last=Bader Aljaber, Nicola Stokes, James Bailey and Jian Pei|url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}&lt;/ref&gt; and anchor text from documents linked in [[mind maps]] may be used too.&lt;ref&gt;Needs new reference link&lt;/ref&gt; [[File:Anchor text.png|thumb|Visual implementation of anchor text]]

==Overview==
Anchor text usually gives the user relevant descriptive or contextual information about the content of the link's destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]'s [[homepage]] might take this form:

:&lt;code&gt;&lt;nowiki&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Main_Page&quot;&gt;Wikipedia&lt;/a&gt;&lt;/nowiki&gt;&lt;/code&gt;

The anchor text in this example is &quot;Wikipedia&quot;; the longer, but vital, URL &lt;code&gt;&lt;nowiki&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/nowiki&gt;&lt;/code&gt; needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.

==Common misunderstanding of the concept==

This proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds [[significant]] [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}

==Search engine algorithms==
Anchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.&lt;ref name=&quot;Search Engine Watch&quot;&gt;{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=
How the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}&lt;/ref&gt;

[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.&lt;ref&gt;{{cite web
|last=Fox
|first=Vanessa
|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html
|title=Get a more complete picture about how other sites link to you
|date=15 March 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= http://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
In the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else's site to rank for an obscure or meaningless query.&lt;ref&gt;{{cite web
|last=Cutts
|first=Matt
|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html
|title=A quick word about Googlebombs
|date=25 January 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= http://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

In April 2012, Google announced in its March &quot;Penguin&quot; update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.&lt;ref&gt;{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google's March Update|publisher=Google}}&lt;/ref&gt;&lt;ref&gt;{{cite web|first=Simon|last=Dalley|accessdate=2012-04-04|date=4 April 2012|url=http://www.growtraffic.co.uk/google-changes-the-way-it-handles-anchor-text|title=Google Changes The way It Handles Anchor Text|publisher=Grow Traffic}}&lt;/ref&gt; Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.
.&lt;ref name=&quot;Search Engine Watch&quot;&gt;{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=
Google Penguin Update: Impact of Anchor Text Diversity &amp; Link Relevancy|accessdate=6 July 2012}}&lt;/ref&gt;

==Anchor Text Terminology==
There are different classifications of anchor text that are used within the search engine optimization community such as the following:

'''Exact Match:''' whenever an anchor is used with a keyword that mirrors the page that is being linked to. Example: &quot;[[search engine optimization]]&quot; is an exact match anchor because it's linking to a page about &quot;search engine optimization.

'''Branded:''' whenever a brand is used as the anchor. &quot;[[Wikipedia]]&quot; is a branded anchor text.

'''Naked Link:''' whenever a URL is used as an anchor. &quot;[[www.wikipedia.com]]&quot; is a naked link anchor.

'''Generic:''' whenever a generic word or phrase is used as the anchor. &quot;Click here&quot; is a generic anchor. Other variations may include &quot;go here&quot;, &quot;visit this website&quot;, etc.

'''Images:''' whenever an image is linked, Google will use the &quot;ALT&quot; tag as the anchor text
.&lt;ref&gt;{{cite web
|last=Gotch
|first=Nathan
|url=http://www.gotchseo.com/anchor-text/
|title=The Epic Guide to Anchor Text
|date=26 October 2014}}&lt;/ref&gt;

==References==

{{reflist|colwidth=30em}}

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Internet terminology]]
[[Category:Search engine optimization]]
[[Category:Hypertext]]</text>
      <sha1>fowjxmmffjfga0otvnubkjybqsr0xwv</sha1>
    </revision>
  </page>
  <page>
    <title>Music information retrieval</title>
    <ns>0</ns>
    <id>261193</id>
    <revision>
      <id>645400507</id>
      <parentid>644495223</parentid>
      <timestamp>2015-02-03T03:11:12Z</timestamp>
      <contributor>
        <username>Albany NY</username>
        <id>1783871</id>
      </contributor>
      <minor/>
      <comment>edited section headings per [[WP:HEAD]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9616">{{multiple issues|
{{technical|date=October 2012}}
{{Expert-subject|date=July 2010}}
{{Expert-subject|Science|date=July 2010}}
}}


'''Music information retrieval''' ('''MIR''') is the interdisciplinary science of retrieving [[information]] from [[music]]. MIR is a small but growing field of research with many real-world applications. Those involved in MIR may have a background in [[musicology]], [[psychology]], academic music study, [[signal processing]], [[machine learning]] or some combination of these.

== Applications ==
MIR is being used by businesses and academics to categorize, manipulate and even create music.

=== Recommender systems ===
Several [[recommender systems]] for music already exist, but surprisingly few are based upon MIR techniques, instead making use of similarity between users or laborious data compilation. [[Pandora]], for example, uses experts to tag the music with particular qualities such as &quot;female singer&quot; or &quot;strong bassline&quot;. Many other systems find users whose listening history is similar and suggests unheard music to the users from their respective collections. MIR techniques for similarity in music are now beginning to form part of such systems.

=== Track separation and instrument recognition ===
Track separation is about extracting the original tracks as recorded, which could have more than one instrument played per track. Instrument recognition is about identifying the instruments involved and/or separating the music into one track per instrument. Various programs have been developed that can separate music into its component tracks without access to the master copy. In this way e.g. karaoke tracks can be created from normal music tracks, though the process is not yet perfect owing to vocals occupying some of the same frequency space as the other instruments.

===Automatic music transcription===
Automatic music transcription is the process of converting an audio recording into symbolic notation, such as a score or a [[MIDI_file#File_formats|MIDI file]].&lt;ref&gt;A. Klapuri and M. Davy, editors. Signal Processing Methods for Music Transcription. Springer-Verlag, New York, 2006.&lt;/ref&gt; This process involves several subtasks, which include multi-pitch detection, [[Onset_detection#Onset_detection|onset detection]], duration estimation, instrument identification, and the extraction of rhythmic information. This task becomes more difficult with greater numbers of instruments and a greater [[Polyphony and monophony in instruments|polyphony level]].

===Automatic categorization===
Musical genre categorization is a common task for MIR and is the usual task for the yearly Music Information Retrieval Evaluation eXchange(MIREX).&lt;ref&gt;http://www.music-ir.org/mirex/wiki/MIREX_HOME - Music Information Retrieval Evaluation eXchange.&lt;/ref&gt; Machine learning techniques such as [[Support Vector Machines]] tend to perform well, despite the somewhat subjective nature of the classification. Other potential classifications include identifying the artist, the place of origin or the mood of the piece. Where the output is expected to be a number rather than a class, [[regression analysis]] is required.

===Music generation===
The automatic generation of music is a goal held by many MIR researchers. Attempts have been made with limited success in terms of human appreciation of the results.

==Methods used==

===Data source===
[[Sheet music|Scores]] give a clear and logical description of music from which to work, but access to sheet music, whether digital or otherwise, is often impractical. [[MIDI]] music has also been used for similar reasons, but some data is lost in the conversion to MIDI from any other format, unless the music was written with the MIDI standards in mind, which is rare. Digital audio formats such as [[WAV]], [[mp3]], and [[ogg]] are used when the audio itself is part of the analysis. Lossy formats such as mp3 and ogg work well with the human ear but may be missing crucial data for study. Additionally some encodings create artifacts which could be misleading to any automatic analyser. Despite this the ubiquity of the mp3 has meant much research in the field involves these as the source material. Increasingly, metadata mined from the web is incorporated in MIR for a more rounded understanding of the music within its cultural context, and this recently includes analysis of [[social tagging|social tags]] for music.

===Feature representation===
Analysis can often require some summarising,&lt;ref&gt;Eidenberger, Horst (2011). “Fundamental Media Understanding”, atpress. ISBN 978-3-8423-7917-6.&lt;/ref&gt; and for music (as with many other forms of data) this is achieved by feature extraction, especially when the audio content itself is analysed and machine learning is to be applied. The purpose is to reduce the sheer quantity of data down to a manageable set of values so that learning can be performed within a reasonable time-frame. One common feature extracted is the [[Mel-frequency cepstral coefficient|Mel-Frequency Cepstral Coefficient]] (MFCC) which is a measure of the [[timbre]] of a piece of music. Other features may be employed to represent the [[Tonality#Computational_methods_to_determine_the_key|key]], chords, harmonies, melody, main pitch, beats per minute or rhythm in the piece.

===Statistics and machine learning===
*Computational methods for classification, clustering, and modelling — musical feature extraction for mono- and [[polyphonic]] music, similarity and [[pattern matching]], retrieval
* Formal methods and databases — applications of automated [[music identification]] and recognition, such as [[score following]], automatic accompaniment, routing and filtering for music and music queries, query languages, standards and other metadata or protocols for music information handling and [[information retrieval|retrieval]], [[multi-agent system]]s, distributed search)
*Software for music information retrieval — [[Semantic Web]] and musical digital objects, intelligent agents, collaborative software, web-based search and [[semantic retrieval]], [[query by humming]], [[acoustic fingerprinting]]
* Music analysis and knowledge representation — automatic summarization, citing, excerpting, downgrading, transformation, formal models of music, digital scores and representations, music indexing and [[metadata]].

==Other issues==
*Human-computer interaction and interfaces — multi-modal interfaces, [[user interface]]s and [[usability]], mobile applications, user behavior
* Music perception, cognition, affect, and emotions — music [[similarity metrics]], syntactical parameters, semantic parameters, musical forms, structures, styles ands, music annotation methodologies
* Music archives, libraries, and digital collections — music [[digital library|digital libraries]], public access to musical archives, benchmarks and research databases
* [[Intellectual property]] rights and music — national and international [[copyright]] issues, [[digital rights management]], identification and traceability
* Sociology and Economy of music — music industry and use of MIR in the production, distribution, consumption chain, user profiling, validation, user needs and expectations, evaluation of music IR systems, building test collections, experimental design and metrics

== See also ==
* [[Audio mining]]
* [[Artificial intelligence]]
* [[Digital rights management]]
* [[Digital signal processing]]
* [[Ethnomusicology]]
* [[Multimedia Information Retrieval]]
* [[Music notation]]
* [[Musicology]]
* [[Parsons code]]
* [[Sound and music computing]]
* [[Music OCR]]

== References ==
{{Reflist}}
* Michael Fingerhut (2004). [http://mediatheque.ircam.fr/articles/textes/Fingerhut04b &quot;Music Information Retrieval, or how to search for (and maybe find) music and do away with incipits&quot;], ''IAML-IASA Congress'', Oslo (Norway), August 8–13, 2004.

==External links==
* [http://www.ismir.net/ International Society for Music Information Retrieval]
* [http://music-ir.org/ Music Information Retrieval research]
* [http://www.music-ir.org/jdownie_papers/downie_mir_arist37.pdf J. Stephen Downie: Music information retrieval]
* [http://dx.doi.org/10.1561/1500000042 M. Schedl, E. Gómez and J. Urbano: Music Information Retrieval: Recent Developments and Applications]
* [http://www.nowpublishers.com/product.aspx?product=INR&amp;doi=1500000002 Nicola Orio: Music Retrieval: A Tutorial and Review]
* [https://ccrma.stanford.edu/wiki/MIR_workshop_2011 Intelligent Audio Systems: Foundations and Applications of Music Information Retrieval, introductory course at Stanford University's Center for Computer Research in Music and Acoustics]
* [http://biblio.ugent.be/record/470088 Micheline Lesaffre: Music Information Retrieval: Conceptual Framework, Annotation and User behavior.]
* [http://the.echonest.com/ The Echo Nest: a company specialising in MIR research and applications.]
* [http://www.imagine-research.com/ Imagine Research : develops platform and software for MIR applications ]
* [http://www.AudioContentAnalysis.org/ AudioContentAnalysis.org: MIR resources and matlab code ]

==Example MIR applications==
* [http://www.musipedia.org/ Musipedia — A melody search engine that offers several modes of searching, including whistling, tapping, piano keyboard, and Parsons code.]
* [http://www.listengame.org/ The Listen Game — UCSD Computer Audition Lab MIR music ranking game]
* [http://www.peachnote.com/ Peachnote — A melody search engine and n-gram viewer that searches through digitized music scores]

[[Category:Information retrieval]]
[[Category:Music software]]</text>
      <sha1>9bgfdb1fcyxq7mv53q9hxu0h04ytswh</sha1>
    </revision>
  </page>
  <page>
    <title>Statistical semantics</title>
    <ns>0</ns>
    <id>7271261</id>
    <revision>
      <id>647047449</id>
      <parentid>609925081</parentid>
      <timestamp>2015-02-14T03:45:36Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>[[WP:AWB/GF|general fixes]], fixed media wikilink, replaced: | journal = [[Computational Linguistics]] → |journal = [[Computational Linguistics (journal)|Computational Linguistics]] using [[Project:AWB|AWB]] (10810)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14303">{{linguistics}}
'''Statistical semantics''' is the study of &quot;how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access&quot; {{citation needed|date=July 2012}}&lt;!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation--&gt;. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?

==History==

The term ''Statistical Semantics'' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].&lt;ref&gt;{{harvnb|Weaver|1955}}&lt;/ref&gt; He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that &quot;a word is characterized by the company it keeps&quot; was advocated by [[J. R. Firth|J.R. Firth]].&lt;ref&gt;{{harvnb|Firth|1957}}&lt;/ref&gt; This assumption is known in [[Linguistics]] as the [[Distributional hypothesis|Distributional Hypothesis]].&lt;ref&gt;{{harvnb|Sahlgren|2008}}&lt;/ref&gt; Emile Delavenay defined ''Statistical Semantics'' as &quot;Statistical study of meanings of words and their frequency and order of recurrence.&quot;&lt;ref&gt;{{harvnb|Delavenay|1960}}&lt;/ref&gt; &quot;[[George Furnas|Furnas]] ''et al.'' 1983&quot; is frequently cited as a foundational contribution to Statistical Semantics.&lt;ref&gt;{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}&lt;/ref&gt;  An early success in the field was [[Latent semantic analysis|Latent Semantic Analysis]].

==Applications of statistical semantics==

Research in Statistical Semantics has resulted in a wide variety of algorithms that use the Distributional Hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:
* Measuring the [[Semantic similarity|similarity in word meanings]] &lt;ref&gt;{{harvnb|Lund|Burgess|Atchley|1995}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Landauer|Dumais|1997}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|McDonald|Ramscar|2001}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Terra|Clarke|2003}}&lt;/ref&gt;
* Measuring the similarity in word relations &lt;ref&gt;{{harvnb|Turney|2006}}&lt;/ref&gt;
* Modeling [[similarity-based generalization]] &lt;ref&gt;{{harvnb|Yarlett|2008}}&lt;/ref&gt;
* Discovering words with a given relation &lt;ref&gt;{{harvnb|Hearst|1992}}&lt;/ref&gt;
* Classifying relations between words &lt;ref&gt;{{harvnb|Turney|Littman|2005}}&lt;/ref&gt;
* Extracting keywords from documents &lt;ref&gt;{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Turney|2000}}&lt;/ref&gt;
* Measuring the cohesiveness of text &lt;ref&gt;{{harvnb|Turney|2003}}&lt;/ref&gt;
* Discovering the different senses of words &lt;ref&gt;{{harvnb|Pantel|Lin|2002}}&lt;/ref&gt;
* Distinguishing the different senses of words &lt;ref&gt;{{harvnb|Turney|2004}}&lt;/ref&gt;
* Subcognitive aspects of words &lt;ref&gt;{{harvnb|Turney|2001}}&lt;/ref&gt;
* Distinguishing praise from criticism &lt;ref&gt;{{harvnb|Turney|Littman|2003}}&lt;/ref&gt;

==Related fields==

Statistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].

Many of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.&lt;ref&gt;{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}&lt;/ref&gt;

==See also==
{{Portal|Linguistics}}
*[[Latent semantic analysis]]
*[[Latent semantic indexing]]
*[[Text mining]]
*[[Information retrieval]]
*[[Natural language processing]]
*[[Computational linguistics]]
*[[Web mining]]
*[[Semantic similarity]]
*[[Co-occurrence]]
*[[Text corpus]]
*[[Semantic Analytics]]

==References==
{{reflist|2}}

===Sources===
{{refbegin}}
* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}

* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 1–32 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}
*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}

* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668–673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | id = {{citeseerx|10.1.1.43.9100}} {{citeseerx|10.1.1.148.3598}} | ref = harv }}

* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 1753–1806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}

* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING '92]] | pages = 539–545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | id = {{citeseerx|10.1.1.36.701}} | ref = harv }}

* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211–240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | id = {{citeseerx|10.1.1.184.4759}} | ref = harv | doi=10.1037/0033-295x.104.2.211}}

* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660–665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}

* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611–616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | id = {{citeseerx|10.1.1.104.7535}} | ref = harv }}

* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD '02]] | pages = 613–619 | id = {{citeseerx|10.1.1.12.6771}} | doi = 10.1145/775047.775138 | ref = harv }}

* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33–53 | ref = harv}}

* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244–251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | id = {{citeseerx|10.1.1.12.9041}} | doi = 10.3115/1073445.1073477 | ref = harv }}

* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303–336 | arxiv = cs/0212020 | id = {{citeseerx|10.1.1.11.1829}} | doi = 10.1023/A:1009976227802 | ref = harv }}

* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409–419 | arxiv = cs/0212015 | id = {{citeseerx|10.1.1.12.8734}} | ref = harv | doi=10.1080/09528130110100270}}

* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434–439 | arxiv = cs/0308033 | id = {{citeseerx|10.1.1.100.3751}} | ref = harv }}

* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239–242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}

* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379–416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | id = {{citeseerx|10.1.1.75.8007}} | ref = harv }}

* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] (TOIS) | volume = 21 | issue = 4 | pages = 315–346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | id = {{citeseerx|10.1.1.9.6425}} | doi = 10.1145/944012.944013 | ref = harv }}

* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 1–3 | pages = 251–278 | arxiv = cs/0508103 | id = {{citeseerx|10.1.1.90.9819}} | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}

* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482–489 | arxiv = cs/0309035 | id = {{citeseerx|10.1.1.5.2939}} | url = http://cogprints.org/3163/ | ref = harv }}

* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 15–23 | ref = harv }}

* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}
{{refend}}

==External links==
* {{cite web | url = http://www.si.umich.edu/people/george-furnas | work = Faculty Profile | title = George Furnas | publisher = University of Michigan, School of Information | accessdate = 2010-07-12 }}
*[http://research.microsoft.com/%7Esdumais/ Susan Dumais]
*[http://www.pearsonkt.com/bioLandauer.shtml Thomas Landauer]
*[http://www.apperceptual.com/ Peter Turney]
*[http://waldron.stanford.edu/~michael/papers/ Michael Ramscar]
*[http://www.cs.ualberta.ca/~lindek/demos.htm Dekang Lin's Demos]
*[http://www.isi.edu/~pantel/Content/demos.htm Patrick Pantel's Demos]
*[http://www.nzdl.org/Kea/ Kea keyphrase extraction]
*[http://seokeywordanalysis.com/seotools/ Online keyphrase extractor]

{{DEFAULTSORT:Statistical Semantics}}
[[Category:Artificial intelligence applications]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]
[[Category:Semantics]]
[[Category:Statistical natural language processing]]
[[Category:Fields of application of statistics]]</text>
      <sha1>aduypfz417oor1x0bvv41e55ihql91w</sha1>
    </revision>
  </page>
  <page>
    <title>Federated search</title>
    <ns>0</ns>
    <id>2421086</id>
    <revision>
      <id>638670080</id>
      <parentid>638629764</parentid>
      <timestamp>2014-12-18T18:15:18Z</timestamp>
      <contributor>
        <username>Stesmo</username>
        <id>98915</id>
      </contributor>
      <comment>Reverted 4 edits by [[Special:Contributions/195.88.75.92|195.88.75.92]] ([[User talk:195.88.75.92|talk]]): Reverting spam. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7753">{{Citations missing|date=June 2008}}

'''Federated search''' is an [[information retrieval]] technology that allows the simultaneous search of multiple searchable resources.  A user makes a single query request which is distributed to the [[search engine]]s participating in the federation.  The federated search then aggregates the results that are received from the [[search engine]]s for presentation to the user.

==Purpose==
Federated search came about to meet the need of searching multiple  disparate content sources with one query.  This allows a user to search multiple databases at once in real time, arrange the results from the various databases into a useful form and then present the results to the user.

==Process==
As described by Peter Jacso (2004&lt;ref&gt;Thoughts About Federated Searching.  Jacsó, Péter, Information Today,  Oct 2004, Vol. 21, Issue 9&lt;/ref&gt;), federated searching consists of (1) transforming a [[Web search query|query]] and broadcasting it to a group of disparate databases or other web resources, with the appropriate syntax, (2) merging the results collected from the databases, (3) presenting them in a succinct and unified format with minimal duplication, and (4) providing a means, performed either automatically or by the portal user, to sort the merged result set.

Federated search portals, either commercial or open access, generally search public access [[bibliographic databases]], public access Web-based library catalogues ([[OPAC]]s), Web-based search engines like [[Google]] and/or open-access, government-operated or corporate data collections. These individual information sources send back to the portal's interface a list of results from the search query. The user can review this hit list.  Some portals will merely [[screen scrape]] the actual database results and not directly allow a user to enter the information source's application. More sophisticated ones will de-dupe the results list by merging and removing duplicates. There are additional features available in many portals, but the basic idea is the same: to improve the accuracy and relevance of individual searches as well as reduce the amount of time required to search for resources.

This process allows federated search some key advantages when compared with existing crawler-based search engines.  Federated search need not place any requirements or burdens on owners of the individual information sources, other than handling increased traffic.  Federated searches are inherently as current as the individual information sources, as they are searched in real time.

==Implementation==
[[File:Fed_search.png|thumb|alt=federated search engine|Federating across three search engines]]

One application of federated searching is the [[metasearch engine]]; however, this is not a complete solution as many documents are not currently indexed. These documents are on what is known as the [[deep Web]], or invisible Web. Many more information sources are not yet stored in electronic form. [[Google Scholar]] is one example of many projects trying to address this.

When the search vocabulary or [[data model]] of the search system is different from the data model of one or more of the foreign target systems the query must be translated into each of the foreign target systems.  This can be done using simple data-element translation or may require [[semantic translation]].

A challenge faced in the implementation of federated search engines is scalability, in other words, the performance of the site as the number of information sources comprising the federated search engine increase. One federated search engine that has begun to address this issue is [[WorldWideScience]], hosted by the [[U.S. Department of Energy]]'s [[Office of Scientific and Technical Information]].  WorldWideScience &lt;ref&gt;[http://www.worldwidescience.org WorldWideScience]&lt;/ref&gt; is composed of more than 40 information sources, several of which are federated search portals themselves.  One such portal is Science.gov &lt;ref name=&quot;Science.gov&quot;&gt;[http://www.science.gov Science.gov]&lt;/ref&gt; which itself federates more than 30 information sources representing most of the R&amp;D output of the U.S. Federal government.  Science.gov returns its highest ranked results to WorldWideScience, which then merges and ranks these results with the search returned by the other information sources that comprise WorldWideScience.&lt;ref name=&quot;Science.gov&quot;/&gt; This approach of cascaded federated search enables large number of information sources to be searched via a single query.

Another application [[Sesam]] running in both Norway and Sweden has been built on top of an open sourced platform specialised for federated search solutions. Sesat,&lt;ref&gt;[http://sesat.no Sesat]&lt;/ref&gt; an acronym for [[Sesam Search Application Toolkit]], is a platform that provides much of the framework and functionality required for handling parallel and pipelined searches and displaying them elegantly in a user interface, allowing engineers to focus on the index/database configuration tuning.

==Challenges==

When federated search is performed against secure data sources, the users' credentials must be passed on
to each underlying search engine, so that appropriate security is maintained.  If the user has different
login credentials for different systems, there must be a means to map their login ID to each search
engine's security domain.&lt;ref&gt;[http://www.ideaeng.com/tabId/98/itemId/124/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search]&lt;/ref&gt;

Another challenge is mapping results list navigators into a common form.  Suppose 3 real-estate sites are searched, each provides a list of hyperlinked city names to click on, to see matches only in each city.  Ideally these facets would be combined into one set, but that presents additional technical challenges.&lt;ref&gt;[http://www.ideaeng.com/tabId/98/itemId/154/20-Differences-Between-Internet-vs-Enterprise-Se.aspx#fed_facets 20+ Differences Between Internet vs. Enterprise Search - part 1]&lt;/ref&gt;  The system also needs to understand &quot;next page&quot; links if it's going to allow the user to page through the combined results.

==Further reading==
*[http://www.libraryjournal.com/article/CA6571320.html Federated Search 101. Linoski, Alexis, Walczyk, Tine, Library Journal, Summer 2008 Net Connect, Vol. 133]{{Dead link|date=November 2010}} Note: this content has been moved [http://www.accessmylibrary.com/article-1G1-182034526/federated-search-101-alexis.html here], but you will need a remote access account through your local library to get the whole article.

*Cox, Christopher N. Federated Search: Solution or Setback for Online Library Services. Binghamton, NY: Haworth Information Press, 2007.[http://lccn.loc.gov/2006101753 Table of Contents]
*[http://www.altsearchengines.com/2009/01/11/federated-search-finds-content-that-google-cant-reach-part-i-of-iii/ Federated Search Primer. Lederman, S., AltSearchEngines, January 2009] {{Dead link|date=July 2010}} Note: This material has been reposted [http://deepwebtechblog.com/federated-search-finds-content-that-google-can%E2%80%99t-reach-part-i-of-iii/ here], on the blog of a commercial search engine company.

* Milad Shokouhi and Luo Si, Federated Search, Foundations and Trends® in Information Retrieval: Vol. 5: No 1, pp 1-102., [http://dx.doi.org/10.1561/1500000010 http://dx.doi.org/10.1561/1500000010]

==See also==
* [[Search aggregator]]
* [[Deep Web]]

==References==
{{Reflist}}
{{Internet search}}

{{DEFAULTSORT:Federated Search}}
[[Category:Information retrieval]]
[[Category:Internet terminology]]
[[Category:Searching]]
[[Category:Internet search algorithms]]
[[Category:Applications of distributed computing]]</text>
      <sha1>scz8o54mvhj2fwcbuj15spsr29ac5bg</sha1>
    </revision>
  </page>
  <page>
    <title>Fuzzy retrieval</title>
    <ns>0</ns>
    <id>25935906</id>
    <revision>
      <id>545840370</id>
      <parentid>529491852</parentid>
      <timestamp>2013-03-21T01:36:32Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q1382890]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8355">'''Fuzzy retrieval''' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.

==Mixed Min and Max model (MMM)==

In fuzzy-set theory, an element has a varying degree of membership, say ''d&lt;sub&gt;A&lt;/sub&gt;'', to a given set ''A'' instead of the traditional membership choice (is an element/is not an element).&lt;br /&gt;
In MMM&lt;ref&gt;{{citation | last=Fox | first=E. A. | coauthors=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}&lt;/ref&gt; each index term has a fuzzy set associated with it. A document's weight with respect to an index term ''A'' is considered to be the degree of membership of the document in the fuzzy set associated with ''A''. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:&lt;br/&gt;
:&lt;math&gt;d_{A\cap B}= min(d_A, d_B)&lt;/math&gt;
:&lt;math&gt;d_{A\cup B}= max(d_A,d_B)&lt;/math&gt;

According to this, documents that should be retrieved for a query of the form ''A or B'', should be in the fuzzy set associated with the union of the two sets ''A'' and ''B''. Similarly, the documents that should be retrieved for a query of the form ''A and B'', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the ''or'' query to be ''max(d&lt;sub&gt;A&lt;/sub&gt;, d&lt;sub&gt;B&lt;/sub&gt;)'' and the similarity of the document to the ''and'' query to be ''min(d&lt;sub&gt;A&lt;/sub&gt;, d&lt;sub&gt;B&lt;/sub&gt;)''. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the ''min'' and ''max'' document weights.

Given a document ''D'' with index-term weights ''d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;'' for terms ''A&lt;sub&gt;1&lt;/sub&gt;, A&lt;sub&gt;2&lt;/sub&gt;, ..., A&lt;sub&gt;n&lt;/sub&gt;'', and the queries:

''Q&lt;sub&gt;or&lt;/sub&gt; = (A&lt;sub&gt;1&lt;/sub&gt; or A&lt;sub&gt;2&lt;/sub&gt; or ... or A&lt;sub&gt;n&lt;/sub&gt;)''&lt;br /&gt;
''Q&lt;sub&gt;and&lt;/sub&gt; = (A&lt;sub&gt;1&lt;/sub&gt; and A&lt;sub&gt;2&lt;/sub&gt; and ... and A&lt;sub&gt;n&lt;/sub&gt;)''

the query-document similarity in the MMM model is computed as follows:

''SlM(Q&lt;sub&gt;or&lt;/sub&gt;, D) = C&lt;sub&gt;or1&lt;/sub&gt; * max(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;) + C&lt;sub&gt;or2&lt;/sub&gt; * min(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;)''&lt;br /&gt;
''SlM(Q&lt;sub&gt;and&lt;/sub&gt;, D) = C&lt;sub&gt;and1&lt;/sub&gt; * min(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt;, ..., d&lt;sub&gt;An&lt;/sub&gt;) + C&lt;sub&gt;and2&lt;/sub&gt; * max(d&lt;sub&gt;A1&lt;/sub&gt;, d&lt;sub&gt;A2&lt;/sub&gt; ..., d&lt;sub&gt;An&lt;/sub&gt;)''

where ''C&lt;sub&gt;or1&lt;/sub&gt;, C&lt;sub&gt;or2&lt;/sub&gt;'' are &quot;softness&quot; coefficients for the ''or'' operator, and ''C&lt;sub&gt;and1&lt;/sub&gt;, C&lt;sub&gt;and2&lt;/sub&gt;'' are softness coefficients for the ''and'' operator. Since we would like to give the maximum of the document weights more importance while considering an ''or'' query and the minimum more importance while considering an ''and'' query, generally we have ''C&lt;sub&gt;or1&lt;/sub&gt; &gt; C&lt;sub&gt;or2&lt;/sub&gt; and C&lt;sub&gt;and1&lt;/sub&gt; &gt; C&lt;sub&gt;and2&lt;/sub&gt;''. For simplicity it is generally assumed that ''C&lt;sub&gt;or1&lt;/sub&gt; = 1 - C&lt;sub&gt;or2&lt;/sub&gt;'' and ''C&lt;sub&gt;and1&lt;/sub&gt; = 1 - C&lt;sub&gt;and2&lt;/sub&gt;''.

Lee and Fox&lt;ref name=&quot;leefox&quot;&gt;{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}&lt;/ref&gt; experiments indicate that the best performance usually occurs with ''C&lt;sub&gt;and1&lt;/sub&gt;'' in the range [0.5, 0.8] and with ''C&lt;sub&gt;or1&lt;/sub&gt;'' &gt; 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].

==Paice model==

The Paice model&lt;ref&gt;{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}&lt;/ref&gt; is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:

:&lt;math&gt;S(D,Q) = \sum_{i=1}^n\frac{r^{i-1}*w_{di}}{\sum_{j=1}^n r^{j-1}}&lt;/math&gt;

where ''r'' is a constant coefficient and ''w&lt;sub&gt;di&lt;/sub&gt;'' is arranged in ascending order for ''and'' queries and descending order for ''or'' queries. When n = 2 the Paice model shows the same behavior as the MMM model.

The experiments of Lee and Fox&lt;ref name=&quot;leefox&quot;/&gt; have shown that setting the ''r'' to 1.0 for ''and'' queries and 0.7 for ''or'' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of ''min'' or ''max'' of a set of term weights each time an ''and'' or ''or'' clause is considered, which can be done in ''O(n)''. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an ''and'' clause or an ''or'' clause is being considered. This requires at least an ''0(n log n)'' sorting algorithm. A good deal of floating point calculation is needed too.

==Improvements over the Standard Boolean model==
Lee and Fox&lt;ref name=&quot;leefox&quot;/&gt; compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:
{| class=&quot;wikitable&quot;
|-
!
! CISI
! CACM
! INSPEC
|-
! MMM
| 68%
| 109%
| 195%
|-
! Paice
| 77%
| 104%
| 206%
|}

These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.

==Recent work==

Recently '''Kang ''et al.'''.&lt;ref&gt;{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last=Kang | first=Bo-Yeong | coauthors=Dae-Won Kim, Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}&lt;/ref&gt; have devised a fuzzy retrieval system indexed by concept identification.

If we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.&lt;br /&gt;
They report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.

Zadrozny&lt;ref&gt;{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first=Sławomir | last=Zadrozny | coauthors=Nowacka, Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}&lt;/ref&gt; revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:
* assuming linguistic terms as importance weights of keywords also in documents
* taking into account the uncertainty concerning the representation of documents and queries
* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadeh’s fuzzy logic (calculus of linguistic statements)
* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries

The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.

==See also==
*[[Information retrieval]]

==Further reading==
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}

==References==
{{reflist}}

{{DEFAULTSORT:Fuzzy Retrieval}}
[[Category:Information retrieval]]</text>
      <sha1>g6vcxikppbobne049pzjkixlwznyfx8</sha1>
    </revision>
  </page>
  <page>
    <title>EXCLAIM</title>
    <ns>0</ns>
    <id>8239120</id>
    <revision>
      <id>532061405</id>
      <parentid>524075409</parentid>
      <timestamp>2013-01-09T01:02:48Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Removing Orphan Tag (Nolonger an Orphan) ([[User_talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5267">{{For|the Canadian magazine|Exclaim!}}
The '''EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)''' is an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006. It is currently in a beta stage of development, with some support for more than a dozen languages. The lead developers are Justin Nuger and Jesse Saba Kirchner.

Early work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.&lt;ref&gt;
{{cite web
|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web
|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf
|format=PDF|publisher=ACM-SIGIR 1999
|accessdate=2006-12-02
}}
&lt;/ref&gt;

EXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).

One of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.

==Current status==

EXCLAIM is in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, is available for the following twenty-three languages:

{| class=&quot;wikitable&quot;
| [[Albanian language|Albanian]]
|-
| [[Amharic]]
|-
| [[Bengali language|Bengali]]
|-
| [[Gothic language|Gothic]]
|-
| [[Greek language|Greek]]
|-
| [[Icelandic language|Icelandic]]
|-
| [[Indonesian language|Indonesian]]
|-
| [[Irish language|Irish]]
|-
| [[Javanese language|Javanese]]
|-
| [[Latvian language|Latvian]]
|-
| [[Malagasy language|Malagasy]]
|-
| [[Mandarin Chinese]]
|-
| [[Nahuatl]]
|-
| [[Navajo language|Navajo]]
|-
| [[Quechua languages|Quechua]]
|-
| [[Sardinian language|Sardinian]]
|-
| [[Swahili language|Swahili]]
|-
| [[Tagalog language|Tagalog]]
|-
| [[Standard Tibetan|Tibetan]]
|-
| [[Turkish language|Turkish]]
|-
| [[Welsh language|Welsh]]
|-
| [[Wolof language|Wolof]]
|-
| [[Yiddish]]
|}

Support using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:

{| class=&quot;wikitable&quot;
|-
| [[Dutch language|Dutch]]
|-
| [[Spanish language|Spanish]]
|}

Significant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.

Future versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.

The EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.

==Further applications==

EXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].&lt;ref&gt;{{cite web
|title=A crosslinguistic readability framework
|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf
|format=PDF|publisher=ACL-IJNLP 2009
|accessdate=2009-09-04
}}
&lt;/ref&gt;

==Notes and references==

{{reflist}}

==External links==
*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website]
*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]
*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]
*[http://ju-st.in/ Justin Nuger's professional webpage]
*[http://people.ucsc.edu/~kirchner/ Jesse Saba Kirchner's professional webpage]

{{DEFAULTSORT:Exclaim}}
[[Category:Information retrieval]]</text>
      <sha1>qadd467cbv4cgq1imtvbdyd8rmwly86</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Music search engines</title>
    <ns>14</ns>
    <id>28073505</id>
    <revision>
      <id>595050409</id>
      <parentid>579767312</parentid>
      <timestamp>2014-02-11T22:57:59Z</timestamp>
      <contributor>
        <username>TenPoundHammer</username>
        <id>736168</id>
      </contributor>
      <comment>no</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="161">[[Category:Information retrieval]]
[[Category:Music software|Search engines]]
[[Category:Internet search engines]]
[[Category:Online music and lyrics databases]]</text>
      <sha1>1o055v6ndmdmx082g28mzd5emp07l4u</sha1>
    </revision>
  </page>
  <page>
    <title>Collaborative filtering</title>
    <ns>0</ns>
    <id>480289</id>
    <revision>
      <id>645119699</id>
      <parentid>643436716</parentid>
      <timestamp>2015-02-01T10:14:18Z</timestamp>
      <contributor>
        <username>Enzoborgfrantz</username>
        <id>23971779</id>
      </contributor>
      <comment>/* Memory-based */ &quot;First, it depends on human ratings.&quot; this is not true since ratings may be obtained implicitly, thus avoiding user rating bias.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24980">{{external links|date=November 2013}}
{{Use dmy dates|date=June 2013}}
{{Recommender systems}}
[[File:Collaborative filtering.gif|600px|thumb|

This image shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about user's rating for an item, which the user hasn't rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won't like the video.

]]

'''Collaborative filtering''' ('''CF''') is a technique used by some [[recommender system]]s.&lt;ref name=&quot;handbook&quot;&gt;Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35&lt;/ref&gt; Collaborative filtering has two senses, a narrow one and a more general one.&lt;ref name=recommender&gt;{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will}}&lt;/ref&gt;  In general, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.&lt;ref name=&quot;recommender&quot; /&gt;  Applications of collaborative filtering typically involve very large data sets.   Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc.  The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.

In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person ''A'' has the same opinion as a person ''B'' on an issue, A is more likely to have B's opinion on a different issue ''x'' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes).&lt;ref&gt;[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV &amp; VOD Recommendations]&lt;/ref&gt; Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.

==Introduction==
The growth of the Internet has made it much more difficult to effectively extract useful information from all the available online information. The overwhelming amount of data necessitates  mechanisms for efficient information filtering. One of the techniques used for dealing with this problem is called collaborative filtering.

The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with similar tastes to themselves. Collaborative filtering explores techniques for matching people with similar interests and making recommendations on this basis.

Collaborative filtering algorithms often require (1) users’ active participation, (2) an easy way  to represent users’ interests to the system, and (3) algorithms that are able to match people with similar interests.

Typically, the workflow of a collaborative filtering system is:
# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user's interest in the corresponding domain.
# The system matches this user’s ratings against other users’  and finds the people with most “similar” tastes.
# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)
A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.

==Methodology==

[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]

Collaborative filtering systems have many forms, but many common systems can be reduced to two steps:
# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).
# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user
This falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].

Alternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:
# Build an item-item matrix determining relationships between pairs of items
# Infer the tastes of the current user by examining the matrix and matching that user's data
See, for example, the [[Slope One]] item-based collaborative filtering family.

Another form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.

Relying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].

==Types==

===Memory-based===
This mechanism uses user rating data to compute similarity between users or items. This is used for making recommendations. This was the earlier mechanism and is used in many commercial systems. It is easy to implement and is effective. Typical examples of this mechanism are neighbourhood based CF and item-based/user-based top-N recommendations.[3] For example, in user based approaches, the value of ratings user 'u' gives to item 'i' is calculated as an aggregation of some similar users rating to the item:
:&lt;math&gt;r_{u,i} = \operatorname{aggr}_{u^\prime \in U} r_{u^\prime, i}&lt;/math&gt;

where 'U' denotes the set of top 'N' users that are most similar to user 'u' who rated item 'i'. Some examples of the aggregation function includes:
:&lt;math&gt;r_{u,i} = \frac{1}{N}\sum\limits_{u^\prime \in U}r_{u^\prime, i}&lt;/math&gt;
:&lt;math&gt;r_{u,i} = k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)r_{u^\prime, i}&lt;/math&gt;
:&lt;math&gt;r_{u,i} = \bar{r_u} +  k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)(r_{u^\prime, i}-\bar{r_{u^\prime}} )&lt;/math&gt;

where k is a normalizing factor defined as &lt;math&gt;k =1/\sum_{u^\prime \in U}|\operatorname{simil}(u,u^\prime)| &lt;/math&gt;. and &lt;math&gt;\bar{r_u}&lt;/math&gt; is the average rating of user u for all the items rated by that user.

The neighborhood-based algorithm calculates the similarity between two users or items, produces a prediction for the user taking the weighted average of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple mechanisms such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.

The Pearson correlation similarity of two users x, y is defined as 
:&lt;math&gt; \operatorname{simil}(x,y) = \frac{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})(r_{y,i}-\bar{r_y})}{\sqrt{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})^2\sum\limits_{i \in I_{xy}}(r_{y,i}-\bar{r_y})^2}} &lt;/math&gt;

where I&lt;sub&gt;xy&lt;/sub&gt; is the set of items rated by both user x and user y.

The cosine-based approach defines the cosine-similarity between two users x and y as:&lt;ref name=&quot;Breese1999&quot;&gt;John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&amp;smnu=2&amp;article_id=231&amp;proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998&lt;/ref&gt;
:&lt;math&gt;\operatorname{simil}(x,y) = \cos(\vec x,\vec y) = \frac{\vec x \cdot \vec y}{||\vec x|| \times ||\vec y||} = \frac{\sum\limits_{i \in I_{xy}}r_{x,i}r_{y,i}}{\sqrt{\sum\limits_{i \in I_{x}}r_{x,i}^2}\sqrt{\sum\limits_{i \in I_{y}}r_{y,i}^2}}&lt;/math&gt;

The user based top-N recommendation algorithm identifies the k most similar users to an active user using similarity based vector model. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.

The advantages with this approach include:  the explainability of the results, which is an important aspect of recommendation systems; it is easy to create and use; new data can be added easily and incrementally; it need not consider the content of the items being recommended; and the mechanism scales well with co-rated items.

There are several disadvantages with this approach.  Its performance decreases when data gets sparse, which is frequent with web related items. This prevents the scalability of this approach and has problems with large datasets. Although it can efficiently handle new users because it relies on a data structure, adding new items becomes more complicated since that representation usually relies on a specific vector space. That would require to include the new item and re-insert all the elements in the structure.

===Model-based===
Models are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], Multiple Multiplicative Factor, [[Latent Dirichlet allocation]] and [[markov decision process]] based models.&lt;ref name=&quot;Suetal2009&quot;&gt;Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.&lt;/ref&gt;

This approach has a more holistic goal to uncover latent factors that explain observed ratings.&lt;ref&gt;[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering]&lt;/ref&gt; Most of the models are based on creating a classification or clustering technique to identify the user based on the test set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].

There are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.

The disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.

===Hybrid===
A number of applications combines the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches. It improves the prediction performance. Importantly, it overcomes the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.&lt;ref&gt;Kernel Mapping Recommender System Algorithms, www.sciencedirect.com/science/article/pii/S0020025512002587
&lt;/ref&gt; Usually most of the commercial recommender systems are hybrid, for example, Google news recommender system.&lt;ref&gt;[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering]&lt;/ref&gt;

==Application on social web==
Unlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.&lt;ref&gt;[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]&lt;/ref&gt;

One scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Digg]] as they are &quot;voted up&quot; (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.

Another aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.

===Problems===
A collaborative filtering system does not necessarily succeed in automatically matching content to one's preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user's preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.

==Challenges of collaborative filtering==

===Data sparsity===
In practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.

One typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users’ past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.

Similarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Recommender system#Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.

===Scalability===
As the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers &lt;math&gt;O(M)&lt;/math&gt; and millions of items &lt;math&gt;O(N)&lt;/math&gt;, a CF algorithm with the complexity of &lt;math&gt;n&lt;/math&gt; is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.&lt;ref name=&quot;twitterwtf&quot;&gt;Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web&lt;/ref&gt;

===Synonyms===
[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.

For example, the seemingly different items “children movie” and “children film” are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the Latent Dirichlet Allocation technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}

===Grey sheep===
Grey sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.

===Shilling attacks===
In a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.

===Diversity and the Long Tail===
Collaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the &quot;[[long tail]].&quot;&lt;ref&gt;{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984}}&lt;/ref&gt;

==Innovations==
{{Prose|date=May 2012}}
* New algorithms have been developed for CF as a result of the [[Netflix prize]].
* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.
* Robust Collaborative Filtering, where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.&lt;ref&gt;{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}&lt;/ref&gt;

==See also==
* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]
* [[Cold start]]
* [[Collaborative model]]
* [[Collaborative search engine]]
* [[Collective intelligence]]
* [[Customer engagement]]
* [[Delegative Democracy]], the same principle applied to voting rather than filtering
* [[Enterprise bookmarking]]
* [[Firefly (website)]], a defunct website which was based on collaborative filtering
* [[Long tail]]
* [[Preference elicitation]]
* [[Recommendation system]]
* [[Relevance (information retrieval)]]
* [[Reputation system]]
* [[Robust collaborative filtering]]
* [[Similarity search]]
* [[Slope One]]
* [[Social translucence]]

==References==
{{Reflist|30em}}

==External links==
*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf ''Beyond Recommender Systems: Helping People Help Each Other''], page 12, 2001
*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.
*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005
*[http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems]{{dead link|date=May 2012}} ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])
*[http://www.grouplens.org/publications.html GroupLens research papers].
*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&amp;nbsp;187–192, Edmonton, Canada, July 2002.
*[http://agents.media.mit.edu/projects.html A collection of past and present &quot;information filtering&quot; projects (including collaborative filtering) at MIT Media Lab]
*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]
*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M
*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web
*[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)
*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]
*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]
*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]

{{DEFAULTSORT:Collaborative Filtering}}
[[Category:Collaboration]]
[[Category:Collaborative software| Collaborative filtering]]
[[Category:Collective intelligence]]
[[Category:Information retrieval]]
[[Category:Recommender systems]]
[[Category:Social information processing]]
[[Category:Behavioral and social facets of systemic risk]]</text>
      <sha1>3g4lwsf49xo1end43t9nxhko035nq6g</sha1>
    </revision>
  </page>
  <page>
    <title>Cross-language information retrieval</title>
    <ns>0</ns>
    <id>296950</id>
    <revision>
      <id>625693082</id>
      <parentid>542487264</parentid>
      <timestamp>2014-09-15T17:48:45Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging using [[Project:AWB|AWB]] (10458)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1802">{{refimprove|date=September 2014}}

'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.  CLIR techniques can be classified into different categories based on different translation resources: 
* Dictionary-based CLIR techniques
* Parallel corpora based CLIR techniques
* Comparable corpora based CLIR techniques
* Machine translator based CLIR techniques

The first workshop on CLIR was held in Zürich during the SIGIR-96 conference.&lt;ref&gt;The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.&lt;/ref&gt; Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).

The term &quot;cross-language information retrieval&quot; has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term &quot;multilingual information retrieval&quot; refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.

==See also==
*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)

==References==
&lt;references /&gt;

==External links==
*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]

{{DEFAULTSORT:Cross-Language Information Retrieval}}
[[Category:Information retrieval]]
[[Category:Natural language processing]]


{{linguistics-stub}}</text>
      <sha1>9yczcdzgelcbpmzbd5nje3z7h8wnq7o</sha1>
    </revision>
  </page>
  <page>
    <title>Multi-document summarization</title>
    <ns>0</ns>
    <id>6870342</id>
    <revision>
      <id>632543718</id>
      <parentid>618828419</parentid>
      <timestamp>2014-11-05T11:18:41Z</timestamp>
      <contributor>
        <username>Ngocminh.oss</username>
        <id>14366918</id>
      </contributor>
      <comment>add navbox</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8789">'''Multi-document summarization''' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].

==Key benefits==
Multi-[[document summarization]] creates information reports that are both concise and comprehensive.
With different opinions being put together &amp; outlined, every topic is described from multiple perspectives within a single document.
While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.

==Technological challenges==
The multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,&lt;ref&gt;http://www-nlpir.nist.gov/projects/duc/index.html&lt;/ref&gt; conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.

An ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:
*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections
*text within sections is divided into meaningful paragraphs
*gradual transition from more general to more specific thematic aspects
*good [[readability]]

The latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:
*no paper-unrelated &quot;[[communication noise|information noise]]&quot; from the respective documents (e.g., web pages)
*no dangling references to what is not mentioned or explained in the overview
*no text breaks across a sentence
*no semantic [[Redundancy (information theory)|redundancy]].

==Real-life systems==
The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.
* Ultimate Research Assistant&lt;ref&gt;http://ultimate-research-assistant.com/&lt;/ref&gt; - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. 
* iResearch Reporter&lt;ref&gt;http://www.iresearch-reporter.com/&lt;/ref&gt; - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.
* Newsblaster&lt;ref&gt;http://newsblaster.cs.columbia.edu&lt;/ref&gt; is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.
* NewsInEssence&lt;ref&gt;http://www.newsinessence.com&lt;/ref&gt; may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.
* NewsFeed Researcher&lt;ref&gt;http://newsfeedresearcher.com&lt;/ref&gt; is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.
* Scrape This&lt;ref&gt;http://www.scrapethis.com&lt;/ref&gt; is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.
* JistWeb&lt;ref&gt;http://www.jastatechnologies.com/productList.html&lt;/ref&gt; is a query specific multiple document summariser.

As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.

==Bibliography==
* Günes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]
* Dragomir R. Radev, Hongyan Jing, Malgorzata Styś, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919–938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]
* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 74–82, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]
* C.-Y. Lin, E. Hovy, &quot;From single to multi-document summarization: A prototype system and its evaluation&quot;, In &quot;Proceedings of the ACL&quot;, pp.&amp;nbsp;457–464, 2002
*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, &quot;Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization&quot;, SIGIR’05, Salvador, Brazil, August 15–19, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]
*R. Barzilay, N. Elhadad, K. R. McKeown, &quot;Inferring strategies for sentence ordering in multidocument news summarization&quot;, Journal of Artificial Intelligence Research, v. 17, pp.&amp;nbsp;35–55, 2002
*M. Soubbotin, S. Soubbotin, &quot;Trade-Off Between Factors Influencing Quality of the Summary&quot;, Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9–10, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]
* C Ravindranath Chowdary, and P. Sreenivasa Kumar. &quot;Esum: an efficient system for query-specific multi-document summarization.&quot; In ECIR (Advances in Information Retrieval), pp.&amp;nbsp;724–728. Springer Berlin Heidelberg, 2009.

==See also==
* [[Automatic summarization]]
* [[Text mining]]
* [[News aggregators]]

==References==
{{reflist}}

==External links==
{{External links|date=September 2010}}
*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]
*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]
*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]

{{Natural Language Processing}}

{{DEFAULTSORT:Multi-Document Summarization}}
[[Category:Natural language processing]]
[[Category:Information retrieval]]</text>
      <sha1>aegqiwesc7ojyuwef3iiibu9r0a8t93</sha1>
    </revision>
  </page>
  <page>
    <title>Search suggest drop-down list</title>
    <ns>0</ns>
    <id>23344134</id>
    <revision>
      <id>586227789</id>
      <parentid>511717838</parentid>
      <timestamp>2013-12-15T19:17:52Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>/* See also */ Linking to a relevant article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1852">{{Refimprove|date=June 2009}}

A '''search [[Suggestion|suggest]] [[drop-down list]]''' is a [[Query language|query]] feature used in [[computing]]. A quick system to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed. Before the query has been typed, a drop-down list with the suggested complete search queries, is given as options to select and access. The suggested queries then enable the searcher to complete the required search quickly.

It is a form of [[Autocomplete|autocompletion]] while typing into a query [[text box]], before a detailed search result is entered. Lists can be based on popular searches or other options. The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from data or a [[database]], with search suggested drop-down lists being a common industry standard for an instant search.

Search suggested lists are used by [[internet browsers]], [[website]]s and [[search engine]]s, local [[operating system]]s and [[database]]s.

[[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].

==See also ==
*[[Autocomplete]]
*[[Search engine (computing)]]
*[[Search box]]
*[[Search algorithm]]
*[[Censorship by Google#Search suggestions]]


{{DEFAULTSORT:Search Suggest Drop-Down List}}
[[Category:Data search engines]]
[[Category:Information retrieval]]
[[Category:Search algorithms]]</text>
      <sha1>dnjm57qypuvbjyhl42cxs1ie488ux98</sha1>
    </revision>
  </page>
  <page>
    <title>Cluster labeling</title>
    <ns>0</ns>
    <id>25202953</id>
    <revision>
      <id>611060921</id>
      <parentid>611060883</parentid>
      <timestamp>2014-06-01T11:12:39Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>/* Example */ same here</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9752">In [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find labeling that summarize the topic of each cluster and distinguish the clusters from each other.

==Differential cluster labeling==
Differential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html&gt;.&lt;/ref&gt;

===Pointwise mutual information===

{{Main|Pointwise mutual information}}

In the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:

&lt;math&gt;I(X, Y) = \sum_{x\in X}{ \sum_{y\in Y} {p(x, y)log_2\left(\frac{p(x, y)}{p_1(x)p_2(y)}\right)}}&lt;/math&gt;

where ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p&lt;sub&gt;1&lt;/sub&gt;(x)'' is the probability distribution of X, and ''p&lt;sub&gt;2&lt;/sub&gt;(y)'' is the probability distribution of Y.

In the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html&gt;.&lt;/ref&gt;  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

&lt;math&gt;I(C, T) = \sum_{c\in {0, 1}}{ \sum_{t\in {0, 1}} {p(C = c, T = t)log_2\left(\frac{p(C = c, T = t)}{p(C = c)p(T = t)}\right)}}&lt;/math&gt;

In this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.

===Chi-Squared Selection===
{{Main|Pearson's chi-squared test}}
The Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:

&lt;math&gt;X^2 = \sum_{a \in A}{\sum_{b \in B}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}&lt;/math&gt;

where ''O&lt;sub&gt;a,b&lt;/sub&gt;'' is the ''observed'' frequency of a and b co-occurring, and ''E&lt;sub&gt;a,b&lt;/sub&gt;'' is the ''expected'' frequency of co-occurrence.

In the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

&lt;math&gt;X^2 = \sum_{a \in {0,1}}{\sum_{b \in {0,1}}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}&lt;/math&gt;

For example, ''O&lt;sub&gt;1,0&lt;/sub&gt;'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E&lt;sub&gt;1,0&lt;/sub&gt;'' is the expected number of documents that are in a particular cluster but don't contain a certain term.
Our initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html&gt;.&lt;/ref&gt;

''E&lt;sub&gt;1,0&lt;/sub&gt; = N * P(C = 1) * P(T = 0)''

where N is the total number of documents in the collection.

==Cluster-Internal Labeling==
Cluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.
Cluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.

===Centroid Labels===
{{Main|Vector space model}}
A frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.
One downside to using centroid labeling is that it can pick up words like &quot;place&quot; and &quot;word&quot; that have a high frequency in written text, but have little relevance to the contents of the particular cluster.

===Contextualized centroid labels===
A simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection. &lt;ref&gt;Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters’ contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155&lt;/ref&gt;
In this approach, a term-term co-occurrence matrix referred as &lt;math&gt;T_k&lt;/math&gt; is first built for each cluster &lt;math&gt;S_k&lt;/math&gt;. Each cell represents the number of times term &lt;math&gt;i&lt;/math&gt; co-occurs with term &lt;math&gt;j&lt;/math&gt; within a certain window of text (a sentence, a paragraph, etc.)
In a second stage, a similarity matrix &lt;math&gt;T_k^{sim}&lt;/math&gt; is obtained by multiplying &lt;math&gt;T_k&lt;/math&gt; with its transpose. We have &lt;math&gt;T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})&lt;/math&gt;. Being the dot product of two normalized vectors &lt;math&gt;\tilde{t}_{i}&lt;/math&gt; and &lt;math&gt;\tilde{t}_{j}&lt;/math&gt;, &lt;math&gt;t_{{sim}_{ij}}&lt;/math&gt; denotes the cosine similarity between terms &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt;. The so obtained &lt;math&gt;T_k^{sim}&lt;/math&gt; can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.

===Title labels===
An alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.

===External knowledge labels===
Cluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.&lt;ref&gt;David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146&lt;/ref&gt; In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.

==External links==
* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]
* [http://erulemaking.ucsur.pitt.edu/doc/papers/dgo06-labeling.pdf Automatically Labeling Hierarchical Clusters]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Cluster Labeling}}
[[Category:Information retrieval]]</text>
      <sha1>s71mykgu259s2okhr22qqn2e31ztilo</sha1>
    </revision>
  </page>
  <page>
    <title>Conference and Labs of the Evaluation Forum</title>
    <ns>0</ns>
    <id>27511028</id>
    <revision>
      <id>526545835</id>
      <parentid>526545727</parentid>
      <timestamp>2012-12-05T16:53:16Z</timestamp>
      <contributor>
        <username>TXiKi</username>
        <id>1492777</id>
      </contributor>
      <minor/>
      <comment>TXiKi moved page [[Cross-Language Evaluation Forum]] to [[Conference and Labs of the Evaluation Forum]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2430">The '''Conference and Labs of the Evaluation Forum''' (formerly '''Cross-Language Evaluation Forum'''), or '''CLEF''', is an organization promoting research  in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to  maintain an underlying framework for testing [[information retrieval]] systems, and creating [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].&lt;ref name=&quot;Peters&quot;&gt;{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | id = {{citeseerx|10.1.1.109.7647}} }}&lt;/ref&gt;
The organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,&lt;ref&gt;{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 1–2 | year = 2004 }}&lt;/ref&gt;&lt;ref&gt;Fredric C. Gey, Noriko Kando, and Carol Peters &quot;Cross-Language Information Retrieval: the way ahead&quot; in ''Information Processing &amp; Management''
vol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}&lt;/ref&gt;  

For example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.&lt;ref name=&quot;ImageCLEFmed&quot;&gt;{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
* [http://www.clef-campaign.org CLEF homepage]

[[Category:Information retrieval]]

{{Compu-conference-stub}}</text>
      <sha1>p4ywsxdvrvqba686ryh5eck8t34xx0h</sha1>
    </revision>
  </page>
  <page>
    <title>Legal information retrieval</title>
    <ns>0</ns>
    <id>24997830</id>
    <revision>
      <id>624913429</id>
      <parentid>607457799</parentid>
      <timestamp>2014-09-10T08:09:40Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* References */Added 1 doi to a journal cite using [[Project:AWB|AWB]] (10434)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14589">'''Legal information retrieval''' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 1&lt;/ref&gt; Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.&lt;ref name=Jackson&gt;Jackson et al., p. 60&lt;/ref&gt; Legal information retrieval is a part of the growing field of [[legal informatics]].  

== Overview ==

In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,&lt;ref name=&quot;Blair, D.C. 1985, p.293&quot;&gt;Blair, D.C., and Maron, M.E., 1985, p.293&lt;/ref&gt; meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.&lt;ref name=&quot;Blair, D.C. 1985, p.293&quot;/&gt; This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.&lt;ref&gt;American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html&lt;/ref&gt; 

Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],&lt;ref&gt;Peters, W. et al. 2007, p. 118&lt;/ref&gt; [[polysemes]]&lt;ref&gt;Peters, W. et al. 2007, p. 130&lt;/ref&gt; (words that have different meanings when used in a legal context), and constant change. 

Techniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.

== Problems ==

Application of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].&lt;ref name=LOIS1&gt;Peters, W. et al. 2007, p. 120&lt;/ref&gt; Instead, the law is generally filled with open-ended terms, which may change over time.&lt;ref name=LOIS1 /&gt; This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.&lt;ref&gt;Saravanan, M. et al.  2009, p. 101&lt;/ref&gt;

Legal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term &quot;worker&quot; has four different meanings:&lt;ref name=&quot;Peters, W. et al. 2007, p. 131&quot;&gt;Peters, W. et al. 2007, p. 131&lt;/ref&gt; 

#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.
#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;
#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;
#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;

In addition, it also has the common meaning: 
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;A person who works at a specific occupation.&lt;ref name=&quot;Peters, W. et al. 2007, p. 131&quot;/&gt; &lt;/li&gt;
&lt;/ol&gt;

Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results. 

Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.&lt;ref name=MaxwellA &gt;Maxwell, K.T., and Schafer, B. 2008, p. 8&lt;/ref&gt; Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court's decision contains more discussion of the relevant facts.&lt;ref name=MaxwellA  /&gt; The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).&lt;ref name=MaxwellA  /&gt; A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.

Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not.&lt;ref name=MaxwellA  /&gt; He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.&lt;ref name=MaxwellA /&gt;

Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day&lt;ref name=Jackson /&gt;), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.&lt;ref name=Jackson /&gt;&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2007, p.1&lt;/ref&gt;

== Techniques ==

===Boolean searches===

[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above. 

The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.&lt;ref name=&quot;Blair, D.C. 1985, p.293&quot;/&gt; Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.&lt;ref&gt;Saravanan M., et al. 2009, p. 116&lt;/ref&gt;

===Manual classification===

In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.&lt;ref name=&quot;Maxwell, K.T. 2008, p. 2&quot;&gt;Maxwell, K.T., and Schafer, B. 2008, p. 2&lt;/ref&gt; These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]'s “Natural Language”&lt;ref name=WL&gt;Westlaw Research, http://www.westlaw.com&lt;/ref&gt; or [[LexisNexis]]' Headnote&lt;ref name=LN&gt;Lexis Research, http://www.lexisnexis.com&lt;/ref&gt; searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers&lt;ref name=WL /&gt; or Lexis' Headnotes.&lt;ref name=LN /&gt; Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).&lt;ref name=&quot;Maxwell, K.T. 2008, p. 2&quot;/&gt;

These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.&lt;ref name=&quot;Maxwell, K.T. 2008, p. 3&quot;&gt;Maxwell, K.T., and Schafer, B. 2008, p. 3&lt;/ref&gt; In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.&lt;ref&gt;Saravanan, M. et al.  2009, p. 116&lt;/ref&gt; The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.&lt;ref&gt;Saravanan, M. et al. 2009, p. 103&lt;/ref&gt;

The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.&lt;ref name=&quot;Maxwell, K.T. 2008, p. 3&quot;/&gt;&lt;ref&gt;Schweighofer, E. and Liebwald, D. 2008, p. 108&lt;/ref&gt; As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2008, p. 4&lt;/ref&gt;

===Natural language processing===

In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.&lt;ref name=Jackson /&gt;&lt;ref name=AshleyA&gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 125&lt;/ref&gt;&lt;ref name=Gelbart&gt;Gelbart, D. and Smith, J.C. 1993, p. 142&lt;/ref&gt; Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,&lt;ref name=Jackson /&gt;&lt;ref name=AshleyA /&gt;&lt;ref name=Gelbart /&gt; few have reported results. One system, “SMILE,” which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).&lt;ref name=AshleyB &gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 159&lt;/ref&gt; This is probably much lower than an acceptable rate for general usage.&lt;ref name=AshleyB /&gt;&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 3&lt;/ref&gt;

Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 9&lt;/ref&gt;&lt;ref&gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 126&lt;/ref&gt;

== Notes ==
{{Reflist|2}}

==References==
{{Refbegin}}
*{{cite journal
|author     = Maxwell, K.T., and Schafer, B.
|year       = 2008
|title      = Concept and Context in Legal Information Retrieval
|url        = http://portal.acm.org/citation.cfm?id=1564016
|journal    = Frontiers in Artificial Intelligence and Applications
|volume     = 189
|pages      = 63–72
|publisher  = IOS Press
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Jackson, P. et al.
|year       = 1998
|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation
|url        = http://portal.acm.org/citation.cfm?id=288627.288642
|journal    = Conference on Information and Knowledge Management
|pages      = 60–67
|publisher  = ACM
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Blair, D.C., and Maron, M.E.
|year       = 1985
|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval
|url        = http://portal.acm.org/citation.cfm?id=3166.3197&amp;coll=GUIDE&amp;dl=GUIDE&amp;CFID=61732097&amp;CFTOKEN=95519997
|journal    = Communications of the ACM
|volume     = 28
|issue      = 3 
|pages      = 289–299
|publisher  = ACM
|accessdate = 2009-11-07
|doi=10.1145/3166.3197
}}
*{{cite journal
|author     = Peters, W. et al.
|year       = 2007
|title      = The structuring of legal knowledge in LOIS
|url        = http://www.springerlink.com/content/d04l7h2507700g45/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 117–135
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9034-4
}}
*{{cite journal
|author     = Saravanan, M. et al.
|year       = 2007
|title      = Improving legal information retrieval using an ontological framework 
|url        = http://www.springerlink.com/content/h66412k08h855626/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 101–124
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9075-y
}}
*{{cite journal
|author     = Schweighofer, E. and Liebwald, D.
|year       = 2007
|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary
|url        = http://www.springerlink.com/content/v62v7131x10413v0/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 103–115
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9029-1
}}
*{{cite journal
|author     = Gelbart, D. and Smith, J.C.
|year       = 1993
|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management
|url        = http://portal.acm.org/citation.cfm?id=158994
|journal    = International Conference on Artificial Intelligence and Law
|pages      = 142–151
|publisher  = ACM
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Ashley, K.D. and Bruninghaus, S.
|year       = 2009
|title      = Automatically classifying case texts and predicting outcomes
|url        = http://www.springerlink.com/content/lhg8837331hgu024/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 125–165
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9077-9
}}
{{Refend}}

{{DEFAULTSORT:Legal Information Retrieval}}
[[Category:Information retrieval]]
[[Category:Natural language processing]]
[[Category:Legal research]]</text>
      <sha1>22veh6nx8cbgz4y73tjn3hhku0b2ix3</sha1>
    </revision>
  </page>
  <page>
    <title>Adversarial information retrieval</title>
    <ns>0</ns>
    <id>11486091</id>
    <revision>
      <id>609264220</id>
      <parentid>609262512</parentid>
      <timestamp>2014-05-19T17:31:09Z</timestamp>
      <contributor>
        <username>Ost316</username>
        <id>6289403</id>
      </contributor>
      <comment>Repairing 2 external links using [[w:WP:CHECKLINKS|Checklinks]]; formatting: whitespace (using [[User:Cameltrader#Advisor.js|Advisor.js]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3072">'''Adversarial information retrieval''' ('''adversarial IR''') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.

On the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].&lt;ref&gt;B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]&lt;/ref&gt;

Activities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.

== Topics ==
Topics related to Web spam (spamdexing):

* [[Link spam]]
* [[Keyword spamming]]
* [[Cloaking]]
* Malicious tagging
* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]

Other topics:
* [[Click fraud]] detection
* Reverse engineering of  [[search engine]]'s [[ranking]] algorithm
* Web [[content filtering]]
* [[Ad filtering|Advertisement blocking]]
* Stealth [[web crawling|crawling]]
*[[Troll (Internet)]]
* Malicious tagging or voting in [[social networks]]
* [[Astroturfing]]
* [[Sockpuppetry]]

== History ==
The term &quot;adversarial information retrieval&quot; was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.&lt;ref&gt;D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]&lt;/ref&gt;

== See also ==
*[[Spamdexing]]
*[[Information retrieval]]

== References ==
{{reflist}}

== External links ==
*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web
*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection
*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection

{{DEFAULTSORT:Adversarial Information Retrieval}}
[[Category:Information retrieval]]
[[Category:Internet fraud]]
[[Category:Searching]]</text>
      <sha1>sou0ulewghxeck96ksbqwx3md8n17ar</sha1>
    </revision>
  </page>
  <page>
    <title>Cosine similarity</title>
    <ns>0</ns>
    <id>8966592</id>
    <revision>
      <id>643375808</id>
      <parentid>640478565</parentid>
      <timestamp>2015-01-20T14:48:10Z</timestamp>
      <contributor>
        <username>Tarantulae</username>
        <id>8508095</id>
      </contributor>
      <minor/>
      <comment>Minor change, only link update due to the site movement (I'm the owner of the blog).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12240">'''Cosine similarity''' is a measure of similarity between two vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a Cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].

Note that these bounds apply for any number of dimensions, and Cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[Information Retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.&lt;ref&gt;Singhal, Amit (2001). &quot;Modern Information Retrieval: A Brief Overview&quot;. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35–43.&lt;/ref&gt;

The technique is also used to measure cohesion within [[cluster (computing)|cluster]]s in the field of [[data mining]].&lt;ref&gt;P.-N. Tan, M. Steinbach &amp; V. Kumar, &quot;Introduction to Data Mining&quot;, , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.&lt;/ref&gt;

''Cosine distance'' is a term often used for the complement in positive space, that is: &lt;math&gt;D_C(A,B) = 1 - S_C(A,B)&lt;/math&gt;. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the triangle inequality property and it violates the coincidence axiom; to repair the triangle inequality property whilst maintaining the same ordering, it is necessary to convert to Angular distance (see below.)

One of the reasons for the popularity of Cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.

==Definition==

The cosine of two vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:

:&lt;math&gt;\mathbf{a}\cdot\mathbf{b}
=\left\|\mathbf{a}\right\|\left\|\mathbf{b}\right\|\cos\theta&lt;/math&gt;

Given two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos(θ)'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vectors|magnitude]] as

:&lt;math&gt;\text{similarity} = \cos(\theta) = {A \cdot B \over \|A\| \|B\|} = \frac{ \sum\limits_{i=1}^{n}{A_i \times B_i} }{ \sqrt{\sum\limits_{i=1}^{n}{(A_i)^2}} \times \sqrt{\sum\limits_{i=1}^{n}{(B_i)^2}} }&lt;/math&gt;

The resulting similarity ranges from &amp;minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 usually indicating independence, and in-between values indicating intermediate similarity or dissimilarity.

For text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.

In the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&amp;nbsp;90°.

If the attribute vectors are normalized by subtracting the vector means (e.g., &lt;math&gt;A - \bar{A}&lt;/math&gt;), the measure is called centered cosine similarity and is equivalent to the [[Pearson_product-moment_correlation_coefficient#For_a_sample|Pearson Correlation Coefficient]].

=== Angular similarity ===

The term &quot;cosine similarity&quot; has also been used on occasion to express a different coefficient, although the most common use is as defined above. Using the same calculation of similarity, the normalised angle between the vectors can be used as a bounded similarity function within [0,1], calculated from the above definition of similarity by:
:&lt;math&gt;1 - \frac{ \cos^{-1}( \text{similarity} )}{ \pi} &lt;/math&gt;
in a domain where vector coefficients may be positive or negative, or
:&lt;math&gt;1 - \frac{ 2 \cdot \cos^{-1}( \text{similarity} ) }{ \pi }&lt;/math&gt;
in a domain where the vector coefficients are always positive.  

Although the term &quot;cosine similarity&quot; has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.

=== Confusion with &quot;Tanimoto&quot; coefficient ===

The cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:

:&lt;math&gt;T(A,B) = {A \cdot B \over \|A\|^2 +\|B\|^2 - A \cdot B}&lt;/math&gt;

In fact, this algebraic form [[Jaccard index#Tanimoto_Similarity_and_Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.

=== Ochiai coefficient ===
This coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:&lt;ref&gt;''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. № 9. P. 526-530.&lt;/ref&gt;&lt;ref&gt;''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. – Assen. Van Gorcum. 1958. 628 p.&lt;/ref&gt;
:&lt;math&gt;K =\frac{n(A \cap B)}{\sqrt{n(A) \times n(B)}}&lt;/math&gt;
Here, &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are sets, and &lt;math&gt;n(A)&lt;/math&gt; is the number of elements in &lt;math&gt;A&lt;/math&gt;. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.

== Properties ==
Cosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual &lt;math&gt;\|A - B\|&lt;/math&gt;, and observe that

:&lt;math&gt;\|A - B\|^2 = (A - B)^\top (A - B) = \|A\|^2 + \|B\|^2 - 2 A^\top B&lt;/math&gt;

by [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, &lt;math&gt;\|A\|^2 = \|B\|^2 = 1&lt;/math&gt; so the previous is equal to

:&lt;math&gt;2 (1 - \cos(A, B))&lt;/math&gt;

'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of &lt;math&gt;1/n&lt;/math&gt; (where &lt;math&gt;n&lt;/math&gt; is the number of dimensions), and although the distribution is bounded between -1 and +1, as &lt;math&gt;n&lt;/math&gt; grows large the distribution is increasingly well-approximated by the [[normal distribution]].&lt;ref&gt;{{cite journal
 | author = Spruill, Marcus C
 | year = 2007
 | title = Asymptotic distribution of coordinates on high dimensional spheres
 | journal = Electronic communications in probability
 | volume = 12 | pages = 234-247
 | doi = 10.1214/ECP.v12-1294
}}&lt;/ref&gt;&lt;ref&gt;[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]&lt;/ref&gt;
For other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.&lt;ref&gt;{{cite journal
 | author = Graham L. Giller 
 | year = 2012
 | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity
 | journal = Giller Investments Research Notes
 | number = 20121024/1
 | doi = 10.2139/ssrn.2167044
}}&lt;/ref&gt;

== Soft Cosine Measure ==
'''Soft cosine measure''' &lt;ref&gt;{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gómez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computación y Sistemas|volume=18|issue=3|pages=491–504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}&lt;/ref&gt;
is a measure of “soft” similarity between two vectors, i.e., the measure that considers similarity of pairs of features. The traditional '''cosine similarity''' considers the [[vector space model]] (VSM) features as independent or completely different, while the '''soft cosine measure''' proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).

For example, in the field of [[natural language processing]] (NLP) the similarity between features is quite intuitive. Features such as words, n-grams or syntactic n-grams&lt;ref&gt;{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernández|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1–11|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}&lt;/ref&gt; can be quite similar, though formally they are considered as different features in the VSM. For example, words “play” and “game” are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).

For calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.  

Given two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:

:&lt;math&gt;\begin{align}
    \operatorname{soft\_cosine}_1(a,b)=
    \frac{\sum\nolimits_{i,j}^N s_{ij}a_ib_j}{\sqrt{\sum\nolimits_{i,j}^N s_{ij}a_ia_j}\sqrt{\sum\nolimits_{i,j}^N s_{ij}b_ib_j}},
\end{align}
&lt;/math&gt;

where {{math|''s&lt;sub&gt;ij&lt;/sub&gt;'' {{=}} similarity(feature&lt;sub&gt;''i''&lt;/sub&gt;, feature&lt;sub&gt;''j''&lt;/sub&gt;)}}.

If there is no similarity between features ({{math|''s&lt;sub&gt;ii&lt;/sub&gt;'' {{=}} 1}}, {{math|''s&lt;sub&gt;ij&lt;/sub&gt;'' {{=}} 0}} for {{math|''i'' ≠ ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.

The complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be even transformed to linear.

== See also ==
* [[Sørensen similarity index|Sørensen's quotient of similarity]]
* [[Hamming distance]]
* [[Correlation]]
* [[Dice's coefficient]]
* [[Jaccard index]]
* [[SimRank]]
* [[Information retrieval]]

==References==
{{reflist}}

== External links ==
* [http://www.appliedsoftwaredesign.com/archives/cosine-similarity-calculator/ Online Cosine Similarity Calculator]
* [http://mathforum.org/kb/message.jspa?messageID=5658016&amp;tstart=0 Weighted cosine measure]
* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]

{{DEFAULTSORT:Cosine Similarity}}
[[Category:Information retrieval]]</text>
      <sha1>i17c3uipjaz4eib5ox5vbrq6g2v0icz</sha1>
    </revision>
  </page>
  <page>
    <title>Exploratory search</title>
    <ns>0</ns>
    <id>4881262</id>
    <revision>
      <id>599259956</id>
      <parentid>580111449</parentid>
      <timestamp>2014-03-12T08:48:32Z</timestamp>
      <contributor>
        <ip>141.54.172.104</ip>
      </contributor>
      <comment>Reference from introduction cite was wrong.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11027">'''Exploratory search''' is a specialization of information exploration which represents the activities carried out by searchers who are either:&lt;nowiki&gt;[3]&lt;/nowiki&gt;
* a) unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)
* b) unsure about the ways to achieve their goals (either the technology or the process)
* c) or even unsure about their goals in the first place.

Consequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.

==History==
Exploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like &quot;what if the user doesn't know which keywords to use?&quot; or &quot;what if the user isn't looking for a single answer?&quot; Consequently, research has begun to focus on defining the broader set of ''information behaviors'' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.

In the last few years, a series of workshops have been held at various related and key events. In 2005, the [http://research.microsoft.com/~ryenw/xsi/index.html Exploratory Search Interfaces workshop] focused on beginning to define some of the key challenges in the field. Since then a series of other workshops have been held at related conferences: [http://research.microsoft.com/~ryenw/eess/index.html Evaluating Exploratory Search] at [http://www.sigir2006.org SIGIR06] and [http://research.microsoft.com/~ryenw/esi/index.html Exploratory Search and HCI] at [http://www.chi2007.org CHI07] (in order to meet with the experts in [[human–computer interaction]]).

In March 2008, an [http://www.sciencedirect.com/science/journal/03064573 ''Information Processing and Management'' special issue]&lt;nowiki&gt;[2]&lt;/nowiki&gt; has focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.

In June 2008, the [[National Science Foundation]] sponsored an [http://www.ils.unc.edu/ISSS_workshop/ invitational workshop] to identify a research agenda for exploratory search and similar fields for the coming years.

==Research challenges==

===Important scenarios===
With the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by [http://mspace.fm mSpace] states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.

===Designing new interfaces===
With one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the user, so that they can choose from a list instead of guess a possible keyword query.

Many of the [[human–computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.

Computational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.&lt;ref&gt;Fu, W.-T., Kannampalill, T. G., &amp; Kang, R. (2010). Facilitating exploratory search by model-based navigational cues. In Proceedings of the ACM International conference on Intelligent User Interface. 199-208.  http://portal.acm.org/citation.cfm?id=1719970.1719998&lt;/ref&gt;

===Evaluating interfaces===
As the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the ''correct'' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.

===Models of exploratory search behavior===
There has been recent attempts to develop process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].&lt;ref&gt;{{Citation
  | doi = 10.1145/1460563.1460600
  | last1 = Fu  | first1 = Wai-Tat
  | title = The Microstructures of Social Tagging: A Rational Model
  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work. 
  | pages = 66–72
  | date = April 2008
  | url = http://portal.acm.org/citation.cfm?id=1460600
  | isbn = 978-1-60558-007-4 }}
&lt;/ref&gt;
.&lt;ref&gt;{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | title = A Semantic Imitation Model of Social Tagging
  | journal = Proceedings of the IEEE conference on Social Computing
  | pages = 66–72
  | date = Aug 2009
  | url = http://www.humanfactors.illinois.edu/Reports&amp;PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}&lt;/ref&gt; The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.&lt;ref&gt;
{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | last2 = Pirolli  | first2 = Peter
  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web
  | journal = Human-Computer Interaction
  | pages = 335–412
  | year = 2007
  | url = http://portal.acm.org/citation.cfm?id=1466608
  | volume = 22}}&lt;/ref&gt;&lt;ref&gt;Kitajima, M., Blackmon, M. H., &amp; Polson, P. G. (2000). A comprehension-based
model of Web navigation and its application to Web usability analysis. In S. Mc-
Donald, Y. Waern, &amp; G. Cockton (Eds.), People and computers XIV—Usability or else!
New York: Springer-Verlag.&lt;/ref&gt;&lt;ref&gt;Miller, C. S., &amp; Remington, R.W. (2004). Modeling information navigation: Implications
for information architecture. Human Computer Interaction, 19, 225–271.&lt;/ref&gt;
Recent development in exploratory search is often concentrated in predicting user's search intents in interaction with the user.&lt;ref&gt;
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Athukorala  | first2 = Kumaripaba
  | last3 = Glowacka  | first3 = Dorota
  | last4 = Konuyshkova  | first4 = Ksenia
  | last5 = Oulasvrita  | first5 = Antti
  | last6 = Kaipiainen  | first6 = Samuli
  | last7 = Kaski  | first7 = Samuel
  | last8 = Jacucci  | first8 = Giulio
  | title = Supporting exploratory search tasks with interactive user modeling
  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&amp;T
  | year = 2013}}
&lt;/ref&gt;
Such predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs
&lt;ref&gt;
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Peltonen  | first2 = Jaakko
  | last3 = Eugster | first3 = Manuel J.A.
  | last4 = Glowacka  | first4 = Dorota
  | last5 = Konuyshkova  | first5 = Ksenia
  | last6 = Athukorala  | first6 = Kumaripaba
  | last7 = Kosunen | first7 = Ilkka    
  | last8 = Reijonen  | first8 = Aki
  | last9 = Myllymäki | first9 = Petri
  | last10 = Kaski  | first10 = Samuel
  | last11 = Jacucci  | first11 = Giulio
  | title = Directing Exploratory Search with Interactive Intent Modeling
  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM
  | year = 2013}}
&lt;/ref&gt;
.&lt;ref&gt;
{{Citation
  | last1 = Glowacka  | first1 = Dorota
  | last2 = Ruotsalo  | first2 = Tuukka
  | last3 = Konuyshkova  | first3 = Ksenia
  | last4 = Athukorala  | first4 = Kumaripaba
  | last5 = Kaski  | first5 = Samuel
  | last6 = Jacucci  | first6 = Giulio
  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords
  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI
  | url = http://dl.acm.org/citation.cfm?id=2449413
  | pages = 117–128 
  | year = 2013}}
&lt;/ref&gt;

==Major figures==

Key figures, including experts from both [[information seeking]] and [[human–computer interaction]], are:
*[http://research.microsoft.com/~ryenw Ryen White]
*[http://ils.unc.edu/~march Gary Marchionini]
*[http://comminfo.rutgers.edu/~belkin/belkin.html Nicholas Belkin]
*[http://users.ecs.soton.ac.uk/mc m.c. schraefel]
*[[Marcia Bates]]

==References==
&lt;References/&gt;
#White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). ''Supporting Exploratory Search'', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&amp;nbsp;36–39.
#Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&amp;nbsp;433–436
#Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.
#P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). ''Exploratory Web Searching with Dynamic Taxonomies and Results Clustering'',13th European Conference on Digital Libraries (ECDL'09), Corfu, Greece, Sep-Oct 2009

{{DEFAULTSORT:Exploratory Search}}
[[Category:Human–computer interaction]]
[[Category:Information retrieval]]
[[Category:Information science]]</text>
      <sha1>l5rliwrkxuwktkyb05tqfpbc2qm3aip</sha1>
    </revision>
  </page>
  <page>
    <title>Extended Boolean model</title>
    <ns>0</ns>
    <id>25271852</id>
    <revision>
      <id>560948403</id>
      <parentid>550771552</parentid>
      <timestamp>2013-06-21T18:13:59Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error #2 fix + general fixes using [[Project:AWB|AWB]] (9276)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7059">The '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.&lt;ref&gt;	
{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last=Salton | first=Gerard | coauthors=Edward A. Fox, Harry Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}&lt;/ref&gt;

Thus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.

==Definitions==
In the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.

The weight of term {{math|''K&lt;sub&gt;x&lt;/sub&gt;''}} associated with document {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} is measured by its normalized [[Term frequency]] and can be defined as:

&lt;math&gt;
w_{x,j}=f_{x,j}*\frac{Idf_{x}}{max_{i}Idf_{i}}
&lt;/math&gt;

where {{math|''Idf&lt;sub&gt;x&lt;/sub&gt;''}} is [[inverse document frequency]].

The weight vector associated with document {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} can be represented as:

&lt;math&gt;\mathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \ldots, w_{i,j}]&lt;/math&gt;

==The 2 Dimensions Example==
{{multiple image
 | width     = 150
 | image1    = 2D_Extended_Boolean_model_OR_example.png
 | alt1      = Figure 1
 | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;or; ''K&lt;sub&gt;y&lt;/sub&gt;'')}} with documents {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} and {{math|''d''&lt;sub&gt;''j''+1&lt;/sub&gt;}}.
 | image2    = 2D_Extended_Boolean_model_AND_example.png
 | alt2      = Figure 2
 | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;and; ''K&lt;sub&gt;y&lt;/sub&gt;'')}} with documents {{math|''d&lt;sub&gt;j&lt;/sub&gt;''}} and {{math|''d''&lt;sub&gt;''j''+1&lt;/sub&gt;}}.
}}

Considering the space composed of two terms {{math|''K&lt;sub&gt;x&lt;/sub&gt;''}} and {{math|''K&lt;sub&gt;y&lt;/sub&gt;''}} only, the corresponding term weights are {{math|''w''&lt;sub&gt;1&lt;/sub&gt;}} and {{math|''w''&lt;sub&gt;2&lt;/sub&gt;}}.&lt;ref&gt;[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]&lt;/ref&gt;  Thus, for query {{math|''q&lt;sub&gt;or&lt;/sub&gt;'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;or; ''K&lt;sub&gt;y&lt;/sub&gt;'')}}, we can calculate the similarity with the following formula:
 
&lt;math&gt;sim(q_{or},d)=\sqrt{\frac{w_1^2+w_2^2}{2}}&lt;/math&gt;

For query {{math|''q&lt;sub&gt;and&lt;/sub&gt;'' {{=}} (''K&lt;sub&gt;x&lt;/sub&gt;'' &amp;and; ''K&lt;sub&gt;y&lt;/sub&gt;'')}}, we can use:

&lt;math&gt;sim(q_{and},d)=1-\sqrt{\frac{(1-w_1)^2+(1-w_2)^2}{2}}&lt;/math&gt;

==Generalizing the idea and P-norms==
We can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.

This can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &amp;le; ''p'' &amp;le; &amp;infin;}} is a new parameter.&lt;ref&gt;{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}&lt;/ref&gt;

*A generalized conjunctive query is given by:
:&lt;math&gt;q_{or}=k_1 \lor^p k_2 \lor^p .... \lor^p k_t  &lt;/math&gt;

*The similarity of &lt;math&gt;q_{or}&lt;/math&gt; and &lt;math&gt;d_j&lt;/math&gt; can be defined as:
''':&lt;math&gt;sim(q_{or},d_j)=\sqrt[p]{\frac{w_1^p+w_2^p+....+w_t^p}{t}}&lt;/math&gt;'''

*A generalized disjunctive query is given by:
:&lt;math&gt;q_{and}=k_1 \land^p k_2 \land^p .... \land^p k_t  &lt;/math&gt;

*The similarity of &lt;math&gt;q_{and}&lt;/math&gt; and &lt;math&gt;d_j&lt;/math&gt; can be defined as:
:&lt;math&gt;sim(q_{and},d_j)=1-\sqrt[p]{\frac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}&lt;/math&gt;

==Examples==
Consider the query {{math|''q'' {{=}} (''K''&lt;sub&gt;1&lt;/sub&gt; &amp;and; ''K''&lt;sub&gt;2&lt;/sub&gt;) &amp;or; ''K''&lt;sub&gt;3&lt;/sub&gt;}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:

&lt;math&gt;sim(q,d)=\sqrt[p]{\frac{(1-\sqrt[p]{(\frac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}&lt;/math&gt;

==Improvements over the Standard Boolean Model==

Lee and Fox&lt;ref&gt;{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}&lt;/ref&gt; compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.
Using P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.&lt;br&gt;
The P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.

==Further reading==
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]
* [http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6VC8-454T5MS-2&amp;_user=513551&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1117914301&amp;_rerunOrigin=google&amp;_acct=C000025338&amp;_version=1&amp;_urlVersion=0&amp;_userid=513551&amp;md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}
* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first= Lucie | last= Skorkovská | coauthors=Pavel Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}

==See also==
*[[Information retrieval]]

==References==
{{reflist}}

{{DEFAULTSORT:Extended Boolean Model}}
[[Category:Information retrieval]]</text>
      <sha1>cojgdpfdzk5zw0rhupwkw4k4f8uvdqt</sha1>
    </revision>
  </page>
  <page>
    <title>Query likelihood model</title>
    <ns>0</ns>
    <id>29979321</id>
    <revision>
      <id>594753278</id>
      <parentid>594752848</parentid>
      <timestamp>2014-02-10T00:47:39Z</timestamp>
      <contributor>
        <ip>210.94.41.89</ip>
      </contributor>
      <comment>formula P(q|M_d)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2402">The '''query likelihood model''' is a [[language model]] used in [[Information Retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.

==Calculating the likelihood==
Using [[Bayes' theorem|Bayes' rule]], the probability &lt;math&gt;P&lt;/math&gt; of a document &lt;math&gt;d&lt;/math&gt;, given a query &lt;math&gt;q&lt;/math&gt; can be written as follows:

:&lt;math&gt;
 P(d|q) = \frac{P(q|d) P(d)}{P(q)}
&lt;/math&gt;

Since the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.

:&lt;math&gt;
 P(d|q) = P(q|d)
&lt;/math&gt;

Documents are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:
:&lt;math&gt;
 P(q|M_d) = K_q \prod_{t \in V} P(t|M_d)^{tf_{t,q}}
&lt;/math&gt;,where the multinomial coefficient is &lt;math&gt;K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tM,q}!)&lt;/math&gt; for query {{math|q}}.

In practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document &lt;math&gt;d&lt;/math&gt;). The language model &lt;math&gt;M_d&lt;/math&gt; should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So &lt;math&gt;P(t|M_d)&lt;/math&gt; is the probability of term &lt;math&gt;t&lt;/math&gt; being generated by the language model &lt;math&gt;M_d&lt;/math&gt; of document &lt;math&gt;d&lt;/math&gt;. This probability is multiplied for all terms from query &lt;math&gt;q&lt;/math&gt; to get a rank for document &lt;math&gt;d&lt;/math&gt; in the interval &lt;math&gt;[0,1]&lt;/math&gt;. The calculation is repeated for all documents to create a ranking of all documents in the document collection.

&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009&lt;/ref&gt;

==References==
 &lt;references/&gt;

[[Category:Information retrieval]]</text>
      <sha1>g8d5qxhh8uyapj9bpalohlgph4xi8m8</sha1>
    </revision>
  </page>
  <page>
    <title>Policy framework</title>
    <ns>0</ns>
    <id>21828505</id>
    <revision>
      <id>525837414</id>
      <parentid>505908194</parentid>
      <timestamp>2012-12-01T10:11:54Z</timestamp>
      <contributor>
        <username>DoctorKubla</username>
        <id>16383496</id>
      </contributor>
      <comment>rm deprecated wikify tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4311">{{refimprove|date=March 2009}}
A '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.

==Principles==
[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.&lt;ref&gt;http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm&lt;/ref&gt;

===Availability===
Government departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....

===Coverage===
Government departments should make the following information increasingly available on an electronic basis:
* all published material or material already in the public domain
* all policies that could be released publicly
* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)
* all documents that the public may be required to complete
* corporate documentation in which the public would be interested

===Pricing=== 
a) Free dissemination of Government-held information is appropriate where:
* dissemination to a target audience is desirable for a public policy purpose, or
* a charge to recover the cost of dissemination is not feasible or cost-effective

b) Pricing to recover the cost of dissemination is appropriate where:
* there is no particular public policy reason to disseminate the information, and 
* a charge to recover the cost of dissemination is both feasible and cost effective

c) Pricing to recover the cost of transformation is appropriate where:
* pricing to recover the cost of dissemination is appropriate, and
* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination

d) Pricing to recover the full costs of information production and dissemination is appropriate where:
* the information is created for the commercial purpose of sale at a profit, and 
* to do so would not breach the other pricing principles

===Ownership===
Government-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.

===Stewardship===
Government departments are stewards of Government-held information, and it is their responsibility to implement good information management.

===Collection===
Government departments should only collect information for specified public policy, operational business or legislative purposes.

===Copyright===
Information created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.
 
===Preservation===
Government-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.

===Quality===
The key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.

===Integrity===
The integrity of Government-held information will be achieved when:
* all guarantees and conditions surrounding the information are met
* the principles are clear and communicated
* any situation relating to Government-held information is handled openly and consistently
* those affected by changes to Government-held information are consulted on those changes
* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well
* there are minimum exceptions to the principles.

===Privacy===
The principles of the Privacy Act 1993 apply.

==References==
{{reflist}}

{{DEFAULTSORT:Policy Framework}}
[[Category:Information retrieval]]
[[Category:Government of New Zealand]]</text>
      <sha1>cqy8ycta6375yyf5jipjstmywp0p0if</sha1>
    </revision>
  </page>
  <page>
    <title>Keyword optimization</title>
    <ns>0</ns>
    <id>15890570</id>
    <redirect title="Search engine optimization" />
    <revision>
      <id>621134028</id>
      <parentid>621133102</parentid>
      <timestamp>2014-08-13T23:46:47Z</timestamp>
      <contributor>
        <username>Flyer22</username>
        <id>4293477</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/67.180.211.59|67.180.211.59]] using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="145">#REDIRECT [[Search engine optimization]]

{{DEFAULTSORT:Keyword Optimization}}
[[Category:Information retrieval]]
[[Category:Internet marketing]]</text>
      <sha1>6aysm9v2w27smv7ly3v9y6kzd4lzf4l</sha1>
    </revision>
  </page>
  <page>
    <title>Datanet</title>
    <ns>0</ns>
    <id>13555870</id>
    <revision>
      <id>624146174</id>
      <parentid>584465683</parentid>
      <timestamp>2014-09-04T11:32:30Z</timestamp>
      <contributor>
        <ip>217.64.121.192</ip>
      </contributor>
      <comment>Removed link to Datanet ISP as the page no longer exists (was removed in 2008).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5849">{{Use mdy dates|date=September 2011}}
''This article is about the U.S. National Science Foundation Office of Cyberinfrastructure .

On September 28, 2007, the U.S. [[National Science Foundation]] Office of Cyberinfrastructure announced a request for proposals with the name '''Sustainable Digital Data Preservation and Access Network Partner (DataNet)'''.&lt;ref name=&quot;datanetprogram&quot;&gt;{{cite web
|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary
|date=September 28, 2007
|accessdate=October 3, 2007
}}&lt;/ref&gt;  The lead paragraph of its synopsis describes the program as:

&lt;blockquote&gt;Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.&lt;/blockquote&gt;

The introduction in the solicitation&lt;ref name=&quot;datanetsolicitation&quot;&gt;{{cite web
|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements &amp; Information
|date=September 28, 2007
|accessdate=October 3, 2007
}}&lt;/ref&gt; goes on to say:

&lt;blockquote&gt;Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF’s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which “science and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.” The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.&lt;/blockquote&gt;

The initial plan called for a $100 million initiative: five awards of $20&amp;nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],&lt;ref&gt;{{cite web|author=William Michener et al |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19}}&lt;/ref&gt; led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,&lt;ref&gt;{{cite web|author=Sayeed Choudhury et al |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19}}&lt;/ref&gt; led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. 

For the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.&lt;ref&gt;{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}&lt;/ref&gt; Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,&lt;ref&gt;{{cite web|author=[[Margaret Hedstrom]] et al |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19}}&lt;/ref&gt; led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the &quot;long tail&quot; of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,&lt;ref&gt;{{cite web|author=[[Reagan Moore]] et al |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19}}&lt;/ref&gt; led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',&lt;ref&gt;{{cite web|author=[[Steven Ruggles]] et al |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19}}&lt;/ref&gt; led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.

==References==
{{reflist|30em}}

==External links==
* [http://www.dataone.org DataONE]
* [http://dataconservancy.org/ Data Conservancy]
* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]
* [http://datafed.org/ DataNet Federation Consortium]
* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] 
 

[[Category:National Science Foundation]]
[[Category:Science and technology in the United States]]
[[Category:Information retrieval]]
[[Category:Digital library projects]]</text>
      <sha1>b3rewfgbxfszft6tn0gnh9aj9n3hq7d</sha1>
    </revision>
  </page>
  <page>
    <title>Stemming</title>
    <ns>0</ns>
    <id>30874683</id>
    <revision>
      <id>633814249</id>
      <parentid>632275709</parentid>
      <timestamp>2014-11-14T14:44:42Z</timestamp>
      <contributor>
        <username>JonaathanKatz</username>
        <id>23177349</id>
      </contributor>
      <minor/>
      <comment>fixing a grammatical mistake</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27669">{{Expert-subject|date=October 2010}}
{{for|the skiing technique|Stem (skiing)}}
'''Stemming''' is the term used in [[linguistic morphology]] and [[information retrieval]] to describe the process for reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] form—generally a written word form. The stem needs not to be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.

Stemming programs are commonly referred to as stemming algorithms or stemmers.

==Examples==
A stemmer for English, for example, should identify the [[string literal|string]] &quot;cats&quot; (and possibly &quot;catlike&quot;, &quot;catty&quot; etc.) as based on the root &quot;cat&quot;, and &quot;stemmer&quot;, &quot;stemming&quot;, &quot;stemmed&quot; as based on &quot;stem&quot;. A stemming algorithm reduces the words &quot;fishing&quot;, &quot;fished&quot;, and &quot;fisher&quot; to the root word, &quot;fish&quot;. On the other hand, &quot;argue&quot;, &quot;argued&quot;, &quot;argues&quot;, &quot;arguing&quot;, and &quot;argus&quot; reduce to the stem &quot;argu&quot; (illustrating the case where the stem is not itself a word or root) but &quot;argument&quot; and &quot;arguments&quot; reduce to the stem &quot;argument&quot;.&lt;!-- using the Porter algorithm --&gt;

==History==
The first published stemmer was written by [[Julie Beth Lovins]] in 1968.&lt;ref&gt;{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=22–31 }}&lt;/ref&gt; This paper was remarkable for its early date and had great influence on later work in this area.

A later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal ''Program''. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.

Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [http://tartarus.org/~martin/PorterStemmer/ free-software implementation] of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.

==Algorithms==
There are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.

===Lookup algorithms===
A simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach is that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.

A lookup approach may use preliminary part-of-speech tagging to avoid overstemming.&lt;ref&gt;Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html ''Y-stemmer'']&lt;/ref&gt;

====The production technique====
The lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is &quot;run&quot;, then the inverted algorithm might automatically generate the forms &quot;running&quot;, &quot;runs&quot;, &quot;runned&quot;, and &quot;runly&quot;. The last two forms are valid constructions, but they are unlikely.

===Suffix-stripping algorithms===
Suffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of &quot;rules&quot; is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:
* if the word ends in 'ed', remove the 'ed'
* if the word ends in 'ing', remove the 'ing'
* if the word ends in 'ly', remove the 'ly'

Suffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. Lemmatisation attempts to improve upon this challenge.

Prefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.

====Additional algorithm criteria====
Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.

It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term ''friendlies'', the algorithm may identify the ''ies'' suffix and apply the appropriate rule and achieve the result of ''friendl''. ''friendl'' is likely not found in the lexicon, and therefore the rule is rejected.

One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ''ies'' with ''y''. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ''ies'' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, ''friendlies'' becomes ''friendly'' instead of ''friendl''.

Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term ''friendly'', where the ''ly'' stripping rule is likely identified and accepted. In summary, ''friendlies'' becomes (via substitution) ''friendly'' which becomes (via stripping) ''friend''.

This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for ''friendlies'' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form ''friend''. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.

===Lemmatisation algorithms===
A more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.

This approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).

===Stochastic algorithms===
[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they &quot;learn&quot;) on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).

Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.

===''n''-gram analysis===
Some stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.&lt;ref name=&quot;Workshop2006&quot;&gt;{{cite book|author=Cross-Language Evaluation Forum. Workshop|title=Accessing Multilingual Information Repositories: 6th Workshop of the Cross-Language Evaluation Forum, CLEF 2005, Vienna, Austria, 21-23 September, 2005, Revised Selected Papers|url=http://books.google.com/books?id=jWKzNGr1H0AC&amp;pg=PA159|date=20 September 2006|publisher=Springer|isbn=978-3-540-45697-1|pages=159–}}&lt;/ref&gt;

===Hybrid approaches===
Hybrid approaches use two or more of the approaches described above in unison. A simple example is a [[probabilistic suffix tree|suffix tree]] algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of &quot;frequent exceptions&quot; like &quot;ran =&gt; run&quot;. If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.

===Affix stemmers===
In [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word ''indefinitely'', identify that the leading &quot;in&quot; is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name '''affix stripping'''. A study of affix stemming for several European languages can be found here.&lt;ref&gt;Jongejan, B.; and Dalianis, H.; ''Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike'', in the ''Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 2–7, 2009'', pp. 145-153
[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]&lt;/ref&gt;

===Matching algorithms===
Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the &quot;brows&quot; in &quot;browse&quot; and in &quot;browsing&quot;). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix &quot;be&quot;, which is the stem of such words as &quot;be&quot;, &quot;been&quot; and &quot;being&quot;, would not be considered as the stem of the word &quot;beside&quot;).

==Language challenges==
While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.&lt;ref&gt;Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf ''Stemming Approaches for East European Languages (CLEF 2007)'']&lt;/ref&gt;&lt;ref&gt;Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 ''Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages''], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2&lt;/ref&gt;&lt;ref&gt;Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384–390&lt;/ref&gt;&lt;ref&gt;[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf ''Stemming in Hungarian at CLEF 2005'']&lt;/ref&gt;&lt;ref&gt;Viera, A. F. G. &amp; Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html ''Uma revisão dos algoritmos de radicalização em língua portuguesa''], Information Research, 12(3), paper 315&lt;/ref&gt;

Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as &quot;dries&quot; being the third-person singular present form of the verb &quot;dry&quot;, &quot;axes&quot; being the plural of &quot;axe&quot; as well as &quot;axis&quot;); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.

===Multilingual stemming===
Multilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{CN|date=October 2013}}.

==Error metrics==
There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been—a [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not—a [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.

For example, the widely used Porter stemmer stems &quot;universal&quot;, &quot;university&quot;, and &quot;universe&quot; to &quot;univers&quot;. This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.

An example of understemming in the Porter stemmer is &quot;alumnus&quot; → &quot;alumnu&quot;, &quot;alumni&quot; → &quot;alumni&quot;, &quot;alumna&quot;/&quot;alumnae&quot; → &quot;alumna&quot;.  This English word keeps Latin morphology, and so these near-synonyms are not conflated.

==Applications==
Stemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning &quot;daffodils&quot; is probably closely related to a text mentioning &quot;daffodil&quot; (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for &quot;marketing&quot; will not be satisfied by most documents mentioning &quot;markets&quot; but not &quot;marketing&quot;.

===Information retrieval===
Stemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.&lt;ref&gt;Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); ''Modern Information Retrieval'', ACM Press/Addison Wesley&lt;/ref&gt; An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, recent research has shown greater benefits for retrieval in other languages.&lt;ref&gt;Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbjörnsson, Börkur (2004); ''Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval'', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); ''Comparative Evaluation of Multilingual Information Access Systems'', Springer Verlag, pp. 152–165&lt;/ref&gt;&lt;ref&gt;Airio, Eija (2006); ''Word Normalization and Decompounding in Mono- and Bilingual IR'', Information Retrieval '''9''':249–271&lt;/ref&gt;

===Domain Analysis===
Stemming is used to determine domain vocabularies in [[domain analysis]].
&lt;ref&gt;Frakes, W.; Prieto-Diaz, R.; &amp; Fox, C. (1998); ''DARE: Domain Analysis and Reuse Environment'', Annals of Software Engineering (5), pp. 125-141&lt;/ref&gt;

===Use in commercial products===
Many commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.&lt;ref&gt;[http://www.dtsearch.co.uk/language.htm ''Language Extension Packs''], dtSearch&lt;/ref&gt;&lt;ref&gt;[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true ''Building Multilingual Solutions by using Sharepoint Products and Technologies''], Microsoft Technet&lt;/ref&gt;

The Snowball stemmers have been compared with commercial lexical stemmers with varying results.&lt;ref&gt;[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]&lt;/ref&gt;&lt;ref&gt;[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson &quot;Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer&quot;]&lt;/ref&gt;

[[Google search]] adopted word stemming in 2003.&lt;ref&gt;[http://www.google.com/support/bin/static.py?page=searchguides.html&amp;ctx=basics#stemming ''The Essentials of Google Search''], Web Search Help Center, [[Google|Google Inc.]]&lt;/ref&gt; Previously a search for &quot;fish&quot; would not have returned &quot;fishing&quot;. Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find &quot;fish&quot; in &quot;fishing&quot; but when searching for &quot;fishes&quot; will not find occurrences of the word &quot;fish&quot;.

==See also==
* [[Root (linguistics)]] - linguistic definition of the term &quot;root&quot;
* [[Stem (linguistics)]] - linguistic definition of the term &quot;stem&quot;
* [[Morphology (linguistics)]]
* [[Lemma (morphology)]] - linguistic definition
* [[Lemmatization]]
* [[Lexeme]]
* [[Inflection]]
* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation
* [[Natural language processing]] - stemming is generally regarded as a form of NLP
* [[Text mining]] - stemming algorithms play a major role in commercial NLP software
* [[Computational linguistics]]

{{Natural Language Processing}}

==References==
{{reflist|2}}

==Further reading==
{{refbegin|2}}
* Dawson, J. L. (1974); ''Suffix Removal for Word Conflation'', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 33–46
* Frakes, W. B. (1984); ''Term Conflation for Information Retrieval'', Cambridge University Press
* Frakes, W. B. &amp; Fox, C. J. (2003); ''Strength and Similarity of Affix Removal Stemming Algorithms'', SIGIR Forum, 37: 26–30
* Frakes, W. B. (1992); ''Stemming algorithms, Information retrieval: data structures and algorithms'', Upper Saddle River, NJ: Prentice-Hall, Inc.
* Hafer, M. A. &amp; Weiss, S. F. (1974); ''Word segmentation by letter successor varieties'', Information Processing &amp; Management 10 (11/12), 371–386
* Harman, D. (1991); ''How Effective is Suffixing?'', Journal of the American Society for Information Science 42 (1), 7–15
* Hull, D. A. (1996); ''Stemming Algorithms&amp;nbsp;– A Case Study for Detailed Evaluation'', JASIS, 47(1): 70–84
* Hull, D. A. &amp; Grefenstette, G. (1996); ''A Detailed Analysis of English Stemming Algorithms'', Xerox Technical Report
* Kraaij, W. &amp; Pohlmann, R. (1996); ''Viewing Stemming as Recall Enhancement'', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); ''Proceedings of the 17th ACM SIGIR conference held at Zurich, August 18–22'', pp.&amp;nbsp;40–48
* Krovetz, R. (1993); ''Viewing Morphology as an Inference Process'', in ''Proceedings of ACM-SIGIR93'', pp.&amp;nbsp;191–203
* Lennon, M.; Pierce, D. S.; Tarry, B. D.; &amp; Willett, P. (1981); ''An Evaluation of some Conflation Algorithms for Information Retrieval'', Journal of Information Science, 3: 177–183
* Lovins, J. (1971); ''[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]'', JASIS, 22: 28–40
* Lovins, J. B. (1968); ''Development of a Stemming Algorithm'', Mechanical Translation and Computational Linguistics, 11, 22—31
* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf ''Conservative Stemming for Search and Indexing'']
* Paice, C. D. (1990); ''[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]'', SIGIR Forum, 24: 56–61
* Paice, C. D. (1996) ''[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]'', JASIS, 47(8): 632–649
* Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&amp;nbsp;384–390
* Porter, Martin F. (1980); ''[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]'', Program, 14(3): 130–137
* Savoy, J. (1993); ''[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&amp;SRETRY=0 Stemming of French Words Based on Grammatical Categories]'' Journal of the American Society for Information Science, 44(1), 1–9
* Ulmschneider, John E.; &amp; Doszkocs, Tamas (1983); ''[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]'', Online Review, 7(4), 301–318
* Xu, J.; &amp; Croft, W. B. (1998); ''[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]'', ACM Transactions on Information Systems, 16(1), 61–81
{{refend}}

==External links==
*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers
* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk' Lancaster stemmers (Java API)
* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)
* [http://snowball.tartarus.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages
* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)
* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]
* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API
* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API
* [http://www.oleandersolutions.com/stemming.html Oleander Porter's algorithm] - stemming library in C++ released under BSD
* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages
* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages
* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK
* [http://www.cmp.uea.ac.uk/Research/stemmer/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK
* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]
* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language
* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages
* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java
* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi
* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech
* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]
* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]

{{FOLDOC}}

[[Category:Linguistic morphology]]
[[Category:Natural language processing]]
[[Category:Tasks of natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]</text>
      <sha1>avdedd1c3av9qjm5llyyqhovi8m9j79</sha1>
    </revision>
  </page>
  <page>
    <title>International Society for Music Information Retrieval</title>
    <ns>0</ns>
    <id>30882491</id>
    <revision>
      <id>626075762</id>
      <parentid>572122069</parentid>
      <timestamp>2014-09-18T12:26:46Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>per [[MOS:BOLDSYN]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8408">The '''International Society for Music Information Retrieval''' ('''ISMIR''') is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000&lt;ref&gt;[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11 ISSN: 1082-9873.]&lt;/ref&gt; which established a yearly symposium - whence &quot;ISMIR&quot;, which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.&lt;ref&gt;[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]&lt;/ref&gt;

==Purpose==
Given the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.

As the term Music Information Retrieval (MIR) indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science and many others.

==Annual Conference==
Since its inception in 2000, ISMIR has been the world’s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.
* [http://ismir2012.ismir.net ISMIR 2012], 8–12 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2012'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2011.ismir.net ISMIR 2011], 24–28 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2011'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2010.ismir.net ISMIR 2010], 9–13 August 2010, Utrecht (Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2010'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2009.ismir.net ISMIR 2009], 26–30 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2009'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2008.ismir.net ISMIR 2008], 14–18 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2008'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2007.ismir.net ISMIR 2007], 23–30 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2007'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2006.ismir.net ISMIR 2006], 8–12 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2006'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2005.ismir.net ISMIR 2005], 11–15 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2005'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2004.ismir.net ISMIR 2004], 10–15 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2004'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2003.ismir.net ISMIR 2003], 26–30 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2003'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2002.ismir.net ISMIR 2002], 13–17 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2002'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2001.ismir.net ISMIR 2001], 15–17 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='2001'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]
* [http://ismir2000.ismir.net ISMIR 2000], 23–25 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&amp;function=search&amp;where_clause=`papers`.`Year`='200'&amp;page=0&amp;order=Authors&amp;order_type=ASC proceedings]

The [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences.

==MIREX==
The Music Information Retrieval Evaluation eXchange (MIREX)&lt;ref&gt;[http://www.music-ir.org/mirex MIREX Wiki]&lt;/ref&gt; is an annual evaluation campaign for Music Information Retrieval (MIR) algorithms, coupled to the ISMIR conference.

MIR tasks evaluated at past MIREXs include:
*[http://www.music-ir.org/mirex/wiki/Audio_Train/Test_Tasks Audio Train/Test Tasks]
**Audio Artist Identification
**Audio Genre Classification
**Audio Music Mood Classification
**Audio Classical Composer Identification
*[http://www.music-ir.org/mirex/wiki/Symbolic_Genre_Classification Symbolic Genre Classification]
*[http://www.music-ir.org/mirex/wiki/Audio_Onset_Detection Audio Onset Detection]
*[http://www.music-ir.org/mirex/wiki/Audio_Key_Detection Audio Key Detection]
*[http://www.music-ir.org/mirex/wiki/Symbolic_Key_Detection Symbolic Key Detection]
*[http://www.music-ir.org/mirex/wiki/Audio_Tag_Classification Audio Tag Classification]
*[http://www.music-ir.org/mirex/wiki/Audio_Cover_Song_Identification Audio Cover Song Identification]
*[http://www.music-ir.org/mirex/wiki/Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following) Real-time Audio to Score Alignment (a.k.a Score Following)]
*[http://www.music-ir.org/mirex/wiki/Query_by_Singing/Humming Query by Singing/Humming]
*[http://www.music-ir.org/mirex/wiki/Multiple_Fundamental_Frequency_Estimation_&amp;_Tracking Multiple Fundamental Frequency Estimation &amp; Tracking]
*[http://www.music-ir.org/mirex/wiki/Audio_Chord_Estimation Audio Chord Estimation]
*[http://www.music-ir.org/mirex/wiki/Audio_Melody_Extraction Audio Melody Extraction]
*[http://www.music-ir.org/mirex/wiki/Query_by_Tapping Query by Tapping]
*[http://www.music-ir.org/mirex/wiki/Audio_Beat_Tracking Audio Beat Tracking]
*[http://www.music-ir.org/mirex/wiki/Audio_Music_Similarity_and_Retrieval Audio Music Similarity and Retrieval]
*[http://www.music-ir.org/mirex/wiki/Symbolic_Melodic_Similarity Symbolic Melodic Similarity]
*[http://www.music-ir.org/mirex/wiki/Structural_Segmentation Structural Segmentation]
*[http://www.music-ir.org/mirex/wiki/Audio_Drum_Detection Audio Drum Detection]
*[http://www.music-ir.org/mirex/wiki/Audio_Tempo_Extraction Audio Tempo Extraction]

==See also==
* [[International Conference on Digital Audio Effects]]
* [[Music Technology]]
* [[Sound and music computing|Sound and Music Computing]]
* [[Sound and Music Computing Conference]]

==Notes==
{{Reflist}}

[[Category:Music technology]]
[[Category:Multimedia]]
[[Category:Information retrieval]]</text>
      <sha1>fslpqsjj6jlkvw1zto7krcedn4jnvcz</sha1>
    </revision>
  </page>
  <page>
    <title>XML retrieval</title>
    <ns>0</ns>
    <id>21106742</id>
    <revision>
      <id>646021973</id>
      <parentid>645146037</parentid>
      <timestamp>2015-02-07T11:08:30Z</timestamp>
      <contributor>
        <ip>188.98.197.14</ip>
      </contributor>
      <comment>Undo citation SPAM for R.W.P. Luk publications</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6162">{{Multiple issues|
{{expert-subject|Computer science|date=January 2015}}
{{COI|date=February 2009}}
}}

'''XML retrieval''', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.&lt;ref&gt;{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}&lt;/ref&gt;

==Queries==
Most XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called &quot;content and structure&quot; (CAS) queries enable users to specify what structure the requested content can or must have.

==Exploiting XML structure==
Taking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.

==Ranking==
Ranking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.&lt;ref name=&quot;INEX2006&quot;&gt;{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf|title=Overview of INEX 2006|last=Malik|first=Saadia|author2=Trotman, Andrew |author3=Lalmas, Mounia |author4= Fuhr, Norbert |year=2007|work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval|accessdate=2009-02-10}}&lt;/ref&gt;

==Existing XML search engines==
An overview of two potential approaches is available.&lt;ref&gt;{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.109.5986&amp;rep=rep1&amp;type=pdf|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR|accessdate=2013-07-04}}&lt;/ref&gt; The INitiative for the Evaluation of XML-Retrieval (''INEX'') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.&lt;ref name=&quot;INEX2006&quot; /&gt; Three different areas influence XML-Retrieval:&lt;ref name=&quot;INEX2002&quot;&gt;{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf|title=INEX: Initiative for the Evaluation of XML Retrieval|last=Fuhr|first=Norbert|author2=Gövert, N. |author3=Kazai, Gabriella |author4= Lalmas, Mounia |year=2003|work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002|publisher=ERCIM Workshop Proceedings, France|accessdate=2009-02-10}}&lt;/ref&gt;

===Traditional XML query languages===
[[Query language]]s such as the [[W3C]] standard [[XQuery]]&lt;ref&gt;{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fernández, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Siméon, Jérôme |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}&lt;/ref&gt; supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' [[Database schema|schemas]].&lt;ref name=&quot;Schlieder2002&quot;&gt;{{Cite journal|url=http://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz|title=Querying and Ranking XML Documents|last=Schlieder|first=Torsten|author2=Meuss, Holger |year=2002|work= Journal of the American Society for Information Science and Technology, Vol. 53, No. 6|accessdate=2009-02-10}}&lt;/ref&gt;

===Databases===
Classic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]&lt;ref name=&quot;INEX2002&quot; /&gt; and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.

===Information retrieval===
Classic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.&lt;ref name=&quot;Schlieder2002&quot;/&gt; They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.&lt;ref&gt;{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR'04|publisher=ACM|accessdate=2009-02-10}}&lt;/ref&gt;

==See also==
*[[Document retrieval]]
*[[Information retrieval applications]]

==References==
{{Reflist}}

{{DEFAULTSORT:Xml-Retrieval}}
[[Category:XML]]
[[Category:Information retrieval]]</text>
      <sha1>rwjz8xq9tn8flimcnadsi5q6d2hdweq</sha1>
    </revision>
  </page>
  <page>
    <title>Literature-based discovery</title>
    <ns>0</ns>
    <id>31149053</id>
    <revision>
      <id>598717941</id>
      <parentid>598717802</parentid>
      <timestamp>2014-03-08T17:55:32Z</timestamp>
      <contributor>
        <username>Sprhodes</username>
        <id>1458622</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3162">'''Literature-based discovery''' refers to the use of papers and other [[Academic publishing|academic publications]] (the &quot;literature&quot;) to find new relationships between existing knowledge (the &quot;discovery&quot;). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. 

Literature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and &quot;neglected&quot;.&lt;ref&gt;{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526–557 }}&lt;/ref&gt; It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].

==Swanson linking==
[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]
''Swanson linking'' is a term proposed in 2003&lt;ref&gt;Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111–135. As quoted by Bekhuis&lt;/ref&gt; that refers to connecting two pieces of knowledge previously thought to be unrelated.&lt;ref&gt;{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}&lt;/ref&gt; For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called &quot;disjoint data&quot;), the relationship between illness A and drug C may be unknown. ''Swanson linking'' aims to find these relationships and report them.

==See also==
*[[Arrowsmith System]]
*[[Implicature]]
*[[Latent semantic indexing]]
*[[Metaphor]]

==References==
* Chen, Ran; Hongfei Lin &amp; Zhihao Yang (2011). &quot;Passage retrieval based hidden knowledge discovery from biomedical literature.&quot; ''Expert Systems with Applications: An International Journal'' (August, 2011), vol. 38, no. 8, pp.&amp;nbsp;9958–9964.
*:  '''Abstract''': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&amp;coll=DL&amp;dl=GUIDE&amp;CFID=23143258&amp;CFTOKEN=52033794 ACM DL]

; Further readings
* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). ''Public Knowledge, Private Ignorance: Toward a Library and Information Policy''. Greenwood Publishing Group. p.&amp;nbsp;156. ISBN 0-8371-9485-7.

; Footnotes
{{reflist}}

[[Category:Information retrieval]]
[[Category:Medical research]]


{{science-stub}}</text>
      <sha1>3fuhulmmbu7uhq67z6maw1jeoxqbb9y</sha1>
    </revision>
  </page>
  <page>
    <title>Personalized search</title>
    <ns>0</ns>
    <id>28010520</id>
    <revision>
      <id>643653087</id>
      <parentid>641157500</parentid>
      <timestamp>2015-01-22T10:56:12Z</timestamp>
      <contributor>
        <username>Sprachpfleger</username>
        <id>7354801</id>
      </contributor>
      <minor/>
      <comment>/* Disadvantages */ &quot;do to the fact&quot; → &quot;due ...&quot;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33651">{{essay-like|date=January 2015}}
{{original research|date=January 2015}}
'''Personalized search''' refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to personalizing search results, one involving modifying the user’s query and the other re-ranking search results.&lt;ref&gt;{{cite journal|last=Pitokow|first=James|author2=Hinrich Schütze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM (CACM)|year=2002|volume=45|issue=9|pages=50–55|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}&lt;/ref&gt;

==History==

Google introduced Personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search set up for not just those who have a Google account but everyone as well. There is not very much information on how exactly Google personalizes their searches, however, it is believed that they use user language, location, and web history.&lt;ref&gt;http://personalization.ccs.neu.edu/paper.pdf&lt;/ref&gt;

Early search engines, like [[Yahoo!]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by [[Google]], has become far more complex with the goal to &quot;understand exactly what you mean and give you exactly what you want.&quot;&lt;ref&gt;{{citation | last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}&lt;/ref&gt; Using mathematical algorithms, search engines are now able to return results based on the number of links to an from sites; the more links a site has, the higher it is placed on the page.&lt;ref&gt;{{cite AV media|last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}&lt;/ref&gt; Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.&lt;ref&gt;{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969–982}}&lt;/ref&gt; If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results  indicating that those near the top are more relevant to a user's wants than those below.&lt;ref&gt;{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969–982}}&lt;/ref&gt;

While many [[search engines]] take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.&lt;ref&gt;{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237–251|doi=10.1002/asi.20477}}&lt;/ref&gt; But user supplied information can be hard to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.&lt;ref&gt;{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287–296}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581–590}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Shen|first=X.|coauthors=Tan, B. and Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824–831}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675–684}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415–422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}&lt;/ref&gt;

There are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization&lt;ref&gt;{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}&lt;/ref&gt;). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. The more you keep going the same site through a search result from Google, it believes that you like that page. So when you do certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if you're signed out, Google may personalize your results because it keeps a 180 day record of what a particular web browser has searched for, linked to a cookie in that browser.&lt;ref&gt;{{cite web|last=Sullivan|first=Danny|title=Of &quot;Magic Keywords&quot; and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}&lt;/ref&gt;

In order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University set out to answer this question. By comparing an aggregate set of searches from logged in users against a control group, the research team found that 11.7% of results show differences due to personalization, however this varies widely by search query and result ranking position.&lt;ref&gt;{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}&lt;/ref&gt; Of various factors tested, the two that had measurable impact were being logged in with a Google account and the IP address of the searching users. It should also be noted that results with high degress of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if you searched for &quot;used car sales&quot;, Google may churn out results of local car dealerships in your area. On the other hand, queries with the least amount of personalization include factual queries (&quot;what is&quot;) and health.&lt;ref&gt;{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}&lt;/ref&gt;

When measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when you perform a search and follow it with a subsequent search, the results of the second search is influenced by the first search. An interesting point to note is that the top ranked URLs are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization, based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.&lt;ref&gt;{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}&lt;/ref&gt;

==The Filter Bubble==
{{Main|Filter bubble}}

Several concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by biasing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the &quot;filter bubble&quot; by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or &quot;filter bubbles&quot; where they only see information that they want to, such a consequence of &quot;The Friendly World Syndrome.&quot; As a result people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).&lt;ref&gt;{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}&lt;/ref&gt;

The methods of personalization, and how useful it is to “promote” certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the Filter Bubble happens. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.&lt;ref&gt;{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal Of Pattern Recognition &amp; Artificial Intelligence |year=2007|pages=183–205}}&lt;/ref&gt;

An area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information. This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.&lt;ref&gt;{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}&lt;/ref&gt; While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.&lt;ref&gt;http://{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|accessdate= 27 April 2014}}&lt;/ref&gt;

Many search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.&lt;ref&gt;{{cite journal|last=Wai-Tin|first=Kenneth|coauthors=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEE transaction on knowledge and data engineering|year=2010|volume=22|issue=7|pages=969–982|doi=10.1109/tkde.2009.144}}&lt;/ref&gt;

The feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.&lt;ref&gt;[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html &quot;Google Personalized Results Could Be Bad for Search&quot;]. ''Network World''. Retrieved July 12, 2010.&lt;/ref&gt; An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in &quot;BP&quot; into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.&lt;ref&gt;{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}&lt;/ref&gt;

Some have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].&lt;ref&gt;{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}&lt;/ref&gt;

==The Case of Google==
{{Main|Google Personalized Search}}

The perfect example of search personalization is [[Google]]. Google is not just a search engine, but a corporation that is entering every facet of our lives. Personalization with Google has gone far beyond just search. There are a host of new applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.&lt;ref&gt;{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20–31}}&lt;/ref&gt; that keeps track of all the information directly under one’s name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the [[New York Times]]. The high level of personalization that was available with Google played a significant part in helping remain the world’s most favorite search engine.

One large example of Google’s ability to personalized search is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed as interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information you want. The concern, however, is that the very important information can be held back because it does not match with the criteria that the program sets for the particular user. This can create the “[[filter bubble]]” as described earlier.&lt;ref&gt;{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}&lt;/ref&gt;

An interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff, is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.
Google’s popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although many people would say that having Google personalize your search results based on what you searched previously would be a good thing, there are negatives that come with it.&lt;ref&gt;{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}&lt;/ref&gt;
With the power from this information, Google has chosen to bully its way into other sectors it owned such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.

Using Search Personalization, Google has doubled its video market share to about eighty percent. The legal definition of a monopoly is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.

The analytical firm Experian Hitwise stated that since two thousand and seven, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from thirty one percent in two thousand and seven to ten percent in two thousand and ten. Even Yahoo Images has gone from twelve percent to seven percent. It becomes very apparent that the decline of these companies has come because of Google’s increase in market share from forty three percent in two thousand and seven to about fifty five percent in two thousand and nine.

It might be easy to say that all of this has come from Google being more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google’s bullying because their numbers went from one point three million unique visitors to eleven point nine unique visitors in one month. That kind of growth can only come with the change of a process.

In the end, there are two things in common theme with all of these graphs. The first is that Google’s market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around two thousand and seven, which is around the time that Google began to use its “Universal Search” method.&lt;ref&gt;{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}&lt;/ref&gt;

==Benefits==

One of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human’s capability of processing information has not expanded much.&lt;ref&gt;Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.&lt;/ref&gt; When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show positive correlation between personalized search and the quality of consumers’ decisions.

The first study was conducted by Kristin Diehl from University of South Carolina. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that ‘consumers make worse choices because lower search costs cause them to consider inferior options.’ It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.&lt;ref&gt;Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.&lt;/ref&gt; The study by Gerald Haubl from University of Alberta and Benedict G.C. Dellaert from Maastricht University mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers’ decision quality and reduced the number of products inspected.&lt;ref&gt;Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.&lt;/ref&gt;

==Models==

Personalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.&lt;ref&gt;Coyle, M., &amp; Smyth, B. (2007). Information recovery and discovery in collaborative web search. Advances in Information Retrieval (pp. 356–367).&lt;/ref&gt; Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.

The first model available is based on the users’ historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.

There is another way to personalize search results. In Bracha Shapira and Boaz Zabar’s “Personalized Search: Integrating Collaboration and Social Networks”, Shapira and Zabar focused on a model that utilizes a recommendation system.&lt;ref&gt;Shapira, B., &amp; Zabar, B. (2011). Personalized search: Integrating collaboration and social networks. Journal Of The American Society For Information Science &amp; Technology, 62(1), 146-160. doi:10.1002/asi.21446&lt;/ref&gt; This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.

==Disadvantages==

While there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users’ search engine results to material that aligns with the users’ interests and history. It limits the users’ ability to become exposed to material that would be relevant to the user’s search query but due to the fact that some of this material differs from the user’s interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. “Objectivity matters little when you know what you are looking for, but its lack is problematic when you do not”.&lt;ref&gt;{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426–445|doi=10.1111/j.1467-9973.2012.01759.x}}&lt;/ref&gt;  One of the main functions of the internet is the collection and sharing of information. This is the criticism of search personalization. It limits a core function of the web. It helps prevent users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user’s search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue. The user’s search results will reflect that. The user not be displayed both sides of the issue if the user’s interests lean to one side or another. The user may be missing out on information that could be important. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users even though each user entered the same search query. “When I further distilled the results, I saw that only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on”.&lt;ref&gt;{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|volume=35.6}}&lt;/ref&gt; If search personalization was not active, all the results in theory should have been the same stories in an identical order.

Another disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling your internet interests and histories to other companies. This raises a privacy issue. The issue is if people are content with companies gather and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.

==Sites that use Personalized Search==

E. Pariser author of the Filter Bubble explains how there are differences that search personalization has on both Facebook and Google. Facebook implements personalization when it comes to the amount of things we share and also what pages we “like”. It also takes into consideration our social interactions, whose profile we visit the most, who we message or chat with are all indicators that are used when Facebook uses personalization. Rather than what we share being an indicator of what is filtered out, but Google takes into consideration what we “click” to filter out what comes up in our searches. In addition Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and we share what other people want to see. Even while tagging photographs, Facebook uses personalization and recognition that will automatically assign a name to face for you without you having to tag them. In terms of Google we are provided similar websites and resources based on what we initially click on. This doesn't just affect Google and Facebook. There are even other websites that use the filter tactic to better adhere to user preferences. For example, Netflix also judges from the users search history to suggest movies that they may be interested in for the future. There are cites like Amazon and personal shopping cites also use other peoples history in order to serve their interests better. Twitter also uses personalization by “suggesting” other people to follow. In addition, based on who we “follow” and who we “tweet” and “retweet” at Twitter filters out to peoples best interest for us.  Mark Zuckerberg, founder of Facebook, believed that we only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn’t true. Although personalized search may seem helpful it is not a very accurate representation of who we are as people. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles in order to make themselves look better. Search personalization is not an ideal representation of any person. There are so many cites used for different purposes and that does not make up one person’s identity at all that, but are in fact false representations of ourselves.&lt;ref&gt;http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf&lt;/ref&gt;

==Personalized Search and Online Shopping==

Search engines, such as Google and Yahoo!, utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual’s web clicks, search engines can use personalized search to put forth advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in brick-and-mortar stores. These types of products and services are called long tail items.&lt;ref&gt;Badke, William. “Personalization and Information Literacy”. Online, 47. Feb. 2012.&lt;/ref&gt; Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.

Aside from aiding consumers and businesses in finding one-another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.&lt;ref&gt;Inside Google. &quot;Traffic Report: How Google Is Squeezing Out Competitors and Muscling Into New Markets.&quot; Consumer Watchdog. http://www.consumerwatchdog.org, 2 June 2010. Web.&lt;/ref&gt; In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word “shoes” using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer’s queries.

==References==
{{reflist|30em}}

{{DEFAULTSORT:Personalized search}}
[[Category:Information retrieval]]
[[Category:Internet search engines| ]]
[[Category:Internet terminology]]
[[Category:Personalized search|*]]</text>
      <sha1>egyr2c8p2kn4epe6gppsv07rme78ymg</sha1>
    </revision>
  </page>
  <page>
    <title>Nearest neighbor search</title>
    <ns>0</ns>
    <id>7309022</id>
    <revision>
      <id>648009525</id>
      <parentid>643449591</parentid>
      <timestamp>2015-02-20T08:07:23Z</timestamp>
      <contributor>
        <username>9prabhat</username>
        <id>24183417</id>
      </contributor>
      <comment>/* Linear search */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21665">'''Nearest neighbor search''' ('''NNS'''), also known as '''proximity search''', '''similarity search''' or '''[[Closest pair of points problem|closest point search]]''',  is an [[optimization problem]] for finding closest (or most similar) points. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set ''S'' of points in a space ''M'' and a query point ''q''&amp;nbsp;∈&amp;nbsp;''M'', find the closest point in ''S'' to ''q''. [[Donald Knuth]] in vol. 3 of ''[[The Art of Computer Programming]]'' (1973) called it the '''post-office problem''', referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a ''k''-NN search, where we need to find the ''k'' closest points.

Most commonly ''M'' is a  [[metric space]] and dissimilarity is expressed as a [[distance metric]], which is symmetric and satisfies the [[triangle inequality]]. Even more common, ''M'' is taken to be the ''d''-dimensional [[vector space]] where dissimilarity is measured using the [[Euclidean distance]], [[Taxicab geometry|Manhattan distance]] or other [[Statistical distance|distance metric]]. However, the dissimilarity function can be arbitrary. One example are asymmetric [[Bregman divergence]]s, for which the triangle inequality does not hold.&lt;ref name=Cayton2008&gt;{{Cite journal
 | last1 = Cayton | first1 = Lawerence
 | year = 2008
 | title =  Fast nearest neighbor retrieval for bregman divergences.
 | journal = Proceedings of the 25th international conference on Machine learning
 | pages = 112–119
}}&lt;/ref&gt;

==Applications==

The nearest neighbor search problem arises in numerous fields of application, including:
*[[Pattern recognition]] - in particular for [[optical character recognition]]
*[[Statistical classification]]- see [[k-nearest neighbor algorithm]]
*[[Computer vision]]
*[[Computational Geometry]] - see [[Closest pair of points problem]]
*[[Database]]s - e.g. [[content-based image retrieval]]
*[[Coding theory]] - see [[Decoding methods|maximum likelihood decoding]]
*[[Data compression]] - see [[MPEG-2]] standard
*[[Recommender system|Recommendation systems]], e.g. see [[Collaborative filtering]]
*[[Internet marketing]] - see [[contextual advertising]] and [[behavioral targeting]]
*[[DNA sequencing]]
*[[Spell checking]] - suggesting correct spelling
*[[Plagiarism detection]]
*[[Contact searching algorithms in FEA]]
*[[Similarity score]]s for predicting career paths of professional athletes.
*[[Cluster analysis]] - assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on [[Euclidean distance]]
*[[Chemical similarity]]
*[[Motion planning#Sampling-Based Algorithms|Sampling-Based Motion Planning]]

==Methods==

Various solutions to the NNS problem have been proposed.  The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the [[curse of dimensionality]] states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.

===Linear search===
The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the &quot;best so far&quot;.  This algorithm, sometimes referred to as the naive approach, has a [[running time]] of ''O''(''dN'') where ''N'' is the [[cardinality]] of ''S'' and ''d'' is the dimensionality of ''M''.  There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.&lt;ref&gt;{{cite web|title=A quantitative analysis and performance study for similarity search methods in high dimensional spaces|author=Weber, Schek, Blott | url=http://www.vldb.org/conf/1998/p194.pdf}}&lt;/ref&gt;

===Space partitioning===
Since the 1970s, [[branch and bound]] methodology has been applied to the problem. In the case of Euclidean space this approach is known as [[spatial index]] or spatial access methods. Several [[Space partitioning|space-partitioning]] methods have been developed for solving the NNS problem.  Perhaps the simplest is the [[k-d tree]], which iteratively bisects the search space into two regions containing half of the points of the parent region.  Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is ''O''(log&amp;nbsp;''N'') &lt;ref&gt;{{cite web|title=An introductory tutorial on KD trees|author=Andrew Moore | url=http://www.autonlab.com/autonweb/14665/version/2/part/5/data/moore-tutorial.pdf?branch=main&amp;language=en}}&lt;/ref&gt; in the case of randomly distributed points, worst case complexity analyses have been performed.&lt;ref name=Lee1977&gt;{{Cite journal
 | last1 = Lee | first1 = D. T. | author1-link = Der-Tsai Lee
 | last2 = Wong | first2 = C. K.
 | year = 1977
 | title = Worst-case analysis for region and partial region searches in multidimensional binary search trees and balanced quad trees
 | journal = Acta Informatica
 | volume = 9
 | issue = 1
 | pages = 23–29
 | doi = 10.1007/BF00263763
 | postscript = .
}}&lt;/ref&gt;
Alternatively the [[R-tree]] data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the [[R* tree]].&lt;ref&gt;{{cite doi|10.1145.2F223784.223794}}&lt;/ref&gt; R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.

In case of general metric space branch and bound approach is known under the name of [[metric trees]]. Particular examples include [[vp-tree]] and [[BK-tree]].

Using a set of points taken from a 3-dimensional space and put into a [[Binary space partitioning|BSP tree]], and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm.  (Strictly speaking, no such point may exist, because it may not be unique.  But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.)  The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point.  This may not be the case, but it is a good heuristic.  After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane.  This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched.  If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space.  If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result.  The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.

===Locality sensitive hashing===

[[Locality sensitive hashing]] (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.&lt;ref&gt;{{cite web|author=A. Rajaraman and J. Ullman| url=http://infolab.stanford.edu/~ullman/mmds.html |title=Mining of Massive Datasets, Ch. 3. |year=2010}}&lt;/ref&gt;

===Nearest neighbor search in spaces with small intrinsic dimension===

The [[cover tree]] has a theoretical bound that is based on the dataset's [[doubling constant]]. The bound on search time is ''O''(''c''&lt;sup&gt;12&lt;/sup&gt;&amp;nbsp;log&amp;nbsp;''n'') where ''c''  is the [[Expansivity constant|expansion constant]] of the dataset.

===Vector approximation files===

In high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.&lt;ref&gt;{{cite web|title=An Approximation-Based Data Structure for Similarity Search|author=Weber, Blott}}&lt;/ref&gt;

===Compression/clustering based search===
The VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most &quot;promising&quot; clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed.&lt;ref&gt;{{cite web|title=Adaptive cluster-distance bounding for similarity search in image databases|author=Ramaswamy, Rose, ICIP 2007}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Adaptive cluster-distance bounding for high-dimensional indexing|author=Ramaswamy, Rose, TKDE 2010}}&lt;/ref&gt; Also note the parallels between clustering and LSH.

===Greedy walks===
One possible way to solve NNS is to construct a graph &lt;math&gt;G(V,E)&lt;/math&gt;, where every point &lt;math&gt;x_i \in S &lt;/math&gt; is uniquely associated with vertex &lt;math&gt;v_i \in V &lt;/math&gt;. The search of the point in the set ''S'' closest to the query ''q'' takes the form of the search of vertex in the graph &lt;math&gt;G(V,E)&lt;/math&gt;.
One of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm. It starts from the random vertex &lt;math&gt;v_i \in V &lt;/math&gt;. The algorithm computes a distance value from the query q to each vertex from the neighborhood &lt;math&gt;\{v_j:(v_i,v_j) \in E\}&lt;/math&gt; of  the current vertex &lt;math&gt;v_i&lt;/math&gt;, and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.
This idea was exploited in VoroNet system &lt;ref name=voroNet&gt;{{Cite journal
 | last1 = Olivier | first1 = Beaumont  
 | last2 = Kermarrec | first2 = Anne-Marie
 | last3 = Marchal | first3 = Loris 
 | last4 = Rivière | first4 = Etienne   
 | year = 2006
 | title = VoroNet: A scalable object network based on Voronoi tessellations
 | journal = INRIA
 | volume = RR-5833
 | issue = 1
 | pages = 23–29
 | doi = 10.1007/BF00263763
 | postscript = .
}}&lt;/ref&gt; for the plane, in RayNet system &lt;ref name=rayNet&gt;{{Cite journal
 | last1 = Olivier | first1 = Beaumont  
 | last2 = Kermarrec | first2 = Anne-Marie
 | last4 = Rivière | first4 = Etienne   
 | year = 2007
 | title = Peer to Peer Multidimensional Overlays: Approximating Complex Structures
 | journal = Principles of Distributed Systems
 | volume =  4878
 | issue = .
 | pages = 315–328
 | doi = 10.1007/978-3-540-77096-1_23
 | isbn = 978-3-540-77095-4
 | postscript = .
}}&lt;/ref&gt; for the &lt;math&gt;\mathbb{E}^n&lt;/math&gt; and for the general metric space in Metrized Small World algorithm &lt;ref name=msw2014&gt;{{Cite journal
 | last1 = Malkov | first1 = Yury  
 | last2 = Ponomarenko | first2 = Alexander
 | last3 = Krylov | first3 = Vladimir 
 | last4 = Logvinov | first4 = Andrey   
 | year = 2014
 | title = Approximate nearest neighbor algorithm based on navigable small world graphs
 | journal = Information Systems
 | volume = 45
 | pages = 61–68
 | doi = 10.1016/j.is.2013.10.006
 | postscript = .
}}&lt;/ref&gt;

==Variants==

There are numerous variants of the NNS problem and the two most well-known are the [[K-nearest neighbor algorithm|''k''-nearest neighbor search]] and the [[&amp;epsilon;-approximate nearest neighbor search]].

===&lt;span id=&quot;K-nearest neighbor&quot;&gt; ''k''-nearest neighbor &lt;/span&gt;===

[[K-nearest neighbor algorithm|''k''-nearest neighbor search]] identifies the top ''k'' nearest neighbors to the query.  This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. ''k''-nearest neighbor graphs are graphs in which every point is connected to its ''k'' nearest neighbors.

===Approximate nearest neighbor===
In some applications it may be acceptable to retrieve a &quot;good guess&quot; of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.

Algorithms that support the approximate nearest neighbor search include [[Locality-sensitive hashing#LSH algorithm for nearest neighbor search|locality-sensitive hashing]], [[best bin first]] and [[balanced box-decomposition tree]] based search.&lt;ref&gt;S. Arya, [[David Mount|D. M. Mount]], [[Nathan Netanyahu|N. S. Netanyahu]], R. Silverman and A. Wu, An optimal algorithm for approximate nearest neighbor searching, Journal of the ACM, 45(6):891-923, 1998. [http://www.cse.ust.hk/faculty/arya/pub/JACM.pdf]&lt;/ref&gt;

===Nearest neighbor distance ratio===

[[Nearest neighbor distance ratio]] do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in [[Content-based image retrieval|CBIR]] to retrieve pictures through a &quot;query by example&quot; using the similarity between local features. More generally it is involved in several [[Pattern matching|matching]] problems.

===Fixed-radius near neighbors===

[[Fixed-radius near neighbors]] is the problem where one wants to efficiently find all points given in [[Euclidean space]] within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.

===All nearest neighbors===

For some applications (e.g. [[entropy estimation]]), we may have ''N'' data-points and wish to know which is the nearest neighbor ''for every one of those N points''. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these ''N'' queries to produce a more efficient search. As a simple example: when we find the distance from point ''X'' to point ''Y'', that also tells us the distance from point ''Y'' to point ''X'', so the same calculation can be reused in two different queries.

Given a fixed dimension, a semi-definite positive norm (thereby including every  [[lp space|L&lt;sup&gt;p&lt;/sup&gt; norm]]), and ''n'' points in this space, the nearest neighbour of every point can be found in ''O''(''n''&amp;nbsp;log&amp;nbsp;''n'') time and the ''m'' nearest neighbours of every point can be found in ''O''(''mn''&amp;nbsp;log&amp;nbsp;''n'') time.&lt;ref&gt;{{citation
 | last = Clarkson | first = Kenneth L. | author-link = Kenneth L. Clarkson
 | contribution = Fast algorithms for the all nearest neighbors problem
 | doi = 10.1109/SFCS.1983.16
 | pages = 226–232
 | title = 24th IEEE Symp. Foundations of Computer Science, (FOCS '83)
 | year = 1983| isbn = 0-8186-0508-1 }}.&lt;/ref&gt;&lt;ref name=Vaidya&gt;{{Cite journal
 | doi = 10.1007/BF02187718
 | last1 = Vaidya | first1 = P. M.
 | year = 1989
 | title = An ''O''(''n''&amp;nbsp;log&amp;nbsp;''n'') Algorithm for the All-Nearest-Neighbors Problem 
 | journal = [[Discrete and Computational Geometry]]
 | volume = 4
 | issue = 1
 | pages = 101–115
 | url = http://www.springerlink.com/content/p4mk2608787r7281/?p=09da9252d36e4a1b8396833710ef08cc&amp;pi=8
 | postscript = .
}}&lt;/ref&gt;

==See also==
{{div col|colwidth=20em}}
* [[Range search]]
* [[Set cover problem]]
*[[Statistical distance]]
*[[Closest pair of points problem]]
*[[Ball tree]]
*[[Cluster analysis]]
*[[Neighbor joining]]
*[[Content-based image retrieval]]
*[[Curse of dimensionality]]
*[[Digital signal processing]]
*[[Dimension reduction]]
*[[Fixed-radius near neighbors]]
*[[Fourier analysis]]
*[[Instance-based learning]]
*[[k-nearest neighbor algorithm|''k''-nearest neighbor algorithm]]
*[[Linear least squares (mathematics)|Linear least squares]]
*[[Locality sensitive hashing]]
*[[Multidimensional analysis]]
*[[Nearest-neighbor interpolation]]
*[[Principal component analysis]]
*[[Singular value decomposition]]
*[[Time series]]
*[[Voronoi diagram]]
*[[Wavelet]]
*[[MinHash]]
{{div col end}}

==Notes==
&lt;references/&gt;

==References==
*Andrews, L.. A template for the nearest neighbor problem.  ''C/C++ Users Journal'', vol. 19, no 11 (November 2001), 40 - 49, 2001, ISSN:1075-2838, [http://www.ddj.com/architect/184401449 www.ddj.com/architect/184401449]
*Arya, S., D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu.  An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions.  ''Journal of the ACM'', vol. 45, no. 6, pp.&amp;nbsp;891–923
*Beyer, K., Goldstein, J., Ramakrishnan, R., and Shaft, U. 1999. When is nearest neighbor meaningful? In Proceedings of the 7th ICDT, Jerusalem, Israel.
*Chung-Min Chen and Yibei Ling - A Sampling-Based Estimator for Top-k Query. ICDE 2002: 617-627
*Samet, H. 2006. Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann. ISBN 0-12-369446-9
*Zezula, P., Amato, G., Dohnal, V., and Batko, M. Similarity Search - The Metric Space Approach. Springer, 2006. ISBN 0-387-29146-6

==Further reading==
*{{cite book | last = Shasha | first = Dennis | title = High Performance Discovery in Time Series | publisher = Springer | location = Berlin | year = 2004 | isbn = 0-387-00857-8 }}

==External links==
*[http://simsearch.yury.name/tutorial.html Nearest Neighbors and Similarity Search] – a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits
*[http://sswiki.tierra-aoi.net Similarity Search Wiki] – a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours
*[http://www.kgraph.org KGraph] – a C++ library for fast approximate nearest neighbor search with user-provided distance metric by Wei Dong.
*[http://www.cs.ubc.ca/research/flann/ FLANN] – a library for performing fast approximate nearest neighbor searches in high dimensional spaces  by Marius Muja and David G. Low
*[http://sisap.org/?f=library Metric Spaces Library] – An open-source C-based library for metric space indexing by Karina Figueroa, Gonzalo Navarro, Edgar Chávez
*[https://github.com/searchivarius/NonMetricSpaceLib  Non-Metric Space Library] – An open-source C++ library for exact and approximate searching in non-metric and metric spaces
*[http://www.cs.umd.edu/~mount/ANN/ ANN] – A library for Approximate Nearest Neighbor searching by David M. Mount and Sunil Arya
*[http://www.irisa.fr/texmex/people/jegou/ann.php Product Quantization] – Matlab implementation of approximate nearest neighbor search in the compressed domain by Herve Jegou
*[http://lsd.fi.muni.cz/trac/messif MESSIF] – Metric Similarity Search Implementation Framework by Michal Batko and David Novak
*[http://www.obsearch.net/ OBSearch] – Similarity Search engine for Java (GPL); implementation by Arnoldo Muller, developed during Google Summer of Code 2007
*[http://mrim.imag.fr/georges.quenot/freesoft/knnlsb/ KNNLSB] – K Nearest Neighbors Linear Scan Baseline (distributed, LGPL); implementation by Georges Quénot (LIG-CNRS)
*[http://neartree.sourceforge.net/ NearTree] – An API for finding nearest neighbors among points in spaces of arbitrary dimensions by Lawrence C. Andrews and Herbert J. Bernstein
*[http://nearpy.io/ NearPy] – Python framework for fast approximated nearest neighbor search by Ole Krause-Sparmann
*[http://www.cgal.org/Pkg/SpatialSearchingD dD Spatial Searching] in [[CGAL]] – the Computational Geometry Algorithms Library
*[https://github.com/ryanrhymes/panns Panns] – A Python library for searching approximate nearest neighbors, optimized for large dataset with high dimensional features, developed by Liang Wang

{{DEFAULTSORT:Nearest Neighbor Search}}
[[Category:Approximation algorithms]]
[[Category:Classification algorithms]]
[[Category:Data mining]]
[[Category:Discrete geometry]]
[[Category:Geometric algorithms]]
[[Category:Information retrieval]]
[[Category:Machine learning]]
[[Category:Numerical analysis]]
[[Category:Mathematical optimization]]
[[Category:Searching]]
[[Category:Search algorithms]]</text>
      <sha1>nfrbl9qs5tbr7hdlkv9oj51zwhpi4pj</sha1>
    </revision>
  </page>
  <page>
    <title>Latent semantic analysis</title>
    <ns>0</ns>
    <id>689427</id>
    <revision>
      <id>646788360</id>
      <parentid>644520210</parentid>
      <timestamp>2015-02-12T12:10:02Z</timestamp>
      <contributor>
        <username>Miller james digi</username>
        <id>24097235</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23693">{{mergefrom|Latent semantic indexing|date=July 2012}}
{{semantics}}
'''Latent semantic analysis''' ('''LSA''') is a technique in [[natural language processing]], in particular in [[vectorial semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.&lt;ref&gt;{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188}}&lt;/ref&gt;

An information retrieval method using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853]) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing '''(LSI)''']].&lt;ref&gt;{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}&lt;/ref&gt;

== Overview ==

=== Occurrence matrix ===
LSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency–inverse document frequency): the element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.

This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.

=== Rank lowering ===
After the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]&lt;ref&gt;Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}&lt;/ref&gt; to the [[term-document matrix]]. There could be various reasons for these approximations:

* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an ''approximation'' (a &quot;least and necessary evil&quot;).
* The original term-document matrix is presumed ''noisy'': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a ''de-noisified matrix'' (a better matrix than the original).
* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the &quot;true&quot; term-document matrix.  That is, the original matrix lists only the words actually ''in'' each document, whereas we might be interested in all words ''related to'' each document—generally a much larger set due to [[synonymy]].

The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:

:: {(car), (truck), (flower)} --&gt;  {(1.3452 * car + 0.2828 * truck), (flower)}

This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the &quot;right&quot; direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.

=== Derivation ===
Let &lt;math&gt;X&lt;/math&gt; be a matrix where element &lt;math&gt;(i,j)&lt;/math&gt; describes the occurrence of term &lt;math&gt;i&lt;/math&gt; in document &lt;math&gt;j&lt;/math&gt; (this can be, for example, the frequency). &lt;math&gt;X&lt;/math&gt; will look like this:

:&lt;math&gt;
\begin{matrix} 
 &amp; \textbf{d}_j \\
 &amp; \downarrow \\
\textbf{t}_i^T \rightarrow &amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\vdots &amp; \ddots &amp; \vdots \\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
\end{matrix}
&lt;/math&gt;

Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:

:&lt;math&gt;\textbf{t}_i^T = \begin{bmatrix} x_{i,1} &amp; \dots &amp; x_{i,n} \end{bmatrix}&lt;/math&gt;

Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:

:&lt;math&gt;\textbf{d}_j = \begin{bmatrix} x_{1,j} \\ \vdots \\ x_{m,j} \end{bmatrix}&lt;/math&gt;

Now the [[dot product]] &lt;math&gt;\textbf{t}_i^T \textbf{t}_p&lt;/math&gt; between two term vectors gives the [[correlation]] between the terms over the documents. The [[matrix product]] &lt;math&gt;X X^T&lt;/math&gt; contains all these dot products. Element &lt;math&gt;(i,p)&lt;/math&gt; (which is equal to element &lt;math&gt;(p,i)&lt;/math&gt;) contains the dot product &lt;math&gt;\textbf{t}_i^T \textbf{t}_p&lt;/math&gt; (&lt;math&gt; = \textbf{t}_p^T \textbf{t}_i&lt;/math&gt;). Likewise, the matrix &lt;math&gt;X^T X&lt;/math&gt; contains the dot products between all the document vectors, giving their correlation over the terms: &lt;math&gt;\textbf{d}_j^T \textbf{d}_q = \textbf{d}_q^T \textbf{d}_j&lt;/math&gt;.

Now, from the theory of linear algebra, there exists a decomposition of &lt;math&gt;X&lt;/math&gt; such that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are [[orthogonal matrix|orthogonal matrices]] and &lt;math&gt;\Sigma&lt;/math&gt; is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):

:&lt;math&gt;
\begin{matrix}
X = U \Sigma V^T
\end{matrix}
&lt;/math&gt;

The matrix products giving us the term and document correlations then become

:&lt;math&gt;
\begin{matrix}
X X^T &amp;=&amp; (U \Sigma V^T) (U \Sigma V^T)^T = (U \Sigma V^T) (V^{T^T} \Sigma^T U^T) = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T \\
X^T X &amp;=&amp; (U \Sigma V^T)^T (U \Sigma V^T) = (V^{T^T} \Sigma^T U^T) (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T
\end{matrix}
&lt;/math&gt;

Since &lt;math&gt;\Sigma \Sigma^T&lt;/math&gt; and &lt;math&gt;\Sigma^T \Sigma&lt;/math&gt; are diagonal we see that &lt;math&gt;U&lt;/math&gt; must contain the [[eigenvector]]s of &lt;math&gt;X X^T&lt;/math&gt;, while &lt;math&gt;V&lt;/math&gt; must be the eigenvectors of &lt;math&gt;X^T X&lt;/math&gt;. Both products have the same non-zero eigenvalues, given by the non-zero entries of &lt;math&gt;\Sigma \Sigma^T&lt;/math&gt;, or equally, by the non-zero entries of &lt;math&gt;\Sigma^T\Sigma&lt;/math&gt;. Now the decomposition looks like this:

:&lt;math&gt;
\begin{matrix} 
 &amp; X &amp; &amp; &amp; U &amp; &amp; \Sigma &amp; &amp; V^T \\
 &amp; (\textbf{d}_j) &amp; &amp; &amp; &amp; &amp; &amp; &amp; (\hat{\textbf{d}}_j) \\
 &amp; \downarrow &amp; &amp; &amp; &amp; &amp; &amp; &amp; \downarrow \\
(\textbf{t}_i^T) \rightarrow 
&amp;
\begin{bmatrix} 
x_{1,1} &amp; \dots &amp; x_{1,n} \\
\\
\vdots &amp; \ddots &amp; \vdots \\
\\
x_{m,1} &amp; \dots &amp; x_{m,n} \\
\end{bmatrix}
&amp;
=
&amp;
(\hat{\textbf{t}}_i^T) \rightarrow
&amp;
\begin{bmatrix} 
\begin{bmatrix} \, \\ \, \\ \textbf{u}_1 \\ \, \\ \,\end{bmatrix} 
\dots
\begin{bmatrix} \, \\ \, \\ \textbf{u}_l \\ \, \\ \, \end{bmatrix}
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\sigma_1 &amp; \dots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \dots &amp; \sigma_l \\
\end{bmatrix}
&amp;
\cdot
&amp;
\begin{bmatrix} 
\begin{bmatrix} &amp; &amp; \textbf{v}_1 &amp; &amp; \end{bmatrix} \\
\vdots \\
\begin{bmatrix} &amp; &amp; \textbf{v}_l &amp; &amp; \end{bmatrix}
\end{bmatrix}
\end{matrix}
&lt;/math&gt;

The values &lt;math&gt;\sigma_1, \dots, \sigma_l&lt;/math&gt; are called the singular values, and &lt;math&gt;u_1, \dots, u_l&lt;/math&gt; and &lt;math&gt;v_1, \dots, v_l&lt;/math&gt; the left and right singular vectors.
Notice the only part of &lt;math&gt;U&lt;/math&gt; that contributes to &lt;math&gt;\textbf{t}_i&lt;/math&gt; is the &lt;math&gt;i\textrm{'th}&lt;/math&gt; row.
Let this row vector be called &lt;math&gt;\hat{\textrm{t}}_i&lt;/math&gt;.
Likewise, the only part of &lt;math&gt;V^T&lt;/math&gt; that contributes to &lt;math&gt;\textbf{d}_j&lt;/math&gt; is the &lt;math&gt;j\textrm{'th}&lt;/math&gt; column, &lt;math&gt;\hat{ \textrm{d}}_j&lt;/math&gt;.
These are ''not'' the eigenvectors, but ''depend'' on ''all'' the eigenvectors.

It turns out that when you select the &lt;math&gt;k&lt;/math&gt; largest singular values, and their corresponding singular vectors from &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, you get the rank &lt;math&gt;k&lt;/math&gt; approximation to &lt;math&gt;X&lt;/math&gt; with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a &quot;semantic space&quot;. The vector &lt;math&gt;\hat{\textbf{t}}_i&lt;/math&gt; then has &lt;math&gt;k&lt;/math&gt; entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the vector &lt;math&gt;\hat{\textbf{d}}_j&lt;/math&gt; is an approximation in this lower-dimensional space. We write this approximation as

:&lt;math&gt;X_k = U_k \Sigma_k V_k^T&lt;/math&gt;

You can now do the following:
* See how related documents &lt;math&gt;j&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; are in the low-dimensional space by comparing the vectors &lt;math&gt;\Sigma_k \hat{\textbf{d}}_j &lt;/math&gt; and &lt;math&gt;\Sigma_k \hat{\textbf{d}}_q &lt;/math&gt; (typically by [[vector space model|cosine similarity]]).
* Comparing terms &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;p&lt;/math&gt; by comparing the vectors &lt;math&gt;\Sigma_k \hat{\textbf{t}}_i^T &lt;/math&gt; and &lt;math&gt;\Sigma_k \hat{\textbf{t}}_p^T &lt;/math&gt;.
* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.
* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.

To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:

:&lt;math&gt;\hat{\textbf{d}}_j = \Sigma_k^{-1} U_k^T \textbf{d}_j&lt;/math&gt;

Note here that the inverse of the diagonal matrix &lt;math&gt;\Sigma_k&lt;/math&gt; may be found by inverting each nonzero value within the matrix.

This means that if you have a query vector &lt;math&gt;q&lt;/math&gt;, you must do the translation &lt;math&gt;\hat{\textbf{q}} = \Sigma_k^{-1} U_k^T \textbf{q}&lt;/math&gt; before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:

:&lt;math&gt;\textbf{t}_i^T = \hat{\textbf{t}}_i^T \Sigma_k V_k^T&lt;/math&gt;

:&lt;math&gt;\hat{\textbf{t}}_i^T = \textbf{t}_i^T V_k^{-T} \Sigma_k^{-1} = \textbf{t}_i^T V_k \Sigma_k^{-1}&lt;/math&gt;

:&lt;math&gt;\hat{\textbf{t}}_i = \Sigma_k^{-1}  V_k^T \textbf{t}_i&lt;/math&gt;

== Applications ==

The new low-dimensional space typically can be used to:
* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).
* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).
* Find relations between terms ([[synonymy]] and [[polysemy]]).
* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).
* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.&lt;ref name=&quot;Alain2009&quot;&gt;{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model | author=Alain Lifchitz, Sandra Jhean-Larose, Guy Denhière | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201–1209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}&lt;/ref&gt;

Synonymy and polysemy are fundamental problems in [[natural language processing]]: 
* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for &quot;doctors&quot; may not return a document containing the word &quot;[[physicians]]&quot;, even though the words have the same meaning.
* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word &quot;tree&quot; probably desire different sets of documents.

=== Commercial applications ===

LSA has been used to assist in performing [[prior art]] searches for [[patents]].&lt;ref name=&quot;Gerry2007&quot;&gt;{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435 | postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

=== Applications in human memory ===

The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].&lt;ref&gt;{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall | author=Marc W. Howard and Michael J. Kahana |year=1999}}&lt;/ref&gt;

When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.&lt;ref&gt;{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb et al. | booktitle=Interspeech'2005|year=2006}}&lt;/ref&gt;

Another model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.&lt;ref&gt;{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=5/8/2011}}&lt;/ref&gt;

== Implementation ==

The [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.&lt;ref name=&quot;Genevi2005&quot;&gt;{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis | author=Geneviève Gorrell and Brandyn Webb | booktitle=Interspeech'2005 |year=2005}}&lt;/ref&gt;
A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.&lt;ref name=&quot;brand2006&quot;&gt;{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20–30 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}&lt;/ref&gt; [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.
In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.&lt;ref&gt;doi: 10.1109/ICCSNT.2011.6182070&lt;/ref&gt;

== Limitations ==
Some of LSA's drawbacks include:

* The resulting dimensions might be difficult to interpret. For instance, in
:: {(car), (truck), (flower)} ↦  {(1.3452 * car + 0.2828 * truck), (flower)}
:the (1.3452 * car + 0.2828 * truck) component could be interpreted as &quot;vehicle&quot;. However, it is very likely that cases close to
:: {(car), (bottle), (flower)} ↦  {(1.3452 * car + 0.2828 * '''bottle'''), (flower)}
:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.

* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word){{Citation needed|date=October 2013}}.  Each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of &quot;chair&quot; in a document containing &quot;The Chair of the Board&quot; and in a separate document containing &quot;the chair maker&quot; are considered the same.  The behavior results in the vector representation being an ''average'' of all the word's different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).

* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words.

* To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.&lt;ref&gt;[http://www.translational-medicine.com/content/12/1/324 J Transl Med. 2014 Nov 27;12(1):324.]&lt;/ref&gt;

* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.&lt;ref name=&quot;Thomas1999&quot;&gt;{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}&lt;/ref&gt;

== See also ==
* [[Compound term processing]]
* [[Explicit semantic analysis]]
* [[Latent semantic mapping]]
* [[Latent Semantic Structure Indexing]]
* [[Principal components analysis]]
* [[Probabilistic latent semantic analysis]]
* [[Spamdexing]]
* [[Topic model]]
** [[Latent Dirichlet allocation]]
* [[Vectorial semantics]]
* [[Coh-Metrix]]

== References ==
{{Reflist}}
* {{cite journal
 | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
 |format=PDF| title=Introduction to Latent Semantic Analysis
 | author=[[Thomas Landauer]], Peter W. Foltz, &amp; Darrell Laham
 | journal=Discourse Processes
 | volume=25
 | pages=259–284
 |year=1998
 | doi=10.1080/01638539809545028
 | issue=2–3
}}
* {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 |format=PDF| title=Indexing by Latent Semantic Analysis
 | author=[[Scott Deerwester]], [[Susan Dumais|Susan T. Dumais]], [[George Furnas|George W. Furnas]], [[Thomas Landauer|Thomas K. Landauer]], [[Richard Harshman]]
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391–407
 | year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9
}} Original article where the model was first exposed.
* {{cite journal
 | url=http://citeseer.ist.psu.edu/berry95using.html
 | title=Using Linear Algebra for Intelligent Information Retrieval
 | author=Michael Berry, [[Susan Dumais|Susan T. Dumais]], Gavin W. O'Brien
 |year=1995
}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.
* {{cite web
 | url=http://iv.slis.indiana.edu/sw/lsa.html
 | title=Latent Semantic Analysis
 | publisher=InfoVis
}}
* {{cite web
 | url=http://cran.at.r-project.org/web/packages/lsa/index.html
 | title=An Open Source LSA Package for R
 | publisher=CRAN
 | author=Fridolin Wild
 | date=November 23, 2005
 | accessdate=2006-11-20
}}
* {{ cite web
 | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM
 | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge
 | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]
 | accessdate=2007-07-02
}}

==External links==

===Articles on LSA===
* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.

===Talks and demonstrations===
* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].
* [http://www.semanticsearchart.com/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.

===Implementations===

Due to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.
* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA
* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA
* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices
* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)
* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA
* [[Gensim]] contains a fast, online Python implementation of LSA for matrices larger than RAM.

{{DEFAULTSORT:Latent Semantic Analysis}}
[[Category:Information retrieval]]
[[Category:Natural language processing]]
[[Category:Latent variable models]]

[[fa:آنالیز پنهان مفهومی احتمالی]]</text>
      <sha1>oc6t0rwzvvh1rbiapxeebhe3r7taq79</sha1>
    </revision>
  </page>
  <page>
    <title>Online search</title>
    <ns>0</ns>
    <id>31385661</id>
    <revision>
      <id>569675660</id>
      <parentid>569675252</parentid>
      <timestamp>2013-08-22T04:41:40Z</timestamp>
      <contributor>
        <username>SFK2</username>
        <id>17240256</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/199.127.56.66|199.127.56.66]] ([[User talk:199.127.56.66|talk]]). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="915">'''Online search''' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].&lt;ref name=&quot;whatis?&quot;&gt;{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=12–18|id=Eric:EJ214713| accessdate=2011-04-04}}&lt;/ref&gt; Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.&lt;ref name=&quot;whatis?&quot;/&gt; In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.&lt;ref name=&quot;whatis?&quot;/&gt; Today, searches through [[web search engine]]s constitute the majority of online searches.

Online searches often supplement reference transactions.

==References==
{{reflist}}

{{Internet search}}

[[Category:Internet terminology]]
[[Category:Information retrieval]]


{{web-stub}}</text>
      <sha1>lu4a3wjx0o8kmvu8axtlux1wg7s95wt</sha1>
    </revision>
  </page>
  <page>
    <title>Pikimal</title>
    <ns>0</ns>
    <id>31737261</id>
    <revision>
      <id>646528583</id>
      <parentid>626813305</parentid>
      <timestamp>2015-02-10T18:11:23Z</timestamp>
      <contributor>
        <username>TexasAndroid</username>
        <id>271376</id>
      </contributor>
      <comment>removed [[Category:Pittsburgh, Pennsylvania]]; added [[Category:Companies based in Pittsburgh, Pennsylvania‎]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2735">{{Infobox company |
 name   = Pikimal |
 logo   = [[File:Pikimal logo.jpg|200px]] |
 type   = [[Limited liability company|LLC]]|
 company_slogan = |
|founder = Eric Silver|
 foundation     = 2010|
 location       = [[Pittsburgh, Pennsylvania]], [[United States]]|
 key_people     = Eric Silver, Chief Executive Officer|
 industry       = [[Search engine technology|Search]] |
&lt;!--Please fill in:--&gt;
 revenue        = |
 operating_income = |
 net_income     = | 
 num_employees  = 13&lt;ref name=&quot;businesstimes&quot;&gt;{{cite news|url=http://www.bizjournals.com/pittsburgh/print-edition/2011/03/25/pikimal-comparison-shopping-stand-out.html | title=Pikimal, a comparison shopping website, hopes to stand out from crowd | work= Pittsburgh Business Times | first=Malia | last=Spencer | date=25 March 2011}}&lt;/ref&gt; |
 homepage       = [http://pikimal.com/ www.pikimal.com]|
}}

'''Pikimal''' (pronounced as pick-em-all)&lt;ref&gt;{{cite web|url=http://www.popcitymedia.com/innovationnews/pikimal033011.aspx | title=Shop Smarter With Pikimal – POP City Media }}&lt;/ref&gt; is a website, designed as a [[decision engine]] that uses consumer input to provide specialized search results for products and categories.

Unlike typical [[Web search engine|search engines]], Pikimal mines data to provide users with only the facts pertaining to their search, as a hopeful solution to [[SEO]] and marketing biased search results.

As of April 2011, Pikimal had 13 full-time employees in Pittsburgh, PA, interns, and various contractors around the world.&lt;ref name=&quot;businesstimes&quot; /&gt;

== History ==

Pikimal was founded in January 2010 by [[Eric Silver]], previously the chief marketing officer at [[Modcloth]]. The Pikimal site was launched in [[public beta]] form in October 2010.&lt;ref name=&quot;businesstimes&quot; /&gt;

== Functionality ==

Pikimal allows users to adjust sliders to express what facts of a product are particularly important to them. These percentages are combined with an algorithm to provide users with product recommendations that are rooted directly in facts, but only the facts they find most relevant.&lt;ref&gt;{{cite web|url=http://www.youtube.com/watch?v=imOUklpphcM&amp;feature=player_embedded | title=What is Pikimal? Video}}&lt;/ref&gt;

== Pivot and Shutdown ==

In 2012 Pikimal changed it name to [[Webkite]] and pivoted to provide faceted search solutions to other companies. As of September 2014 pikimal.com and all associated sites has been shutdown.

== References ==
{{Reflist}}

== External links ==
* {{official website|http://pikimal.com}}

[[Category:Internet search engines]]
[[Category:Information retrieval]]
[[Category:Internet properties established in 2010]]
[[Category:Knowledge markets]]
[[Category:Companies based in Pittsburgh, Pennsylvania‎]]</text>
      <sha1>o0otiuvbxyo27bptcruu7lgoait9dqh</sha1>
    </revision>
  </page>
  <page>
    <title>Masterseek</title>
    <ns>0</ns>
    <id>31747262</id>
    <revision>
      <id>640465781</id>
      <parentid>640465758</parentid>
      <timestamp>2015-01-01T03:11:10Z</timestamp>
      <contributor>
        <username>Ceyockey</username>
        <id>150564</id>
      </contributor>
      <comment>added [[Category:Software companies established in 1999]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12067">{{refimprove|date=July 2014}}
{{Infobox company
| name     = Masterseek
| logo     = [[Image:masterseek logo.png|260px]]
| type     = [[Private company|Private]]
| traded_as        = 
| foundation       = [[Denmark]] (1999)
| founder          = [[Rasmus Refer]]
| location_city    = [[New York City]]
| location_country = {{nowrap|United States}}
| area_served      = Worldwide
| key_people       = Rasmus Refer &lt;small&gt;(Co-Founder, [[Chief executive officer|CEO]])&lt;/small&gt;&lt;br /&gt;Jørgen Trygved &lt;small&gt;(Co-Founder)&lt;/small&gt;&lt;br /&gt;Qasim Raza &lt;small&gt;([[Chief technology officer|CTO]])&lt;/small&gt;&lt;br /&gt;Robert Perz &lt;small&gt;(COO, 2005-08)&lt;/small&gt;
| industry         = Internet&lt;br&gt;Computer software
| products         = [[Business-to-business|B2B]] [[Search Engine]]
| revenue          = 
| operating_income =
| net_income       = 
| assets           = 
| num_employees    =
| subsid           = [[Accoona]]
| homepage         = {{URL|http://www.masterseek.com/}}
| intl = yes
}}

'''Masterseek Corp.''' is a [[Business-to-business|B2B]] (Business to Business) [[search engine]] founded in [[Denmark]] in 1999.&lt;ref name=&quot;hartzer&quot;/&gt; It currently hosts over 83 million worldwide company profiles from 75 countries,&lt;ref name=&quot;usti&quot;/&gt; and business subscribers are given complete control over their corporate profiles.&lt;ref name=&quot;hartzer&quot;/&gt; According to the amount of listed profiles, they are the largest B2B search engine worldwide.&lt;ref name=&quot;seochat&quot;/&gt;

==Founding==
&lt;!-- Deleted image removed: [[File: Rasmus refer.png|thumb|150px|left|[[Rasmus Refer]]]] --&gt;
Masterseek was founded in Denmark by [[Rasmus Refer]] in 1999.&lt;ref name=&quot;hartzer&quot;/&gt; Their Denmark headquarters is located at Bredgade 29, DK-1260 Kbh. K, and they also have a current headquarters in [[New York City]], at 82 [[Wall Street]].&lt;ref name=&quot;csc&quot;/&gt;

According to its executives, Masterseek utilizes a business model based on an annual business subscription fee of USD $149, in return for which subscribers receive full editing control over their corporate profile, content and advertising, and control over widgets and embedded video, among other factors.&lt;ref name=&quot;betaversion&quot;/&gt;

==Finances==
As of June 2008, accountancy firm Horwart International had approximated the raw market value of the Masterseek company at $150 million.&lt;ref name=&quot;investors&quot;/&gt; The company remains privately owned, but also in June 2008, it sold 10% of its authorized stocks to a range of foreign investors.&lt;ref name=&quot;investors&quot;/&gt; The company announced on January 31, 2009 that they company was again offering a limited number of shares for sale in order to raise $4–6 million in order to gain a listing on the [[Swedish people|Swedish]] marketplace AktieTorget. Founder Refer also announced there were plans for an [[Initial Public Offering|IPO]].&lt;ref name=&quot;ipo&quot;/&gt; By October 2009, they had signed with the Swedish-based company Thenberg &amp; Kinde Fondkommission AB for financing.&lt;ref name=&quot;seochat&quot;/&gt;

In the Danish company register the name Masterseek is coming up in three bankruptcies and one compulsory dissolution.&lt;ref name=&quot;businessdk-cheats&quot; /&gt;

==Statistics==
In June 2008, the company stated it had 50 million company profiles, from over 75 countries, and handled 90,000 B2B searches daily.&lt;ref name=&quot;hartzer&quot;/&gt;&lt;ref name=&quot;ipo&quot;/&gt;&lt;ref name=&quot;strengthen&quot;/&gt; The company stated they had 82 million profiles on March 21, 2011, with an average of 300,000 new profiles added monthly.&lt;ref name=&quot;betaversion&quot;/&gt;

==Acquiring Accoona==
[[File:accoona logo.png|right|220px]]
On October 30, 2008, it was announced that Masterseek had acquired the B2B search engine [[Accoona]].&lt;ref name=&quot;hartzer&quot;/&gt;&lt;ref name=&quot;paidcontent&quot;/&gt; 
&quot;When [Business Insider] first heard about the money-losing Jersey City-based startup filing for IPO last year, [their] impulse was to run away screaming.&quot;&lt;ref name=&quot;accoonnaddead&quot;/&gt; The search engine had been fairly successful in the United States and [[China]],&lt;ref name=&quot;search&quot;/&gt; where it had an exclusive partnership with ''[[China Daily News]]''.&lt;ref name=&quot;accoonnaddead&quot;/&gt; On August 3, 2006, ''[[Time Magazine|TIME]]'' had dubbed Accoona one of its &quot;50 best websites,&quot; illustrating how the search engine used [[artificial intelligence]] to &quot;understand&quot; the meaning of keyword queries.&lt;ref name=&quot;coolest&quot;/&gt; Accoona had run into difficulties and gone defunct by early October 2008, withdrawing its [[Initial Public Offering|IPO]].&lt;ref name=&quot;accoonnaddead&quot;/&gt;

After Masterseek bought the remaining search engine codes, domain name, and assets,&lt;ref name=&quot;hartzer&quot;/&gt; Accoona was integrated with Masterseek, and re-launched in the USA and China. It was launched in Europe in January 2009.&lt;ref name=&quot;search&quot;/&gt; Accoona information was also integrated into the Masterseek search engine.&lt;ref name=&quot;hartzer&quot;/&gt;

==Technology==
The Masterseek search engine relies on web crawlers that automatically collect and sort company details from the internet.&lt;ref name=&quot;strengthen&quot;/&gt; Searches can look up company profiles, contact information, and descriptions of products and services. Searches can be global, national, regional, or involved local markets. Hits are listed by relevance according to search terms. There are different search options, including a specific product search, company searches, and people searches. Results can be displayed in most languages.&lt;ref name=&quot;seochat&quot;/&gt; The search engine also offers MasterRank, a point system for ranking corporate websites.&lt;ref name=&quot;Bussinessweek&quot;/&gt;

===Beta version===
Masterseek released a new [[Beta Version]] of its search engine on March 25, 2011. Before then, it was accessible to business owners, managers, and other professionals, but the Beta Version made searching the site free and open to the public.&lt;ref name=&quot;betaversion&quot;/&gt;

== Warnings against Masterseek Corp. ==
In December 2009 the Swedish [[Financial Supervisory Authority (Sweden)|Financial Supervisory Authority]] issued a warning against Masterseek Corp. to warn investors &lt;ref name=&quot;fise&quot; /&gt;&lt;ref name=&quot;businessdk&quot; /&gt;

In February 2010 the  issued a warning against Masterseek and related companies Bark Group and Blogger Wave.&lt;ref name=&quot;shareholdersdk&quot; /&gt;

==Sponsorships==
On July 5, 2007, Masterseek announced they were cosponsors to [[Team CSC]], Denmark's cycling team, beginning with the team's involvement in the [[Tour de France]]. The Masterseek name began to be displayed on the team's apparel that week, with the Tour's start in [[London]].&lt;ref name=&quot;csc&quot;/&gt;

==Management==
*[[Rasmus Refer]] - Founder, Director, President of Technology
*Qasim Raza - Chief Technology Officer&lt;ref name=&quot;Bussinessweek&quot;/&gt;

==See also==
*[[Accoona]]
*[[Business-to-business|B2B]]
*[[Search engines]]

== References ==
{{reflist|2| refs =

&lt;ref name=&quot;shareholdersdk&quot;&gt;{{cite news
|url=http://www.shareholders.dk/art/templates/pressemeddelelse.aspx?articleid=512&amp;zoneid=29
|title=Danish Shareholders Association warns against Bark Group, Blogger Wave and Masterseek
|publisher=[http://shareholders.dk/ Dansk Aktionærforening]
|accessdate=2014-07-27}}&lt;/ref&gt;

&lt;ref name=&quot;businessdk&quot;&gt;{{cite news
|url=http://www.business.dk/digital/it-firma-snyder-investorer
|title=IT-firma snyder investorer (IT firm cheating investors)
|publisher=[http://business.dk/ Business.dk]
|accessdate=2014-07-03}}&lt;/ref&gt;

&lt;ref name=&quot;businessdk-cheats&quot;&gt;{{cite news
|url=http://bizzen.blogs.business.dk/2010/02/09/plattenslagere-skamrider-danske-varem%C3%A6rker-som-carlsberg-danisco-og-coop/
|title=Plattenslagere skamrider danske varemærker som Carlsberg, Danisco og Coop (Cheats shame rides Danish brands)
|publisher=[http://business.dk/ Business.dk]
|accessdate=2014-07-25}}&lt;/ref&gt;

&lt;ref name=&quot;fise&quot;&gt;{{cite news
|url=http://www.fi.se/Folder-EN/Startpage/Register/Investor-alerts/Warning-list/Warning-against-Masterseek-Corp/
|title=Warning against Masterseek Corp.
|publisher=Finansinspektionen
|accessdate=2014-07-03}}&lt;/ref&gt;

&lt;ref name=&quot;Bussinessweek&quot;&gt;{{cite news
|url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=29327873
|title=Masterseek Corp.
|last=
|first=
|date=
|publisher=''[[Businessweek]]''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;coolest&quot;&gt;{{cite news
|url=http://www.time.com/time/business/article/0,8599,1222614,00.html
|title=50 Coolest Websites: 2006
|last=Buechner
|first=Maryanne
|date=August 3, 2006
|publisher=''[[Time Magazine|TIME]]''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;strengthen&quot;&gt;{{cite news
|url=http://www.reuters.com/article/2008/06/26/idUS149456+26-Jun-2008+MW20080626
|title=Global Business Search Engine to Strengthen Its Advertising Network for B2B Search
|last=
|first=
|date=June 26, 2008
|publisher=''[[Reuters]]''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;investors&quot;&gt;{{cite news
|url=http://www.reuters.com/article/2008/06/27/idUS85249+27-Jun-2008+MW20080627
|title=Search Engine is Looking for Strategic Investors
|last=
|first=Masterseek
|date=June 27, 2008
|publisher=''[[Reuters]]''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;accoonnaddead&quot;&gt;{{cite news
|url=http://www.businessinsider.com/2008/10/dead-search-engine-accoona-officially-dead
|title=Dead Search Engine Accoona Officially Dead
|last=Krangel
|first=Eric
|date=October 3, 2008
|publisher=''[[Business Insider]]''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;hartzer&quot;&gt;{{cite news
|url=https://www.billhartzer.com/pages/b2b-search-engine-accoona-acquired-by-masterseek/
|title=B2B Search Engine Accoona Acquired by Masterseek
|last=Hartzer
|first=Bill
|date=November 5, 2008
|publisher=BillHartzer.com: Search Engine Marketing
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;search&quot;&gt;{{cite news
|url=http://blog.searchenginewatch.com/081105-115108
|title=Accoona Acquired by Masterseek
|last=Johnson
|first=Nathania 
|date=November 5, 2008
|publisher=''[[Search Engine Watch]]''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;paidcontent&quot;&gt;{{cite news
|url=http://paidcontent.org/article/419-almost-dead-search-engine-accoona-bought-by-denmarks-masterseek/
|title=Almost-Dead Search Engine Accoona Bought by Denmark's Masterseek
|last=
|first=
|date=November 2008
|publisher=''Paid Content''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;ipo&quot;&gt;{{cite news
|url=http://www.reuters.com/article/2009/01/31/idUS85625+31-Jan-2009+MW20090131
|title=Masterseek.com Is Planning for an IPO
|last=
|first=Masterseek
|date=January 31, 2009
|publisher=''[[Reuters]]''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;seochat&quot;&gt;{{cite news
|url=http://www.seochat.com/c/a/Search-Engine-News/Masterseek-a-Global-Business-Search-Engine/
|title=Masterseek: a Global Business Search Engine
|last=Morgan
|first=KC
|date=October 27, 2009
|publisher=''SEOchat''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;betaversion&quot;&gt;{{cite news
|url=http://www.wiredprnews.com/2011/03/21/masterseek-expeced-to-release-beta-version-of-search-engine-continue-dominance-in-business-to-business-search_2011032117895.html
|title=Masterseek Expected to Release Beta Version of Search Engine
|last=
|first=
|date=March 21, 2011
|publisher=WirePRNews
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;usti&quot;&gt;{{cite news
|url=http://usbusinesstimes.com/internet/3424-business-search-powerhouse-masterseek-partners-with-cutting-edge-job-database-simply-hired.html
|title=Masterseek.com Partners with Simply Hired
|last=
|first=admin
|date=April 16, 2011
|publisher=''US Business Times''
|accessdate=2011-05-08}}&lt;/ref&gt;

&lt;ref name=&quot;csc&quot;&gt;{{cite news
|url=http://www.global-business-profiles.com/masterseek-cosponsors-team-csc/
|title=Masterseek Cosponsors Team Csc
|last=
|first=
|date=May 4, 2011
|publisher=Global Business Profiles
|accessdate=2011-05-08}}&lt;/ref&gt;

}}

==External links==
*{{Official website|http://www.masterseek.com/}}
*[http://twitter.com/#!/masterseek_tw1 Masterseek] on [[Twitter]]

[[Category:Internet search engines]]
[[Category:Web service providers]]
[[Category:Internet properties established in 1999]]
[[Category:Business services companies established in 1999]]
[[Category:Information retrieval]]
[[Category:Online companies]]
[[Category:Software companies established in 1999]]

[[da:Dansk Aktionærforening|Danish Shareholders Association]]</text>
      <sha1>kpxzm43js5hv2xyhrzg2jf76ur5col0</sha1>
    </revision>
  </page>
  <page>
    <title>Personalization</title>
    <ns>0</ns>
    <id>1656760</id>
    <revision>
      <id>642471854</id>
      <parentid>641524621</parentid>
      <timestamp>2015-01-14T16:32:56Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor/>
      <comment>/* Promotional merchandise */CS1 fixes;  |language= spelling; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13185">{{cleanup-reorganize|date=June 2008}}

'''Personalization''', also known as '''customization''', involves using technology to accommodate the differences between individuals.

==Web pages==
{{see also|Web pages|Adaptive hypermedia}}  
[[Web page]]s are personalized based on the characteristics (interests, social category, context, ...) of an individual. Personalization implies that the changes are based on implicit data, such as items purchased or pages viewed. The term ''customization'' is used instead when the site only uses explicit data such as ratings or preferences. 

On an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role.  The term '''customization''' in this context refers to the ability of users to modify the page layout or specify what content should be displayed.

There are three categories of personalization:
# Profile / Group based
# Behaviour based (also known as Wisdom of the Crowds)
# Collaboration based



There are three broad methods of personalization:
# Implicit
# Explicit
# Hybrid

With implicit personalization the personalization is performed by the web page (or information system) based on the different categories mentioned above. It can also be learned from interactions with the user directly.&lt;ref&gt;{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now's Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}&lt;/ref&gt; With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system.
Hybrid personalization combines the above two approaches to leverage the ''best of both worlds''.

Many companies offer services for web recommendation and email recommendation that are based on personalization or anonymously collected user behaviors.&lt;ref name=behaviors&gt;[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html?mod=googlenews_wsj ''Wall Street Journal'', “On the Web's Cutting Edge, Anonymity in Name Only”], August 4, 2010&lt;/ref&gt;  

Web personalization is closely linked to the notion of '''[[Adaptive hypermedia]]''' (AH). The main difference is that the former would usually work on what is considered an Open Corpus Hypermedia, whilst the latter would traditionally work on Closed Corpus Hypermedia. However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.

Personalization is also being considered for use in less overtly commercial applications to improve the user experience online.&lt;ref&gt;[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), ''[[Museums and the Web]] 2004: Selected Papers from an International Conference'', Arlington, Virginia, USA, 31 March – 3 April 2004. Archives &amp; Museum Informatics, pages 63–78, 2004.&lt;/ref&gt; [[Remote control]] manufacturer [[Ruwido]] developed an [[interactive]] [[IPTV]] platform in 2010 called Voco Media, which controls [[digital media]] in the [[living room]] using web personalization. It uses personalization as a tool that supports modern forms of [[TV]] usage, by allowing users to create different profiles for each family member, personalized menu structures and [[fingerprint recognition]].&lt;ref&gt;[http://www.digitaltveurope.net/news_articles/mar_10/23_mar_10/ruwido_wins_virgin_media_contract,_announces_new_voco_apps Ruwido Wins Virgin Media Contract, Announces New Voco App]{{dead link|date=January 2013}}&lt;/ref&gt;

Internet activist [[Eli Pariser]] has documented that search engines like Google and Yahoo News give different results to different people (even when logged out).  He also points out social media site Facebook changes user's friend feeds based on what it thinks they want to see.  Pariser warns that these algorithms can create a &quot;[[filter bubble]]&quot; that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.

==Digital media==
Another aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards.&lt;ref&gt;{{cite news| url=http://www.guardian.co.uk/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}&lt;/ref&gt; Ordnance Survey Open Data This data is structured to allow it to be inter-connected and re-used by third parties.&lt;ref&gt;{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}&lt;/ref&gt;

Data available from a user’s personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].

Current [[open data]] standards on the Web include:
# [[Attention Profiling Mark-up Language]] (APML)
# [[DataPortability]]
# [[OpenID]]
# [[OpenSocial]]

== Mobile phones ==

Over time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.&lt;ref&gt;May, Harvey, and Greg Hearn. &quot;The Mobile Phone as Media.&quot; International Journal of Cultural Studies 8.2 (2005): 195-211. Print.&lt;/ref&gt;

==Print media==
{{main|Mail merge}}

In print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients’ information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipient’s demographics or interests using fields within the database, such as &quot;first name&quot;, &quot;last name&quot;, &quot;company&quot;, etc. 

The term &quot;personalization&quot; should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children's books are created by companies who are using and leveraging all the strengths of [[variable data printing| variable data printing (VDP)]]. This allows for full image and text variability within a printed book.
With the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.

== Promotional merchandise ==
Promotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized children’s storybooks — wherein the child becomes the [[protagonist]], with the name and image of the child personalized — are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained. In addition, with the advent of [[3D printing]], personalised apparel and accessories, such as jewellery made by [[StyleRocks]], is also increasing in popularity.&lt;ref&gt;{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&amp;h=New-jewellery-website-targets-|title=New jewellery website targets 'customisers'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015|quote=StyleRocks founder and CEO, Pascale Helyar-Moray, said the site offers women’s and men’s rings, necklaces, bracelets, earrings and cufflinks. Working alongside an Australian jewellery wholesaler, Helyar-Moray said customers have access to a variety of different styles and designs in an attempt to widen the site’s ability to personalise pieces.}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.ragtrader.com.au/news/style-first|title=Style first|date=15 August 2014|publisher=Ragtrader|language=|accessdate=6 January 2015|quote=Online retailer StyleRocks is about to introduce an Australian first for the jewellery sector. The customisable fine jewellery retailer has introduced 3D printing in conjunction with the launch of a new website.}}&lt;/ref&gt;

== Mass personalization ==

{{tone|section|date=January 2011}}

Mass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.&lt;ref&gt;{{cite web|url=http://www.answers.com/personalization&amp;r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}&lt;/ref&gt; From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers’ individual specific needs with manufacturers’ customization capabilities. &lt;ref&gt;	Chen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167&lt;/ref&gt; The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.&lt;ref&gt;Haag et al., ''Management Information Systems for the Information Age'', 3rd edition, 2006, page 331.&lt;/ref&gt; Clothing industry has also adopted the mass customization paradigm and some footwear retailers are producing mass customized shoes.&lt;ref&gt;{{cite web|url=http://www.botisto.com/how.php?language=EN |title=Botisto |publisher=Botisto |date= |accessdate=2013-01-16}}&lt;/ref&gt;&lt;ref&gt;[http://www.promoline1.com/Custom-T-Shirts-s/1814.htm Clothing ]&lt;/ref&gt; The gaming market is seeing personalization in the new custom controller industry. A new, and notable, company called &quot;Experience Custom&quot; gives customers the opportunity to order personalized gaming controllers.&lt;ref&gt;{{cite web|url=http://www.experiencecustom.com/|title=Custom Controllers |publisher=ExperienceCustom.com |date= |accessdate=2014-11-20}}&lt;/ref&gt; 

A website knowing a user's location, and buying habits, will present offers and suggestions tailored to the user's demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.&lt;ref&gt;{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}&lt;/ref&gt;

[[Behavioral targeting]] represents a concept that is similar to mass personalization.

== Predictive personalization ==

Predictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.&lt;ref&gt;{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}&lt;/ref&gt;  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.

==See also==
* [[Adaptation (computer science)]]
* [[Mass customization]]
* [[Adaptive hypermedia]]
* [[Behavioral targeting]]
* [[Bespoke]]
* [[Collaborative filtering]]
* [[Configurator]]
* [[Personalized learning]]
* [[Preorder economy]]
* [[Real-time marketing]]
* [[Recommendation system]]
* [[User modeling]]

==References==
{{reflist|2}}

==External links==
* [http://www.iimcp.org International Institute on Mass Customization &amp; Personalization which organizes MCP, a biannual conference on customization and personalization]
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] ''The Journal of Personalization Research''

[[Category:Human–computer interaction]]
[[Category:World Wide Web]]
[[Category:User interface techniques]]
[[Category:Usability|Personas]]
[[Category:Types of marketing]]
[[Category:Information retrieval]]</text>
      <sha1>2faez60d2lm0lk03ib4528pe6qrki8m</sha1>
    </revision>
  </page>
  <page>
    <title>Dragomir R. Radev</title>
    <ns>0</ns>
    <id>31253847</id>
    <revision>
      <id>634259027</id>
      <parentid>620425547</parentid>
      <timestamp>2014-11-17T19:11:58Z</timestamp>
      <contributor>
        <username>Rahulkj</username>
        <id>16939479</id>
      </contributor>
      <comment>/* Books */ Added a book by Drago published by Springer.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5341">'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]].  
He is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.

Radev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006–present) and associate editor of [http://www.jair.org JAIR].

== Awards ==
As [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].

== IOL==
Radev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].

== Books ==
* Puzzles in Logic, Languages and Computation (2013) &lt;ref&gt;{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt;
* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']

== Selected Papers ==
* SIGIR 1995 Generating summaries of multiple news articles
* ANLP 1997 Building a generation knowledge source using internet-accessible newswire
* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources
* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities
* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation
* CIKM 2001 Mining the web for answers to natural language questions
* AAAI 2002 Towards CST-enhanced summarization
* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project
* Information Processing and Management 2004 Centroid-based summarization of multiple documents
* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization
* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web
* Communications of the ACM 2005 NewsInEssence: summarizing online news topics
* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing
* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network
* IEEE Intelligent Systems 2008 natural language processing and the web
* NAACL 2009 Generating surveys of scientific paradigms
* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways
* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts
* KDD 2010 Divrank: the interplay of prestige and diversity in information networks
* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs
* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language
* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology

==External links==
* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]
* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]
* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]
* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]
* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]

== References ==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*
*
*
*

{{Persondata &lt;!-- Metadata: see [[Wikipedia:Persondata]]. --&gt;
| NAME              = Radev, Dragomir R.
| ALTERNATIVE NAMES =
| SHORT DESCRIPTION = American computer scientist
| DATE OF BIRTH     =
| PLACE OF BIRTH    =
| DATE OF DEATH     =
| PLACE OF DEATH    =
}}

{{DEFAULTSORT:Radev, Dragomir R.}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]

[[Category:Columbia University alumni]]
[[Category:American computer scientists]]
[[Category:University of Michigan faculty]]
[[Category:Natural language processing]]
[[Category:Information retrieval]]</text>
      <sha1>ptfq19g46belevjnihwz44ya1tkanl1</sha1>
    </revision>
  </page>
  <page>
    <title>Ness Computing</title>
    <ns>0</ns>
    <id>32567205</id>
    <revision>
      <id>601405782</id>
      <parentid>601405514</parentid>
      <timestamp>2014-03-26T20:30:19Z</timestamp>
      <contributor>
        <ip>68.51.79.86</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2034">{{Notability|Companies|date=July 2011}}

'''Ness Computing''' is a personal search company. It was acquired by OpenTable in 2014 and is being shut down in April.&lt;ref&gt;{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}&lt;/ref&gt; 

It was founded in October 2009 by Corey Reese,&lt;ref&gt;http://www.linkedin.com/in/coreyreese&lt;/ref&gt; Paul Twohey,&lt;ref&gt;http://www.linkedin.com/in/twohey&lt;/ref&gt; Nikhil Raghavan,&lt;ref&gt;http://www.linkedin.com/in/nikhilraghavan&lt;/ref&gt; and Steven Schlansker.&lt;ref&gt;http://www.linkedin.com/in/stevenschlansker&lt;/ref&gt; The company is headquartered in Los Altos, California.

The company, whose mission is to make search personal, is sometimes referred to as the &quot;Palantir for fun&quot;. It aims to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. 

Ness' mission is to make search personal. The company refers to its technology as the &quot;Likeness Engine&quot;, a combination of a [[recommendation engine]] that uses [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that serves up results based on these signals. 

The free Ness Dining App (for iPhone) has been referred to as the [[Netflix]] &lt;ref&gt;http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php&lt;/ref&gt; or [[Pandora]] &lt;ref&gt;http://gigaom.com/2011/08/25/ness-restaurant-app/&lt;/ref&gt; for restaurants. Based on a user's ratings and preferences, the service will deliver recommendations for a particular time, location, price range, and cuisine preference. Users may view the menu for a place via SinglePlatform,&lt;ref&gt;http://www.singleplatform.com/&lt;/ref&gt; browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]]. The app is free and available in the [[App Store (iOS)]].

==References==
{{Reflist}}

[[Category:Information retrieval]]
[[Category:Software companies based in California]]</text>
      <sha1>9mzgurcmotd8herhjsfma8gty8twrk9</sha1>
    </revision>
  </page>
  <page>
    <title>Search engine indexing</title>
    <ns>0</ns>
    <id>7602386</id>
    <revision>
      <id>646087691</id>
      <parentid>646023027</parentid>
      <timestamp>2015-02-07T20:58:46Z</timestamp>
      <contributor>
        <ip>72.94.233.245</ip>
      </contributor>
      <comment>/* HTML Priority System */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34101">{{Too many see alsos|date=December 2012}}
'''Search engine indexing''' collects, parses, and stores [[data (computing)|data]] to facilitate fast and accurate [[information retrieval]]. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, [[Information technology|informatics]], and computer science.  An alternate name for the process in the context of [[search engine]]s designed to find web pages on the Internet is ''[[web indexing]]''.

Popular engines focus on the full-text indexing of online, natural language documents.&lt;ref&gt;Clarke, C., Cormack, G.: Dynamic Inverted Indexes for a Distributed Full-Text Retrieval System. TechRep MT-95-01, University of Waterloo, February 1995.&lt;/ref&gt; [[Multimedia|Media types]] such as video and audio&lt;ref&gt;http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf&lt;/ref&gt; and graphics&lt;ref&gt;Charles E. Jacobs, Adam Finkelstein, David H. Salesin. [http://grail.cs.washington.edu/projects/query/mrquery.pdf Fast Multiresolution Image Querying]. Department of Computer Science and Engineering, University of Washington. 1995. Verified Dec 2006&lt;/ref&gt; are also searchable.

[[Metasearch engine|Meta search engines]] reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the  [[text corpus|corpus]]. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while [[Intelligent agent|agent]]-based search engines index in [[Real time business intelligence|real time]].

==Indexing==
The purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would [[Lexical analysis|scan]] every document in the corpus, which would require considerable time and computing power.  For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional computer storage required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval.

===Index design factors===
Major factors in designing a search engine's architecture include:

; Merge factors : How data enters the index, or how words or subject features are added to the index during text corpus traversal, and whether multiple indexers can work asynchronously. The indexer must first check whether it is updating old content or adding new content. Traversal typically correlates to the [[Web crawling|data collection]] policy. Search engine index merging is similar in concept to the [[Merge (SQL)|SQL Merge]] command and other merge algorithms.&lt;ref&gt;Brown, E.W.: Execution Performance Issues in Full-Text Information Retrieval. Computer Science Department, University of Massachusetts Amherst, Technical Report 95-81, October 1995.&lt;/ref&gt;
; Storage techniques : How to store the index [[data]], that is, whether information should be data compressed or filtered.
; Index size : How much computer storage is required to support the index.
; Lookup speed : How quickly a word can be found in the inverted index. The speed of finding an entry in a data structure, compared with how quickly it can be updated or removed, is a central focus of computer science.
; Maintenance : How the index is maintained over time.&lt;ref&gt;Cutting, D., Pedersen, J.: Optimizations for dynamic inverted index maintenance. Proceedings of SIGIR, 405-411, 1990.&lt;/ref&gt;
;Fault tolerance : How important it is for the service to be reliable. Issues include dealing with index corruption, determining whether bad data can be treated in isolation, dealing with bad hardware, [[partition (database)|partitioning]], and schemes such as [[hash function|hash-based]] or composite partitioning,&lt;ref&gt;[http://dev.mysql.com/doc/refman/5.1/en/partitioning-linear-hash.html Linear Hash Partitioning]. MySQL 5.1 Reference Manual. Verified Dec 2006&lt;/ref&gt; as well as [[Replication (computer science)|replication]].

===Index data structures===
Search engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors.

;[[Suffix tree]] : Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [[trie]]. Tries support extendable hashing, which is important for search engine indexing.&lt;ref&gt;[http://www.nist.gov/dads/HTML/trie.html trie], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology].&lt;/ref&gt; Used for searching for patterns in [[DNA]] sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.&lt;ref name=&quot;Gus97&quot;&gt;{{cite book
 | last = Gusfield
 | first = Dan
 | origyear = 1997
 | year = 1999
 | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology
 | publisher = Cambridge University Press
 | location = USA
 | isbn = 0-521-58519-8}}.
&lt;/ref&gt; An alternate representation is a [[suffix array]], which is considered to require less virtual memory and supports data compression such as the [[Burrows-Wheeler transform|BWT]] algorithm.

;[[Inverted index]] : Stores a list of occurrences of each atomic search criterion,&lt;ref&gt;Black, Paul E., [http://www.nist.gov/dads/HTML/invertedIndex.html inverted index], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology] Oct 2006. Verified Dec 2006.&lt;/ref&gt; typically in the form of a [[hash table]] or [[binary tree]].&lt;ref&gt;C. C. Foster, Information retrieval: information storage and retrieval using AVL trees, Proceedings of the 1965 20th national conference, p.192-205, August 24–26, 1965, Cleveland, Ohio, United States&lt;/ref&gt;&lt;ref&gt;Landauer, W. I.: The balanced tree and its utilization in information retrieval. IEEE Trans. on Electronic Computers, Vol. EC-12, No. 6, December 1963.&lt;/ref&gt;

;[[Citation index]] : Stores citations or hyperlinks between documents to support citation analysis, a subject of [[Bibliometrics]].
;[[N-gram|Ngram index]] : Stores sequences of length of data to support other types of retrieval or text mining.&lt;ref&gt;[http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13 Google Ngram Datasets] for sale at [http://www.ldc.upenn.edu/ LDC] Catalog&lt;/ref&gt;
;[[Document-term matrix]] : Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [[sparse matrix]].

===Challenges in parallelism===
A major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for [[race conditions]] and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a web crawler is the consumer of this information, grabbing the text and storing it in a cache (or [[Text corpus|corpus]]). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a '''producer-consumer model'''. The indexer is the producer of searchable information and users are the consumers that need to search.  The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engine's architecture may involve [[distributed computing]], where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture.&lt;ref&gt;Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. Google, Inc. OSDI. 2004.&lt;/ref&gt;

===Inverted indices===
Many search engines incorporate an [[inverted index]] when evaluating a [[search query]] to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct [[random access|access]] to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index:

{| align=&quot;center&quot; class=&quot;wikitable&quot;
|+ Inverted Index
|-
! Word !! Documents
|-
| the || Document 1, Document 3, Document 4, Document 5, Document 7
|-
| cow || Document 2, Document 3, Document 4
|-
| says || Document 5
|-
| moo || Document 7
|}

This index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a [[boolean datatype|boolean]] index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document.&lt;ref&gt;Grossman, Frieder, Goharian. [http://www.cs.clemson.edu/~juan/CPSC862/Concept-50/IR-Basics-of-Inverted-Index.pdf IR Basics of Inverted Index]. 2002. Verified Aug 2011.&lt;/ref&gt; Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of [[information retrieval]].

The inverted index is a [[sparse matrix]], since not all words are present in each document. To reduce computer storage memory requirements, it is stored differently from a two dimensional [[Array data structure|array]]. The index is similar to the [[document-term matrix|term document matrices]] employed by [[latent semantic analysis]]. The inverted index can be considered a form of a hash table. In some cases the index is a form of a [[binary tree]], which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a [[distributed hash table]].&lt;ref&gt;Tang, Hunqiang. [[Sandhya Dwarkadas|Dwarkadas, Sandhya]]. &quot;Hybrid Global Local Indexing for Efficient
Peer to Peer Information Retrieval&quot;. University of Rochester. Pg 1. http://www.cs.rochester.edu/u/sandhya/papers/nsdi04.ps&lt;/ref&gt;

===Index merging===
The inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing,&lt;ref&gt;Tomasic, A., et al.: Incremental Updates of Inverted Lists for Text Document Retrieval. Short Version of Stanford University Computer Science Technical Note STAN-CS-TN-93-1, December, 1993.&lt;/ref&gt; where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives.

After parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is  so named because it is an inversion of the forward index.

===The forward index===
The forward index stores a list of words for each document. The following is a simplified form of the forward index:

{| align=&quot;center&quot; class=&quot;wikitable&quot;
|+ Forward Index
|-
! Document !! Words
|-
| Document 1 || the,cow,says,moo
|-
| Document 2 || the,cat,and,the,hat
|-
| Document 3 || the,dish,ran,away,with,the,spoon
|}

The rationale behind developing a forward index is that as documents are parsing, it is better to immediately store the words per document.  The delineation enables Asynchronous system processing, which partially circumvents the inverted index update [[wikt:bottleneck|bottleneck]].&lt;ref&gt;Sergey Brin and Lawrence Page. [http://infolab.stanford.edu/~backrub/google.html The Anatomy of a Large-Scale Hypertextual Web Search Engine]. [[Stanford University]]. 1998. Verified Dec 2006.&lt;/ref&gt; The forward index is [[Sorting algorithm|sorted]] to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index.

===Compression===
Generating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of compression to reduce the size of the indices on [[computer storage|disk]].&lt;ref&gt;H.S. Heaps. Storage analysis of a compression coding for a document database. 1NFOR, I0(i):47-61, February 1972.&lt;/ref&gt; Consider the following scenario for a full text, Internet search engine.

* It takes 8 bits (or 1 [[byte]]) to store a single character. Some [[character encoding|encodings]] use 2 bytes per character&lt;ref&gt;[http://www.unicode.org/faq/basic_q.html#15 The Unicode Standard - Frequently Asked Questions]. Verified Dec 2006.&lt;/ref&gt;&lt;ref&gt;[http://www.uplink.freeuk.com/data.html Storage estimates]. Verified Dec 2006.&lt;/ref&gt;
* The average number of characters in any given word on a page may be estimated at 5 ([[Wikipedia:Size comparisons]])

Given this scenario, an uncompressed index (assuming a non-[[conflation|conflated]], simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression.

Notably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost.

==Document parsing==
Document parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called ''tokens'', and so, in the context of search engine indexing and [[natural language processing]], parsing is more commonly referred to as [[Tokenization (lexical analysis)|tokenization]]. It is also sometimes called [[word boundary disambiguation]], [[Part-of-speech tagging|tagging]], [[text segmentation]], [[content analysis]], text analysis, [[text mining]], [[Agreement (linguistics)|concordance]] generation, [[speech segmentation]], [[Lexical analysis|lexing]], or [[lexical analysis]]. The terms 'indexing', 'parsing', and 'tokenization' are used interchangeably in corporate slang.

Natural language processing, as of 2006, is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets.

=== Challenges in natural language processing ===
; Word Boundary Ambiguity : Native [[English language|English]] speakers may at first consider tokenization to be a straightforward task, but this is not the case with designing a [[multilingual]] indexer.  In digital form, the texts of other languages such as [[Chinese language|Chinese]], [[Japanese language|Japanese]] or [[Arabic language|Arabic]] represent a greater challenge, as words are not clearly delineated by [[Whitespace (computer science)|whitespace]]. The goal during tokenization is to identify words for which users will search. Language-specific logic is employed to properly identify the boundaries of words, which is often the rationale for designing a parser for each language supported (or for groups of languages with similar boundary markers and syntax).

; Language Ambiguity : To assist with properly ranking matching documents, many search engines collect additional information about each word, such as its [[language]] or [[lexical category]] ([[part of speech]]). These techniques are language-dependent, as the syntax varies among languages. Documents do not always clearly identify the language of the document or represent it accurately. In tokenizing the document, some search engines attempt to automatically identify the language of the document.

; Diverse File Formats : In order to correctly identify which bytes of a document represent characters, the file format must be correctly handled. Search engines which support multiple file formats must be able to correctly open and access the document and be able to tokenize the characters of the document.

; Faulty Storage : The quality of the natural language data may not always be perfect.  An unspecified number of documents, particular on the Internet, do not closely obey proper file protocol.  [[Binary data|Binary]] characters may be mistakenly encoded into various parts of a document. Without recognition of these characters and appropriate handling, the index quality or indexer performance could degrade.

=== Tokenization ===
Unlike [[literacy|literate]] humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not 'know' that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word, referred to as a token. Such a program is commonly called a [[tokenizer]] or [[parser]] or [[Lexical analysis|lexer]]. Many search engines, as well as other natural language processing software, incorporate [[Comparison of parser generators|specialized programs]] for parsing, such as [[YACC]] or [[Lex programming tool|Lex]].

During tokenization, the parser identifies sequences of characters which represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify [[Entity extraction|entities]] such as [[email]] addresses, phone numbers, and [[Uniform Resource Locator|URL]]s. When identifying each token, several characteristics may be stored, such as the token's case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like 'noun' or 'verb'), position, sentence number, sentence position, length, and line number.

=== Language recognition ===
If the search engine supports multiple languages, a common initial step during tokenization is to identify each document's language; many of the subsequent steps are language dependent (such as [[stemming]] and [[part of speech]] tagging). [[Language identification|Language recognition]] is the process by which a computer program attempts to automatically identify, or categorize, the [[language]] of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in [[natural language processing]]. Finding which language the words belongs to may involve the use of a [[language recognition chart]].

=== Format analysis ===
If the search engine supports multiple [[File format|document formats]], documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content.  For example, [[HTML]] documents contain HTML tags, which specify formatting information such as new line starts, '''bold''' emphasis, and [[font]] size or [[Font family|style]].  If the search engine were to ignore the difference between content and 'markup', extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning, and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include:

* [[HTML]]
* [[ASCII]] text files (a text document without specific computer readable formatting)
* [[Adobe Systems|Adobe]]'s Portable Document Format ([[PDF]])
* [[PostScript]] (PS)
* [[LaTeX]]
* [[UseNet]] netnews server formats
* [[XML]] and derivatives like [[RSS]]
* [[SGML]]
* [[Multimedia]] [[meta data]] formats like [[ID3]]
* [[Microsoft Word]]
* [[Microsoft Excel]]
* [[Microsoft PowerPoint]]
* IBM [[Lotus Notes]]
Options for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom [[parser]].

Some search engines support inspection of files that are stored in a [[Compressor (software)|compressed]] or encrypted file format.  When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported [[list of archive formats|compressed file format]]s include:

* [[ZIP (file format)|ZIP]] - Zip archive file
* [[RAR]] - Roshal ARchive file
* [[Cabinet (file format)|CAB]] - [[Microsoft Windows]] Cabinet File
* [[Gzip]] - File compressed with gzip
* [[Bzip2|BZIP]] - File compressed using bzip2
* [[Tar (file format)|Tape ARchive (TAR)]], [[Unix]] archive file, not (itself) compressed
* TAR.Z, TAR.GZ or TAR.BZ2 - [[Unix]] archive files compressed with Compress, GZIP or BZIP2

Format analysis can involve quality improvement methods to avoid including 'bad information' in the index.  Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for [[spamdexing]]:

* Including hundreds or thousands of words in a section which is hidden from view on the computer screen, but visible to the indexer, by use of formatting (e.g. hidden [[Span and div|&quot;div&quot; tag]] in [[HTML]], which may incorporate the use of [[CSS]] or [[JavaScript]] to do so).
* Setting the foreground font color of words to the same as the background color, making words hidden on the computer screen to a person viewing the document, but not hidden to the indexer.

=== Section recognition ===
Some search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages.  Many documents on the [[Internet|web]], such as newsletters and corporate reports, contain erroneous content and side-sections which do not contain primary material (that which the document is about). For example, this article displays a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted:

* Content in different sections is treated as related in the index, when in reality it is not
* Organizational 'side bar' content is included in the index, but the side bar content does not contribute to the meaning of the document, and the index is filled with a poor representation of its documents.

Section analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not 'see' this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the [[Noscript tag]] to ensure that the web page is indexed properly.  At the same time, this fact can also be [[spamdexing|exploited]] to cause the search engine indexer to 'see' different content than the viewer.

=== HTML Priority System ===
{{Section OR|date=November 2013}}
Indexing often has to recognize the [[HTML]] tags to organize priority. Indexing low priority to high margin to labels like ''strong'' and ''link'' to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like [[Google]] and [[Bing]] ensure that the [[search engine]] does not take the large texts as relevant source due to[[ strong type system]] compatibility.&lt;ref&gt;Google Webmaster Tools, &quot;Hypertext Markup Language 5&quot;, Conference for SEO January 2012.&lt;/ref&gt;

=== Meta tag indexing ===
Specific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the [[meta tag]] contains keywords which are also included in the index. Earlier Internet [[search engine technology]] would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was [[computer hardware]] able to support such technology.  The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization.&lt;ref&gt;Berners-Lee, T., &quot;Hypertext Markup Language - 2.0&quot;, RFC 1866, Network Working Group, November 1995.&lt;/ref&gt;

As the Internet grew through the 1990s, many [[brick and mortar business|brick-and-mortar corporations]] went 'online' and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to [[spamdexing]], which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many 'marketing keywords' into the content of a webpage before draining it of all interesting and useful information.  Given that conflict of interest with the business goal of designing user-oriented websites which were 'sticky', the [[customer lifetime value]] equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies.

In [[Desktop search]], many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index.

== See also ==
{{div col|colwidth=25em}}
* [[Compound term processing]]
* [[Concordance (publishing)|Concordance]]
* [[Content analysis]]
* [[Controlled vocabulary]]
* [[Desktop search]]
* [[Documentation]]
* [[Document retrieval|Document Retrieval]]
* [[Full text search]]
* [[Index (database)]]
* [[Information extraction]]
* [[Information retrieval]]
* [[Key Word in Context|Keyword In Context Indexing]]
* [[Latent semantic indexing]]
* [[List of search engines]]
* [[Natural language processing]]
* [[Search engine]]
* [[Selection-based search]]
* [[Semantic Web]]
* [[Site map]]
* [[Text mining]]
* [[Text retrieval|Text Retrieval]]
* [[Vertical search]]
* [[Web crawler]]
* [[Web indexing]]
* [[Website Parse Template]]
*[[Windows indexing service]]&lt;ref&gt;Krishna Nareddy. [http://msdn2.microsoft.com/en-us/library/ms951558.aspx Indexing with Microsoft Index Server]. MSDN Library. Microsoft Corporation. January 30, 1998. Verified Dec 2006. Note that this is a commercial, external link.&lt;/ref&gt;
{{div col end}}

== References ==
&lt;references/&gt;

==Further reading==
*R. Bayer and E. McCreight. Organization and maintenance of large ordered indices. Acta Informatica, 173-189, 1972.
*[[Donald E. Knuth]]. The art of computer programming, volume 1 (3rd ed.): fundamental algorithms, Addison Wesley Longman Publishing Co. Redwood City, CA, 1997.
*[[Donald E. Knuth]]. The art of computer programming, volume 3: (2nd ed.) sorting and searching, Addison Wesley Longman Publishing Co. Redwood City, CA, 1998.
*[[Gerald Salton]]. Automatic text processing, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1988.
*[[Gerard Salton]]. Michael J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986.
*[[Gerard Salton]]. Lesk, M.E.: Computer evaluation of indexing and text processing. Journal of the ACM. January 1968.
*[[Gerard Salton]]. The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, 1971.
*[[Gerard Salton]]. The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, Mass., 1989.
*Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. Chapter 8. ACM Press 1999.
*G. K. Zipf. Human Behavior and the Principle of Least Effort. Addison-Wesley, 1949.
*Adelson-Velskii, G.M., Landis, E. M.: An information organization algorithm. DANSSSR, 146, 263-266 (1962).
*[[Edward H. Sussenguth Jr.]], Use of tree structures for processing files, Communications of the ACM, v.6 n.5, p.&amp;nbsp;272-279, May 1963
*Harman, D.K., et al.: Inverted files. In Information Retrieval: Data Structures and Algorithms, Prentice-Hall, pp 28–43, 1992.
*Lim, L., et al.: Characterizing Web Document Change, LNCS 2118, 133–146, 2001.
*Lim, L., et al.: Dynamic Maintenance of Web Indexes Using Landmarks. Proc. of the 12th W3 Conference, 2003.
*Moffat, A., Zobel, J.: Self-Indexing Inverted Files for Fast Text Retrieval. ACM TIS, 349–379, October 1996, Volume 14, Number 4.
*[[Kurt Mehlhorn|Mehlhorn, K.]]: Data Structures and Efficient Algorithms, Springer Verlag, EATCS Monographs, 1984.
*[[Kurt Mehlhorn|Mehlhorn, K.]], [[Mark Overmars|Overmars, M.H.]]: Optimal Dynamization of Decomposable Searching Problems. IPL 12, 93–98, 1981.
*[[Kurt Mehlhorn|Mehlhorn, K.]]: Lower Bounds on the Efficiency of Transforming Static Data Structures into Dynamic Data Structures. Math. Systems Theory 15, 1–16, 1981.
*Koster, M.: ALIWEB: Archie-Like indexing in the Web. Computer Networks and ISDN Systems, Vol. 27, No. 2 (1994) 175-182 (also see Proc. First Int'l World Wide Web Conf., Elsevier Science, Amsterdam, 1994, pp.&amp;nbsp;175–182)
*[[Serge Abiteboul]] and [[Victor Vianu]]. [http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&amp;doc=1996-20&amp;format=text&amp;compression=&amp;name=1996-20.text Queries and Computation on the Web]. Proceedings of the International Conference on Database Theory. Delphi, Greece 1997.
*Ian H Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. New York: Van Nostrand Reinhold, 1994.
*A. Emtage and P. Deutsch, &quot;Archie--An Electronic Directory Service for the Internet.&quot; Proc. Usenix Winter 1992 Tech. Conf., Usenix Assoc., Berkeley, Calif., 1992, pp.&amp;nbsp;93–110.
*M. Gray, [http://www.mit.edu/people/mkgray/net/ World Wide Web Wanderer].
*D. Cutting and J. Pedersen. &quot;Optimizations for Dynamic Inverted Index Maintenance.&quot; Proceedings of the 13th International Conference on Research and Development in Information Retrieval, pp.&amp;nbsp;405–411, September 1990.
*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

{{Internet search}}

{{DEFAULTSORT:Index (Search Engine)}}
[[Category:Information retrieval]]
[[Category:Searching]]
[[Category:Indexing]]
[[Category:Internet search algorithms]]</text>
      <sha1>dfzxa30vm2y7m31fzx5lfw6xazbeucv</sha1>
    </revision>
  </page>
  <page>
    <title>Multimedia Information Retrieval</title>
    <ns>0</ns>
    <id>33407925</id>
    <revision>
      <id>616498155</id>
      <parentid>616496670</parentid>
      <timestamp>2014-07-11T08:42:06Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{COI}} {{FV}} {{Original research}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6765">{{COI|date=July 2014}}
{{Original research|date=July 2014}}
{{Use dmy dates|date=February 2012}}
'''Multimedia Information Retrieval''' (MMIR or MIR) is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.&lt;ref&gt;H Eidenberger. &quot; Fundamental Media Understanding &quot;, atpress, 2011, p. 1.&lt;/ref&gt;{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:

# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.
# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])
# Methods for the [[categorization]] of media descriptions into classes.

== Feature Extraction Methods ==

Feature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.&lt;ref&gt;H Eidenberger. &quot; Fundamental Media Understanding &quot;, atpress, 2011, p. 2.&lt;/ref&gt;{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:

* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel Frequency Cepstral Coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms&lt;ref&gt;A Del Bimbo. &quot; Visual Information Retrieval &quot;, Morgan Kaufmann, 1999.&lt;/ref&gt; such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.
* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,&lt;ref&gt;HG Kim , N Moreau, T Sikora. &quot; MPEG-7 Audio and Beyond&quot;, Wiley, 2005.&lt;/ref&gt; texture description in the visual domain and n-grams in text information retrieval.

== Merging and Filtering Methods ==

Multimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.&lt;ref&gt;MS Lew (Ed.). &quot; Principles of Visual Information Retrieval &quot;, Springer, 2001.&lt;/ref&gt; Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions - as they frequently occur in motion description - have to be normalized to a fixed length first.

Frequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.

== Categorization Methods ==

Generally, all forms of machine learning can be employed for the categorization of multimedia descriptions&lt;ref&gt;H Eidenberger. &quot; Fundamental Media Understanding &quot;, atpress, 2011,p. 125.&lt;/ref&gt;{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[Hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[Dynamic Time Warping]] - a semantically related method - is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:

* Metric approaches ([[Cluster Analysis]], [[Vector Space Model]], [[Minkowski]] Distances, Dynamic Alignment)
* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-Means, [[Self-Organizing Map]])
* Risk Minimization (Support Vector Regression, [[Support Vector Machine]], [[Linear Discriminant Analysis]])
* Density-based Methods (Bayes Nets, [[Markov Processes]], Mixture Models)
* Neural Networks ([[Perceptron]], Associative Memories, Spiking Nets)
* Heuristics ([[Decision Trees]], Random Forests, etc.)

The selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.

== Open Problems ==

The quality of MMIR Systems&lt;ref&gt;JC Nordbotten. &quot;[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]&quot;. Retrieved 14 October 2011.&lt;/ref&gt; depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.&lt;ref&gt;H Eidenberger. &quot; Frontiers of Media Understanding &quot;, atpress, 2012.&lt;/ref&gt; The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.

== Related Areas ==

MMIR provides an overview over methods employed in the areas of information retrieval.&lt;ref&gt;H Eidenberger. &quot; Professional Media Understanding &quot;, atpress, 2012.&lt;/ref&gt; Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:

* [[Bioinformatics|Bioinformation Analysis]]
* [[Biosignal|Biosignal Processing]]
* [[Content-based image retrieval|Content-based Image and Video Retrieval]]
* [[Facial recognition system|Face Recognition]]
* [[Music information retrieval|Audio and Music Classification]]
* [[Speech Recognition]]
* [[Technical analysis|Technical Chart Analysis]]
* [[Information retrieval|Text Information Retrieval]]

The Journal of Multimedia Information Retrieval&lt;ref&gt;&quot;[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]&quot;, Springer, 2011, Retrieved 21 October 2011.&lt;/ref&gt; documents the development of MMIR as a research discipline that is independent of these areas. See also &lt;ref&gt;H Eidenberger. &quot; Handbook of Multimedia Information Retrieval &quot;, atpress, 2012.&lt;/ref&gt; for a complete overview over this research discipline.

==References==
{{reflist}}

&lt;!--

&lt;ref&gt;AUTH. &quot;TITLE&quot;, PUB, YEAR.&lt;/ref&gt;
&lt;ref&gt;AUTH. &quot;[LINK TITLE]&quot;, MEDIA, PRODDATE. Retrieved UPDATE.&lt;/ref&gt;

--&gt;



[[Category:Information retrieval]]</text>
      <sha1>c1teqyuqmkpto336eejhvamtzzm3opf</sha1>
    </revision>
  </page>
  <page>
    <title>Automatic Content Extraction</title>
    <ns>0</ns>
    <id>33675011</id>
    <revision>
      <id>643628098</id>
      <parentid>621104303</parentid>
      <timestamp>2015-01-22T05:39:35Z</timestamp>
      <contributor>
        <username>Niceguyedc</username>
        <id>5288432</id>
      </contributor>
      <minor/>
      <comment>[[:en:WP:CLEANER|WPCleaner]] v1.34 - Repaired 1 link to disambiguation page - [[WP:DPL|(You can help)]] - [[Transduction]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2915">{{Multiple issues|
{{citation style|date=December 2011}}
{{technical|date=October 2012}}
{{abbreviations|date=October 2012}}
}}
'''Automatic Content Extraction (ACE)''' is a program for developing advanced [[Information extraction]] [[technologies]]. Given a text in [[natural language]], the ACE challenge is to detect:
# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.
# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.
# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.

This program began with a [[pilot study]] in 1999.

While the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of ASR and OCR output from such transducers is.

The program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.

The effort involves:
* defining the research tasks in detail,
* collecting and annotating data needed for training, development, and evaluation,
* supporting the research with evaluation tools and [[research workshop]]s.

In general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called “named entity” task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an
answer. In a real sense, the task is to detect things that “aren’t there”.

The ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.

==References==
* [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. The automatic content extraction (ACE) program–tasks, data, and evaluation. 2004]

==External links==
* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.
* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)
* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)

[[Category:Information retrieval]]</text>
      <sha1>go8vnzbbniqtkpdfin9mn77ew8v1q1z</sha1>
    </revision>
  </page>
  <page>
    <title>Type-1 OWA operators</title>
    <ns>0</ns>
    <id>33591382</id>
    <revision>
      <id>618358701</id>
      <parentid>608556154</parentid>
      <timestamp>2014-07-25T02:03:52Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Special cases of Type-1 OWA operators */Task 5: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10634">The [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]&lt;ref name=&quot;yagerOWA&quot;&gt;{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183–190|doi=10.1109/21.87068}}&lt;/ref&gt;  have been widely used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decisin making, multi-criteria multi-expert decision making).&lt;ref&gt;{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}&lt;/ref&gt; It is widely accepted that fuzzy sets&lt;ref&gt;{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338–353|doi=10.1016/S0019-9958(65)90241-X}}&lt;/ref&gt; are more suitable for representing preferences of criteria in decision making. But fuzzy sets are not crisp values, how can we aggregate fuzzy sets in OWA mechanism? 

The type-1 OWA operators&lt;ref name=&quot;fssT1OWA&quot;&gt;{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281–3296|doi=10.1016/j.fss.2008.06.018}}&lt;/ref&gt;&lt;ref name=&quot;kdeT1OWA&quot;&gt;{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455–1468|doi=10.1109/TKDE.2010.191}}&lt;/ref&gt;  have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.

First, there are two definitions for type-1 OWA operators, one is based on Zadeh's Extension Principle, the other is based on &lt;math&gt;\alpha&lt;/math&gt;-cuts of fuzzy sets. The two definitions lead to equivalent results.

==Definitions==

'''Definition 1.&lt;ref name=&quot;fssT1OWA&quot; /&gt; '''
Let &lt;math&gt;F(X)&lt;/math&gt; be the set of fuzzy sets with domain of discourse &lt;math&gt;X&lt;/math&gt;, a type-1 OWA operator is defined as follows:

Given n linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i = 1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,1]&lt;/math&gt;, a type-1 OWA operator is a mapping, &lt;math&gt;\Phi&lt;/math&gt;,

:&lt;math&gt;\Phi \colon F(X)\times \cdots \times F(X)  \longrightarrow  F(X)&lt;/math&gt;
:&lt;math&gt;(A^1 , \cdots ,A^n)  \mapsto   Y&lt;/math&gt;

such that

:&lt;math&gt;\mu _{Y} (y) =\displaystyle \sup_{\displaystyle \sum_{k =1}^n \bar {w}_i a_{\sigma (i)}  = y }\left({\begin{array}{*{1}l}\mu _{W^1 } (w_1 )\wedge \cdots \wedge \mu_{W^n } (w_n )\wedge \mu _{A^1 } (a_1 )\wedge \cdots \wedge \mu _{A^n } (a_n )\end{array}}\right)&lt;/math&gt;

where &lt;math&gt;\bar {w}_i = \frac{w_i }{\sum_{i = 1}^n {w_i } }&lt;/math&gt;,and &lt;math&gt;\sigma \colon \{1, \cdots ,n\} \longrightarrow \{1, \cdots ,n\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \geq a_{\sigma (i + 1)},\ \forall i = 1, \cdots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma(i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th highest element in the set &lt;math&gt;\left\{ {a_1 , \cdots ,a_n } \right\}&lt;/math&gt;.

'''Definition 2.&lt;ref name=&quot;kdeT1OWA&quot; /&gt; '''

The definition below is based on the alpha-cuts of fuzzy sets:

Given the n linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, then for each &lt;math&gt;\alpha \in [0,\;1]&lt;/math&gt;, an &lt;math&gt;\alpha &lt;/math&gt;-level type-1 OWA operator with &lt;math&gt;\alpha &lt;/math&gt;-level sets &lt;math&gt;\left\{ {W_\alpha ^i } \right\}_{i = 1}^n &lt;/math&gt; to aggregate the &lt;math&gt;\alpha &lt;/math&gt;-cuts of fuzzy sets &lt;math&gt;\left\{ {A^i} \right\}_{i =1}^n &lt;/math&gt; is given as

: &lt;math&gt;
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}&lt;/math&gt;

where  &lt;math&gt;W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}&lt;/math&gt;, and &lt;math&gt;\sigma :\{\;1, \cdots ,n\;\} \to \{\;1, \cdots ,n\;\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \cdots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma (i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th largest
element in the set &lt;math&gt;\left\{ {a_1 , \cdots ,a_n } \right\}&lt;/math&gt;.

== Representation theorem of Type-1 OWA operators&lt;ref name=&quot;kdeT1OWA&quot; /&gt;==

Given the ''n'' linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, and the fuzzy sets &lt;math&gt;A^1, \cdots ,A^n&lt;/math&gt;, then we have that&lt;ref name=&quot;kdeT1OWA&quot; /&gt;
:&lt;math&gt;Y=G&lt;/math&gt;

where &lt;math&gt;Y&lt;/math&gt; is the aggregation result obtained by Definition 1, and &lt;math&gt;G&lt;/math&gt; is the result obtained by in Definition 2.

==Programming problems for Type-1 OWA operators==

According to the '''''Representation Theorem of Type-1 OWA Operators''''',a general type-1 OWA operator can be decomposed into a series of &lt;math&gt;\alpha&lt;/math&gt;-level type-1 OWA operators. In practice, these series of  &lt;math&gt;\alpha&lt;/math&gt;-level type-1 OWA operators are used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals &lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)&lt;/math&gt;. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:

:&lt;math&gt;\mu _{G} (x) = \mathop \vee \limits_{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha &lt;/math&gt;

For the left end-points, we need to solve the following programming problem:
:&lt;math&gt; \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \mathop {\min }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } &lt;/math&gt;

while for the right end-points, we need to solve the following programming problem:
:&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \mathop {\max }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } &lt;/math&gt;

A fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.&lt;ref name=&quot;kdeT1OWA&quot; /&gt;

== Alpha-level approach to Type-1 OWA operation&lt;ref name=&quot;kdeT1OWA&quot; /&gt;==
* '''Step 1'''.To set up the &lt;math&gt;\alpha &lt;/math&gt;- level resolution in [0, 1].
* '''Step 2'''. For each &lt;math&gt;\alpha \in [0,1]&lt;/math&gt;,
''Step 2.1.'' To calculate &lt;math&gt;\rho _{\alpha +} ^{i_0^\ast } &lt;/math&gt;
# Let &lt;math&gt;i_0 = 1&lt;/math&gt;;
# If &lt;math&gt;\rho _{\alpha +} ^{i_0 } \ge A_{\alpha + }^{\sigma (i_0 )} &lt;/math&gt;, stop, &lt;math&gt;\rho _{\alpha +} ^{i_0 } &lt;/math&gt; is the solution; otherwise go to ''Step 2.1-3''.
# &lt;math&gt;i_0 \leftarrow i_0 + 1&lt;/math&gt;, go to ''Step 2.1-2''.

''Step 2.2.'' To calculate&lt;math&gt;\rho _{\alpha -} ^{i_0^\ast } &lt;/math&gt;
# Let &lt;math&gt;i_0 = 1&lt;/math&gt;;
# If &lt;math&gt;\rho _{\alpha -} ^{i_0 } \ge A_{\alpha - }^{\sigma (i_0 )} &lt;/math&gt;, stop, &lt;math&gt;\rho _{\alpha -} ^{i_0 } &lt;/math&gt; is the solution; otherwise go to ''Step 2.2-3.''
#&lt;math&gt;i_0 \leftarrow i_0 + 1&lt;/math&gt;, go to step ''Step 2.2-2.''

'''Step 3.'''To construct the aggregation resulting fuzzy set &lt;math&gt;G&lt;/math&gt; based on all the available intervals &lt;math&gt;\left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]&lt;/math&gt;: 

:&lt;math&gt;\mu _{G} (x) = \mathop \vee \limits_{\alpha :x \in \left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]} \alpha &lt;/math&gt;

==Special cases of Type-1 OWA operators==
* Any OWA operators, like maximum, minimum, mean operators;&lt;ref name=&quot;yagerOWA&quot; /&gt;
* Join operators of (type-1) fuzzy sets,&lt;ref name=&quot;MT&quot;&gt;{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312–40|doi=10.1016/s0019-9958(76)80011-3}}&lt;/ref&gt;&lt;ref name=&quot;zadehJ&quot;&gt;{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199–249|doi=10.1016/0020-0255(75)90036-5}}&lt;/ref&gt; i.e., fuzzy maximum operators;
* Meet operators of (type-1) fuzzy sets,&lt;ref name=&quot;MT&quot;/&gt;&lt;ref name=&quot;zadehJ&quot;/&gt; i.e., fuzzy minimum operators;
* Join-like operators of (type-1) fuzzy sets;&lt;ref name=&quot;kdeT1OWA&quot;/&gt;&lt;ref name=&quot;bookT1OWA&quot;&gt;{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91–109|doi=10.1007/978-3-642-17910-5_5}}&lt;/ref&gt;
* Meet-like operators of (type-1) fuzzy sets.&lt;ref name=&quot;kdeT1OWA&quot;/&gt;&lt;ref name=&quot;bookT1OWA&quot;/&gt;

==Generalizations==
Type-2 OWA operators&lt;ref&gt;{{cite journal|last=Zhou|first=S.M.|coauthors=R. I. John, F. Chiclana and J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540–558|doi=10.1002/int.20420}}&lt;/ref&gt; have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.

== References ==
{{reflist}}

[[Category:Artificial intelligence]]
[[Category:Logic in computer science]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval]]</text>
      <sha1>2a9eojcttj0opx9y1ds6l4afpbjxl1a</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Substring indices</title>
    <ns>14</ns>
    <id>33958933</id>
    <revision>
      <id>548118218</id>
      <parentid>466150242</parentid>
      <timestamp>2013-04-01T09:00:23Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8823550]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="252">{{cat main|Substring index}}

[[Category:String (computer science)]]
[[Category:Algorithms on strings]]
[[Category:String data structures]]
[[Category:Database index techniques]]
[[Category:Information retrieval]]
[[Category:Bioinformatics algorithms]]</text>
      <sha1>ju0whibzc35vtpk07rjozptqaietxtg</sha1>
    </revision>
  </page>
  <page>
    <title>Macroglossa Visual Search</title>
    <ns>0</ns>
    <id>33558264</id>
    <revision>
      <id>602201787</id>
      <parentid>597606618</parentid>
      <timestamp>2014-04-01T01:47:29Z</timestamp>
      <contributor>
        <username>OKBot</username>
        <id>4559949</id>
      </contributor>
      <minor/>
      <comment>Bot: Updating Alexa ranking ([[User talk:OsamaK/AlexaBot.js|Help get more pages covered]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6331">{{Infobox Website
| name           = Macroglossa
| logo           = [[File:Macroglossa Visual Search Engine Logo, 2012.gif]]
| screenshot     = 
| caption        = Macroglossa logo
| url            = [http://www.macroglossa.com macroglossa.com]
| type           = [[Visual search engine|Visual]] [[Search engine|Search Engine]]
| language       = English
| registration   = optional
| author         = MVE
| launch date    = 2010
| current status = beta 0.1
| slogan         = search is visual
| alexa          = {{IncreaseNegative}} 2,828,096 ({{as of|2014|4|1|alt=April 2014}})&lt;ref name=&quot;alexa&quot;&gt;{{cite web|url= http://www.alexa.com/siteinfo/macroglossa.com |title= Macroglossa.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}&lt;/ref&gt;&lt;!--Updated monthly by OKBot.--&gt;
}}
'''Macroglossa''' is a [[visual search engine]] based on the comparison of images,&lt;ref&gt;Nicola Mattina. &quot;[http://blog.wired.it/startupcloud/2010/12/29/macroglossa-usare-le-immagini-per-effettuare-ricerche-sul-web.html Macroglossa: usare le immagini per effettuare ricerche sul web]&quot;, Wired.it, Retrieved December 29, 2010.&lt;/ref&gt;&lt;ref&gt;GreatStartups.com . &quot;[http://greatstartups.com/2010/10/13/macroglossa-com-whats-in-the-picture/ Macroglossa.com-What’s In The Picture ]&quot;, greatstartups.com, Retrieved October 13, 2010.&lt;/ref&gt; coming from an Italian Group. The development of the project began in 2009. In April 2010 is released the first public [[Alpha stage#Alpha|alpha]].&lt;ref&gt;Liva Judic. &quot;[http://searchenginewatch.com/article/2050950/Macroglossas-Visual-Search-Engine-fails-to-meet-basic-expectations Macroglossa's Visual Search Engine fails to meet basic expectations ]&quot;, SEW - searchenginewatch, Retrieved April 26, 2010.&lt;/ref&gt;
Users can upload photos or images that they aren't sure what they are to determine what the images contain. Macroglossa compares images to return search results based on specific search categories. 
The engine does not use technologies and solutions such as [[Optical character recognition|OCR]], [[Tag (metadata)|tags]], vocabulary trees. The comparison is directly based on the contents of the image which the user wants to know more.

Interesting features are the categorization of the elements, the ability to search specific portions of the image or start a search from a video file,&lt;ref&gt;Mve. &quot;[http://www.macroglossa.com/press_macrog_eng_a2dot0.pdf - Macroglossa PR]&quot;,  - Retrieved 2011.&lt;/ref&gt; but the main function is to simulate a digital eye on trying to find similarities of an unknown subject. This feature makes the engine unique.

This technology has several advantages. First, it allows users to pull results from collections of visual content&lt;ref&gt;Make Use OF . &quot;[http://www.makeuseof.com/dir/macroglossa-identify-objects-in-image/ - MacroGlossa: Find Similar Images &amp; Identify Objects In Image ]&quot;,  - Makeuseof.com. 2010.&lt;/ref&gt; without using tags for search. Second, the visuals can be [[Crowdsourcing|crowd sourced]]. In fact by being a search engine, rather than simply a tool, Macroglossa should be able to crowdsourced and scale its recognition vocabulary faster than anyone else and a technology like this would increase the cognitive and spatial skills in [[humanoid]] robotics.&lt;ref&gt;J. Sturm, A. Visser. &quot;[http://cvpr.in.tum.de/old/pub/pub/sturm09ras.pdf An appearance-based visual compass for mobile robots ]&quot;, Appearance-based, mobile robot localization, active vision, machine learning. 2000.&lt;/ref&gt; In addition Macroglosssa can also be used as a Reverse Image Search to find [[orphan works]] and possible violations of copyright of images.

Macroglossa supports all popular image extensions such [[Jpg|jpeg]], [[Portable Network Graphics|png]], [[BMP file format|bmp]], [[gif]] and video formats such [[Audio Video Interleave|avi]], [[.mov|mov]], [[mp4]], [[m4v]], [[3gp]], [[wmv]], [[mpeg]].

Macroglossa enters [[Beta stage#Beta beta|beta]] stage in September 2011&lt;ref&gt;Mve. &quot;[http://www.macroglossa.com/disclaimer.html - macroglossa.com]&quot;,  - Releases and Features. 2011.&lt;/ref&gt; and at the same time open to the public the opportunity to use the developed [[Interface (object-oriented programming)|interfaces]] ( Api for web and mobile applications ) in order to expand the use of the engine in the [[Business-to-business|B2B]] and [[Business-to-consumer|B2C]] fields. Macroglossa becomes a [[Software as a service|SaaS]].

[[Api|API]] are distributed on three levels : free, basic, and premium. The free API has limited use, but basic and premium do not. The premium API also offers custom services allowing customers to extend and mold the features offered by computer vision.&lt;ref&gt;J. R. Martínez-de Dios, C. Serna y A. Ollero. &quot;[http://grvc.us.es/publica/revistas/documentos/FishFarms.pdf Computer vision and robotics techniques in fish farms ]&quot;, Robotica. Vo. 21. No. 3. Editor Cambridge University Press. June 2003.&lt;/ref&gt;

==References==
{{reflist}}

==Notes==
* ''Wired.it, Retrieved December 29, 2010 :'' Macroglossa is an Italian project born from a passion for research and innovation by the MVE group of independent developers. The startup has developed a visual search engine based on the comparison of the subjects in the images. The owners of the project define it as &quot;a sort of digital eye can capture, compare and draw conclusions.&quot; The purpose of this service is to provide a new type of research within the network. The search engine allows you to upload a picture on the platform and look for similar images on the web. The engine is not based on text tags and does not use OCR to extract strings from images to locate the target. Everything focuses on the key points of the image uploaded by the user. The aim is to give as much information as possible on the results obtained. Each image has a direct result of the source.

==External links==
* [http://www.macroglossa.com Macroglossa] home page
* Macroglossa [http://www.macroglossa.com/api.html Api program]
* Macroglossa on [http://www.killerstartups.com/Search/macroglossa-com-carry-out-visual-searches Killer Startups]
* [http://yourstory.in/2011/07/macroglossa-reaches-alpha-version-4-0-a-picture-search-engine/ Yourstory.in] talks about Macroglossa

[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Data search engines]]
[[Category:Multimedia]]
[[Category:Image search]]</text>
      <sha1>5ojcwsoi2cyu8xuetky8xt9116x8oyz</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic compression</title>
    <ns>0</ns>
    <id>34087348</id>
    <revision>
      <id>629817402</id>
      <parentid>504489118</parentid>
      <timestamp>2014-10-16T06:12:56Z</timestamp>
      <contributor>
        <ip>58.6.93.99</ip>
      </contributor>
      <comment>/* Applications and advantages */ Minor grammar fixes.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5186">In [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build 
a textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. 
As a result, the same ideas can be represented using a smaller set of words.

Semantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document 
cannot be reconstructed in a reverse process.

==Semantic compression by generalization==
Semantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:
#	determining cumulated term frequencies to identify target lexicon,
#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.&lt;ref&gt;[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010&lt;/ref&gt;

Step 1 requires assembling word frequencies and 
information on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, 
a cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:
&lt;math&gt;cum f(k_{i}) = f(k_{i}) + \sum_{j} cum f(k_{j})&lt;/math&gt; where &lt;math&gt;k_{i}&lt;/math&gt; is a hypernym of &lt;math&gt;k_{j}&lt;/math&gt;.
Then, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.

In the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence 
of a less frequent hyponym as its hypernym in output text.

;Example

The below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.

&lt;blockquote&gt;They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' 
in very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects 
'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the 
'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of 
'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.&lt;/blockquote&gt;

The procedure outputs the following text:

&lt;blockquote&gt;They are both '''facility''' building '''insect''', but '''insect''' and honey '''insects''' '''arrange''' their '''biological groups''' 
in very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects 
'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the 
'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of 
'''organic process''', and there are '''impinging difference of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.&lt;/blockquote&gt;

==Implicit semantic compression==
A natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)
.&lt;ref&gt;[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],
COLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982&lt;/ref&gt;

==Applications and advantages==
In the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less 
[[computational complexity]] and a positive influence on efficiency. 

Semantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).&lt;ref&gt;[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010&lt;/ref&gt; This is due to more precise descriptors (reduced effect of language diversity – limited language redundancy, a step towards a controlled dictionary).

As in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).

==See also==
* [[Text simplification]]
* [[Lexical substitution]]
* [[Information theory]]
* [[Quantities of information]]

==References==
&lt;references/&gt;

==External links==
* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]

[[Category:Information retrieval]]
[[Category:Natural language processing]]
[[Category:Quantitative linguistics]]
[[Category:Computational linguistics]]</text>
      <sha1>f1qtidpbv13jqut1uyct42q66thzheb</sha1>
    </revision>
  </page>
  <page>
    <title>Preference learning</title>
    <ns>0</ns>
    <id>34072838</id>
    <revision>
      <id>618012185</id>
      <parentid>611084999</parentid>
      <timestamp>2014-07-22T17:55:25Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* References */Task 2: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9127">'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.&lt;/ref&gt; In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.

While the concept of preference learning has been emerged for some time in many fields such as [[economics]],&lt;ref name=&quot;SHOG00&quot; /&gt; it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.&lt;ref name=&quot;WEB:WORKSHOP&quot; /&gt;

==Tasks==

The main task in preference learning concerns problems in &quot;[[learning to rank]]&quot;. According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':&lt;ref name=&quot;FURN11&quot; /&gt;

===Label ranking===

In label ranking, the model has an instance space &lt;math&gt;X=\{x_i\}\,\!&lt;/math&gt; and a finite set of labels &lt;math&gt;Y=\{y_i|i=1,2,\cdots,k\}\,\!&lt;/math&gt;. The preference information is given in the form &lt;math&gt;y_i \succ_{x} y_j\,\!&lt;/math&gt; indicating instance &lt;math&gt;x\,\!&lt;/math&gt; shows preference in &lt;math&gt;y_i\,\!&lt;/math&gt; rather than &lt;math&gt;y_j\,\!&lt;/math&gt;. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.

It was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:&lt;ref name=&quot;HARP03&quot; /&gt; if a training instance &lt;math&gt;x\,\!&lt;/math&gt; is labeled as class &lt;math&gt;y_i\,\!&lt;/math&gt;, it implies that &lt;math&gt;\forall j \neq i, y_i \succ_{x} y_j\,\!&lt;/math&gt;. In [[Multi-label classification|multi-label]] situation, &lt;math&gt;x\,\!&lt;/math&gt; is associated with a set of labels &lt;math&gt;L \subseteq Y\,\!&lt;/math&gt; and thus the model can extract a set of preference information &lt;math&gt;\{y_i \succ_{x} y_j | y_i \in L, y_j \in Y\backslash L\}\,\!&lt;/math&gt;. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.

===Instance ranking===

Instance ranking also has the instance space &lt;math&gt;X\,\!&lt;/math&gt; and label set &lt;math&gt;Y\,\!&lt;/math&gt;. In this task, labels are defined to have a fixed order &lt;math&gt;y_1 \succ y_2 \succ \cdots \succ y_k\,\!&lt;/math&gt; and each instance &lt;math&gt;x_l\,\!&lt;/math&gt; is associated with a label &lt;math&gt;y_l\,\!&lt;/math&gt;. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.

===Object ranking===

Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form &lt;math&gt;x_i \succ x_j\,\!&lt;/math&gt; and the model should find out a ranking order among instances.

==Techniques==

There are two practical representations of the preference information &lt;math&gt;A \succ B\,\!&lt;/math&gt;. One is assigning &lt;math&gt;A\,\!&lt;/math&gt; and &lt;math&gt;B\,\!&lt;/math&gt; with two real numbers &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt; respectively such that &lt;math&gt;a &gt; b\,\!&lt;/math&gt;. Another one is assigning a binary value &lt;math&gt;V(A,B) \in \{0,1\}\,\!&lt;/math&gt; for all pairs &lt;math&gt;(A,B)\,\!&lt;/math&gt; denoting whether &lt;math&gt;A \succ B\,\!&lt;/math&gt; or &lt;math&gt;B \succ A\,\!&lt;/math&gt;. Corresponding to these two different representations, there are two different techniques applied to the learning process.

===Utility function===

If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function &lt;math&gt;f: X \times Y \rightarrow \mathbb{R}\,\!&lt;/math&gt; such that &lt;math&gt;y_i \succ_x y_j \Rightarrow f(x,y_i) &gt; f(x,y_j)\,\!&lt;/math&gt;. For instance ranking and object ranking, the mapping is a function &lt;math&gt;f: X \rightarrow \mathbb{R}\,\!&lt;/math&gt;.

Finding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.

===Preference relations===

The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz, Johannes and Hüllermeier proposed this approach in label ranking problem.&lt;ref name=&quot;FURN03&quot; /&gt; For object ranking, there is an early approach by Cohen et al.&lt;ref name=&quot;COHE98&quot; /&gt;

Using preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.&lt;ref name=&quot;FURN03&quot; /&gt;

==Uses==

Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.&lt;ref name=&quot;LIU09&quot; /&gt;

Another application of preference learning is [[recommender systems]].&lt;ref name=&quot;GEMM09&quot; /&gt; Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.

==See also==
*[[Learning to rank]]

==References==

{{Reflist|
refs=

&lt;ref name=&quot;SHOG00&quot;&gt;{{
cite journal
|last       = Shogren
|first      = Jason F.
|coauthors  = List, John A.; Hayes, Dermot J.
|year       = 2000
|title      = Preference Learning in Consecutive Experimental Auctions
|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm
|journal    = American Journal of Agricultural Economics
|volume     = 82
|pages      = 1016–1021
|doi=10.1111/0002-9092.00099
}}&lt;/ref&gt;

&lt;ref name=&quot;WEB:WORKSHOP&quot;&gt;{{
cite web
|title      = Preference learning workshops
|url        = http://www.preference-learning.org/#Workshops
}}&lt;/ref&gt;

&lt;ref name=&quot;FURN11&quot;&gt;{{
cite book
|last       = F&amp;uuml;rnkranz
|first      = Johannes
|coauthors  = H&amp;uuml;llermeier, Eyke
|year       = 2011
|title      = Preference Learning
|url        = http://books.google.com/books?id=nc3XcH9XSgYC
|chapter    = Preference Learning: An Introduction
|chapterurl = http://books.google.com/books?id=nc3XcH9XSgYC&amp;pg=PA4
|publisher  = Springer-Verlag New York, Inc.
|pages      = 3–8
|isbn       = 978-3-642-14124-9
}}&lt;/ref&gt;

&lt;ref name=&quot;HARP03&quot;&gt;{{
cite journal
|last       = Har-peled
|first      = Sariel
|coauthors  = Roth, Dan; Zimak, Dav
|year       = 2003
|title      = Constraint classification for multiclass classification and ranking
|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02
|pages      = 785–792
}}&lt;/ref&gt;

&lt;ref name=&quot;FURN03&quot;&gt;{{
cite journal
|last       = F&amp;uuml;rnkranz
|first      = Johannes
|coauthors  = H&amp;uuml;llermeier, Eyke
|year       = 2003
|title      = Pairwise Preference Learning and Ranking
|journal    = Proceedings of the 14th European Conference on Machine Learning
|pages      = 145–156
}}&lt;/ref&gt;

&lt;ref name=&quot;COHE98&quot;&gt;{{
cite journal
|last       = Cohen
|first      = William W.
|coauthors  = Schapire, Robert E.; Singer, Yoram
|year       = 1998
|title      = Learning to order things
|url        = http://dl.acm.org/citation.cfm?id=302528.302736
|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems
|pages      = 451–457
}}&lt;/ref&gt;

&lt;ref name=&quot;LIU09&quot;&gt;{{
cite journal
|last       = Liu
|first      = Tie-Yan
|year       = 2009
|title      = Learning to Rank for Information Retrieval
|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304
|journal    = Foundations and Trends in Information Retrieval
|volume     = 3
|issue      = 3
|pages      = 225–331
|doi        = 10.1561/1500000016
}}&lt;/ref&gt;

&lt;ref name=&quot;GEMM09&quot;&gt;{{
cite journal
|last       = Gemmis
|first      = Marco De
|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni 
|year       = 2009
|title      = Preference Learning in Recommender Systems
|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45
|journal    = PREFERENCE LEARNING
|volume     = 41
|pages      = 387–407
|doi=10.1007/978-3-642-14125-6_18
}}&lt;/ref&gt;

}}

==External links==
*[http://www.preference-learning.org/ Preference Learning site]

[[Category:Information retrieval]]
[[Category:Machine learning]]</text>
      <sha1>bldafin8jfxmkojv3554cn3gb41lw8d</sha1>
    </revision>
  </page>
  <page>
    <title>Question answering</title>
    <ns>0</ns>
    <id>360030</id>
    <revision>
      <id>647239635</id>
      <parentid>647239145</parentid>
      <timestamp>2015-02-15T12:47:59Z</timestamp>
      <contributor>
        <username>Newwikieditor678</username>
        <id>24128590</id>
      </contributor>
      <minor/>
      <comment>Inline citation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23930">{{other uses|question|answer}}
{{multiple issues|
{{cleanup|date=January 2012|reason=extensive use of jargon to define jargon, and inconsistent use of bold and italics font styles}}
{{cleanup-rewrite|date=January 2012}}
{{more footnotes|date=February 2014}}
}}

'''Question Answering''' ('''QA''') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].

A QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents&lt;ref&gt;&quot;[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]&quot;, Mittal et al., IJIIDS, 5(2), 119-142, 2011  
&lt;/ref&gt;

Some examples of natural language document collections used for QA systems include:
* a local collection of reference texts

* internal organization documents and web pages
* compiled [[newswire]] reports
* a set of [[Wikipedia]] pages
* a subset of [[World Wide Web]] pages

QA research attempts to deal with a wide range of question types including: fact, list, definition, ''How'', ''Why'', hypothetical, semantically constrained, and cross-lingual questions.

* ''Closed-domain'' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease &lt;ref&gt;Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer’s Disease. CLEF 2012 Evaluation Labs and Workshop. September 17 2012&lt;/ref&gt;
* ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.

==History==

Two early QA systems were BASEBALL and LUNAR.{{when|date=November 2012}}{{who|date=November 2012}}{{citation needed|date=November 2012}} BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.

[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the &quot;blocks world&quot;), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.

In the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with these [[expert system]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.

The 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.

Recently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.

==Architecture==
Most modern QA systems use [[natural language]] text documents as their underlying knowledge source.  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted. An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge. However, many of these tools do not produce a human-like answer, but rather employ &quot;shallow&quot; methods (keyword-based techniques, templates...) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.

In an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s. It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system. Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors. An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.

Current QA systems&lt;ref&gt;Hirschman, L. &amp; Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.&lt;/ref&gt; typically include a '''question classifier''' module that determines the type of question and the type of answer. After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text. Thus, a '''document retrieval module''' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer. Subsequently a '''filter''' preselects small text fragments that contain strings of the same type as the expected answer. For example, if the question is &quot;Who invented
Penicillin&quot; the filter returns text that contain names of people. Finally, an '''answer extraction''' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.

A '''multiagent''' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge. The meta–agent controls the cooperation between question answering agents and chooses the most relevant answer(s).&lt;ref&gt;{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124
}}&lt;/ref&gt;

==Question answering methods==
QA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,&lt;ref&gt;Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).&lt;/ref&gt; leading to two benefits:
# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.
# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.

Question answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],&lt;ref&gt;{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=http://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}&lt;/ref&gt; a [[logic programming]] language associated with [[artificial intelligence]].

===Open domain question answering===
In [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user’s question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.

The system takes a [[natural language]] question as an input rather than a set of keywords, for example, “When is the national day of China?” The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.

Keyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. “Who”, “Where” or “How many”, these words tell the system that the answers should be of type “Person”, “Location”, “Number” respectively. In the example above, the word “When” indicates that the answer should be of type “Date”. POS tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is “Chinese National Day”, the predicate is “is” and the adverbial modifier is “when”, therefore the answer type is “Date”. Unfortunately, some interrogative words like “Which”, “What” or “How” do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.

Once the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as “Who” or “Where”, a Named Entity Recogniser is used to find relevant “Person” and “Location” names from the retrieved documents. Only the relevant paragraphs are selected for ranking.

A [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is “1st Oct.”

==Issues==
In 2002 a group of researchers wrote a roadmap of research in question answering.&lt;ref&gt;Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R. [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues, Tasks and Program Structures to Roadmap Research in Question Answering (QA)].&lt;/ref&gt; The following
issues were identified.&lt;!-- much of the text in this section is copied and pasted from the &quot;roadmap&quot; document; somebody may try and simplify the text --&gt;

;Question classes : Different types of questions (e.g., &quot;What is the capital of [[Liechtenstein]]?&quot; vs. &quot;Why does a [[rainbow]] form?&quot; vs. &quot;Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?&quot;) require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}}

;Question processing : The same information request can be expressed in various ways, some interrogative (&quot;Who is the King of Lesotho?&quot;) and some assertive (&quot;Tell me the name of the King of Lesotho.&quot;). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification.

;Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, &quot;Why did Joe Biden visit Iraq in January 2010?&quot; might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner's intent.)

;Data sources for QA : Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained.

;Answer extraction : Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}}

;Answer formulation : The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, &quot;On what day did Christmas fall in 1989?&quot;) the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents.

;Real time question answering : There is need for developing Q&amp;A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question.

;Multilingual (or cross-lingual) question answering : The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].)

;Interactive QA : It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions.&lt;ref&gt;Perera, R. and Nand, P. 2014. [http://link.springer.com/chapter/10.1007%2F978-3-319-11716-4_11 Interaction History Based Answer Formulation for Question Answering.]&lt;/ref&gt; (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.)

;Advanced reasoning for QA : More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system.

;Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process.&lt;ref&gt;Perera, R. 2012. [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6305919&amp;isnumber=6305918 IPedagogy: Question Answering System Based on Web Information Clustering.]&lt;/ref&gt;

;User profiling for QA : The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}}

==Progress==
QA systems have been extended in recent years to encompass additional domains of knowledge&lt;ref&gt;Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.&lt;/ref&gt;  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:

* interactivity—clarification of questions or answers
* answer reuse or caching
* knowledge representation and reasoning
* social media analysis with QA systems
* [[sentiment analysis]]&lt;ref&gt;[http://totalgood.com/bitcrawl/ BitCrawl] by Hobson Lane&lt;/ref&gt;
* utilization of thematic roles&lt;ref&gt;Perera, R. and Perera, U. 2012. [http://www.aclweb.org/anthology/W12-6004 Towards a thematic role based target identification model for question answering.]&lt;/ref&gt;
* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts&lt;ref&gt;{{cite conference | author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR'08)| pages= 430–437 | publisher=Springer Berlin Heidelberg | title= [http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 The impact of semantic class identification and semantic role labeling on natural language answer extraction]}}&lt;/ref&gt;
* utilization of linguistic resources,&lt;ref&gt;{{cite journal |author=Bahadorreza Ofoghi, John Yearwood, and Liping Ma|title=[http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&amp;userIsAuthenticated=false The impact of frame semantic annotation levels, frame‐alignment techniques, and fusion methods on factoid answer processing] | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247–263 |year =2009}}&lt;/ref&gt; such as [[WordNet]], [[FrameNet]], and the similar

IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.
&lt;ref&gt;http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0&lt;/ref&gt;

==References==
* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.
* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.
*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}
* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, ''The History of Mathematical Logic''. North-Holland, Amsterdam.

&lt;references/&gt;

==External links==
* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]
* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]
* [http://celct.fbk.eu/QA4MRE/ Question Answering Evaluation at CLEF]

{{Computable knowledge}}
{{Natural Language Processing}}

[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]</text>
      <sha1>9so7p6aht2tkc3z4hnqc1kjka31dkem</sha1>
    </revision>
  </page>
  <page>
    <title>Natural language user interface</title>
    <ns>0</ns>
    <id>18863997</id>
    <revision>
      <id>643032508</id>
      <parentid>635991426</parentid>
      <timestamp>2015-01-18T10:24:13Z</timestamp>
      <contributor>
        <username>SmackoVector</username>
        <id>19552626</id>
      </contributor>
      <minor/>
      <comment>link [[Braina]] using [[User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16769">'''Natural Language User Interfaces''' (LUI or NLUI) are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.

In [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.&lt;ref&gt;Hill, I. (1983). &quot;Natural language versus computer language.&quot; In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.&lt;/ref&gt;
Natural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general Natural language interface is one of the active goals of the [[Semantic Web]].

Text interfaces are 'natural' to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a 'shallow' Natural language user interface.

==Overview==
A natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which [[United States|U.S.]] state has the highest [[income tax]]?', conventional search engines ignore the question and instead search on the [[index term|keywords]] 'state', 'income' and 'tax'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.

==History==

Prototype Nl interfaces had already appeared in the late sixties and early seventies.&lt;ref name=&quot;edin&quot;&gt;Natural Language Interfaces to Databases – An Introduction,
I. Androutsopoulos,
G.D. Ritchie,
P. Thanisch,
Department of Artificial Intelligence, University of Edinburgh&lt;/ref&gt;

*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual &quot;blocks world&quot;
*''Lunar'', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].
*''Chat-80'' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.&lt;ref&gt;[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]&lt;/ref&gt;
*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.&lt;ref&gt;[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]&lt;/ref&gt;
* ''Janus'' is also one of the few systems to support temporal questions.
* ''Intellect'' from [[Trinzic]] (formed by the merger of AICorp and Aion).
* BBN’s ''Parlance'' built on experience from the development of the ''Rus''  and ''Irus'' systems.
* [[IBM]] ''Languageaccess''
* [[Q&amp;A (software)|Q&amp;A]] from [[Symantec]].
* ''Datatalker'' from Natural Language Inc.
* ''Loqui''  from [[Bim]].
* ''English Wizard'' from [[Linguistic Technology Corporation]].
* ''iAskWeb'' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001&lt;ref&gt;{{cite book | last = Galitsky
 | first = Boris
 | title = Natural Language Question Answering: technique of semantic headers
 | publisher = Advance Knowledge International
 | date = 2003
 | location = Adelaide, Australia
 | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799
 | isbn = 0868039799
  }}&lt;/ref&gt;

==Challenges==
Natural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.

A [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges:&lt;ref name=&quot;edin&quot;/&gt;
* ''Modifier attachment''
The request “List all employees in the company with a driving licence” is ambiguous unless you know companies can't have drivers licences.

* ''Conjunction and disjunction''
“List all applicants who live in California and Arizona” is ambiguous unless you know that a person can't live in two places at once.
* ''[[Anaphora resolution]]''
- resolve what a user means by 'he', 'she' or 'it', in a self-referential query.

Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market.

Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.

==Uses and applications==

The natural language interface gives rise to technology used for many different applications. 

Some of the main uses are:

* ''Dictation'', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.
* ''Command and control'', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like &quot;Open Netscape&quot; and &quot;Start a new xterm&quot; will do just that.
* ''Telephony'', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.
* ''Wearables'', because inputs are limited for wearable devices, speaking is a natural possibility.
* ''Medical, disabilities'', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.
* ''Embedded applications'', some new cellular phones include C&amp;C speech recognition that allow utterances such as &quot;call home&quot;. This may be a major factor in the future of automatic speech recognition and [[Linux]].

Below are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.

===Ubiquity===
{{main|Ubiquity (Firefox)}}
Ubiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.

===Wolfram Alpha===
{{main|Wolfram Alpha}}
Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.&lt;ref&gt;{{cite news |url=http://www.guardian.co.uk/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}&lt;/ref&gt; It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.&lt;ref name=&quot;launch date&quot;&gt;{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}&lt;/ref&gt;

===Siri===
{{main|Siri (software)}}
Siri is a [[personal assistant]] application for the operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations. The iPhone app is the first public product by its makers, who are focused on [[artificial intelligence]] applications.

Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.&lt;ref&gt;[http://www.apple.com/iphone/features/siri.html Siri webpage]&lt;/ref&gt;

===Others===
* [[Anboto Group]] provides Web customer service and e-commerce technology based on semantics and natural language processing. The main offer of [http://www.anbotogroup.com/en/index.php Anboto Group] are the virtual sales agent and intelligent chat.
* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.
* [[Braina]]&lt;ref&gt;[http://www.brainasoft.com/braina/ Braina]&lt;/ref&gt; - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.
* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user's questions by looking up relevant information in Wikipedia and some news sources.
* C-Phrase&lt;ref&gt;[http://code.google.com/p/c-phrase/ C-Phrase]&lt;/ref&gt; - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English. C-Phrase is hosted on [[Google Code]] site.
* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.
[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]
* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).&lt;ref&gt;Ubuntu 10.04 Add/Remove Applications description for GNOME Do&lt;/ref&gt;
* [[Invention Machine]] Goldfire - powered by a semantic research engine that has the capability to transform unstructured documents from various electronic sources into an index that, when searched, delivers answers to research questions. Goldfire’s Natural Language query interface enables the user to put a question in a free text format, which would be the same format as if the question were given to another person. And, once knowledge has been retrieved, Goldfire presents the results in a way that makes their meaning readily apparent.
* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.
* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as &quot;How old is Wikipedia?&quot; When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.
* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.
* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as 'What is the time in São Paulo when it is 6pm on the 2nd of June in Detroit'.
* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.
* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.
* [[Powerset (company)|Powerset]] — On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.&lt;ref&gt;{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}&lt;/ref&gt; On July 1, 2008, it was purchased by [[Microsoft]].&lt;ref&gt;{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=http://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}&lt;/ref&gt;
* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a company’s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011
* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with &quot;just the right information,&quot; instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.
* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you're looking for (according to their site).
* [[Yebol]] - Yebol is a vertical &quot;decision&quot; search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.&lt;ref&gt;Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ &quot;Yebol.com steps into the search market&quot;] ''Geek.com''. 31 July 2009.&lt;/ref&gt;

==See also==
*[[Natural language programming]]
**[[xTalk]], a family of English-like programming languages
*[[Chatterbot]], a computer program that simulates human conversations
*[[Noisy text]]
*[[Question answering]]
*[[Selection-based search]]
*[[Semantic search]]
*[[Semantic Web]]

==References==
{{reflist}}

{{Internet search}}
{{Computable knowledge}}

{{DEFAULTSORT:Natural language user interface}}
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]</text>
      <sha1>nr1y4lsn8q8lmk6ozr6y1k7kp2yh3o6</sha1>
    </revision>
  </page>
  <page>
    <title>Search engine technology</title>
    <ns>0</ns>
    <id>6422823</id>
    <revision>
      <id>638463112</id>
      <parentid>638463111</parentid>
      <timestamp>2014-12-17T07:13:42Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contributions/Sjukunte|Sjukunte]] to version by Jimblackler. False positive? [[User:ClueBot NG/FalsePositives|Report it]]. Thanks, [[User:ClueBot NG|ClueBot NG]]. (2067134) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="36800">{{multiple issues|
{{Refimprove|date=May 2014}}
{{Tone|article|date=January 2013}}
}}
A search engine is a type of computer software used to search data in the form of text or a database for specified information.&lt;ref&gt;{{cite web|title=Define search engine|url=http://www.webopedia.com/TERM/S/search_engine.html|accessdate=1 June 2014}}&lt;/ref&gt;

Search engines normally consist of spiders (also known as bots) which roam the web searching for links and keywords. They send collected data back to the indexing software which categorizes and adds the links to databases with their related keywords. When you specify a search term the engine does not scan the whole web but extracts related links from the database.

==History of Search Technology==

{{Empty section|date=July 2014}}

== The Memex ==

The concept of hypertext and a memory extension originates from an article that was published in [[The Atlantic Monthly]] in July 1945 written by [[Vannevar Bush]], titled [[As We May Think]].  Within this article Vannevar urged scientists to work together to help build a body of knowledge for all mankind. He then proposed the idea of a virtually limitless, fast, reliable, extensible, associative memory storage and retrieval system. He named this device a [[memex]].&lt;ref&gt;{{cite journal|last1=Yeo|first1=Richard|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|page=21|doi=10.1017/S0269889706001128}}&lt;/ref&gt;

Bush regarded the notion of “associative indexing” as his key conceptual contri- bution. As he explained, this was “a provision whereby any item may be caused at will to select immediately and automatically another. This is the essential feature of the memex. The process of tying two items together is the important thing.” This “linking” (as we now say) constituted a “trail” of documents that could be named, coded, and found again. Moreover, after the original two items were coupled, “numerous items” could be “joined together to form a trail”; they could be “reviewed in turn, rapidly or slowly, by deflecting a lever like that used for turning the pages of a book. It is exactly as though the physical items had been gathered together from widely separated sources and bound together to form a new book”&lt;ref&gt;{{cite journal|title=Before Memex: Robert Hooke, John Locke, and Vannevar Bush on External Memory|journal=Science in Context|date=30 January 2007|volume=20|issue=01|pages=21–47|doi=10.1017/S0269889706001128|accessdate=1 June 2014|postscript=The example Bush gives is a quest to find information on the relative merits of the Turkish short bow and the English long bow in the crusades}}&lt;/ref&gt;

All of the documents used in the memex would be in the form of microfilm copy acquired as such or, in the case of personal records, transformed to microfilm by the machine itself. Memex would also employ new retrieval techniques based on a new kind of associative indexing the basic idea of which is a provision whereby any item may be caused at will to select immediately and automatically another to create personal &quot;trails&quot; through linked documents. The new procedures, that Bush anticipated facilitating information storage and retrieval would lead to the development of wholly new forms of encyclopedia.

The most important mechanism, conceived by Bush and considered as closed to the modern hypertext systems is the associative trail. It would be a way to create a new linear sequence of microfilm frames across any arbitrary sequence of microfilm frames by creating a chained sequence of links in the way just described, along with personal comments and side trails.
The essential feature of the memex [is] the process of tying two items together… When the user is building a trail, he names it in his code book, and taps it out on his keyboard. Before him are the two items to be joined, projected onto adjacent viewing positions. At the bottom of each there are a number of blank code spaces, and a pointer is set to indicate one of these on each item. The user taps a single key, and the items are permanently joined… Thereafter, at any time, when one of these items is in view, the other can be instantly recalled merely by tapping a button below the corresponding code space.

In the article of Bush is not described any automatic search, nor any universal metadata scheme such as a standard library classification or a hypertext element set. Instead, when the user made an entry, such as a new or annotated manuscript, or image, he was expected to index and describe it in his personal code book. Later on, by consulting his code book, the user could retrace annotated and generated entries.

In 1965 Bush took part in the project INTREX of MIT, for developing technology for mechanization the processing of information for library use. In his 1967 essay titled &quot;Memex Revisited&quot;, he pointed out that the development of the digital computer, the transistor, the video, and other similar devices had heightened the feasibility of such mechanization, but costs would delay its achievements. He was right again.

Ted Nelson, who later did pioneering work with first practical hypertext system and coined the term &quot;hypertext&quot; in the 1960s, credited Bush as his main influence.&lt;ref&gt;{{cite web|title=The MEMEX of Vannevar Bush|url=http://history-computer.com/Internet/Dreamers/Bush.html}}&lt;/ref&gt;

== SMART ==

Gerard Salton, who died on August 28 of 1995, was the father of modern search technology. His teams at Harvard and Cornell developed the SMART informational retrieval system. Salton’s Magic Automatic Retriever of Text included important concepts like the vector space model, Inverse Document Frequency (IDF), Term Frequency (TF), term discrimination values, and relevancy feedback mechanisms.

He authored a 56 page book called A Theory of Indexing which explained many of his tests upon which search is still largely based.

== String Search Engines ==

In 1987 an article was published detailing the development of a character string search engine (SSE) for rapid text retrieval on a double-metal 1.6-μm n-well CMOS solid-state circuit with 217,600 transistors lain out on a 8.62x12.76-mm die area. The SSE accommodated a novel string-search architecture which combines a 512-stage finite-state automaton (FSA) logic with a content addressable memory (CAM) to achieve an approximate string comparison of 80 million strings per second. The CAM cell consisted of four conventional static RAM (SRAM) cells and a read/write circuit. Concurrent comparison of 64 stored strings with variable length was achieved in 50 ns for an input text stream of 10 million characters/s, permitting performance despite the presence of single character errors in the form of character codes. Furthermore, the chip allowed nonanchor string search and variable-length `don't care' (VLDC) string search.&lt;ref&gt;{{cite journal|last=Yamada|first=H.|author2=Hirata, M. |author3=Nagai, H. |author4= Takahashi, K. |title=A high-speed string-search engine|journal=IEEE Journal of Solid-State Circuits|date=Oct 1987|volume=22|issue=5|pages=829–834|doi=10.1109/JSSC.1987.1052819|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=1052819&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D1052819|accessdate=30 May 2014|publisher=IEEE}}&lt;/ref&gt;

&lt;!-- Potential source for article expansion:  http://ieeexplore.ieee.org/search/searchresult.jsp?queryText%3Dsearch-engine&amp;sortType=asc_p_Publication_Year&amp;pageNumber=1&amp;resultAction=SORT --&gt;

== Web Search Engines ==

=== Archie ===

The first web search engines was Archie, created in 1990&lt;ref name=&quot;intelligent-technologies&quot;&gt;{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=87|url=http://books.google.com/books?id=HqXxoWK7tucC&amp;pg=PA87&amp;lpg=PA87&amp;dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&amp;source=bl&amp;ots=Xt7TQz0a6Y&amp;sig=vusKa34uORNCBI6lT3-sEy5qv-Q&amp;hl=en&amp;sa=X&amp;ei=KzqOU7PCDcOlyATtt4L4DA&amp;ved=0CEoQ6AEwBQ#v=onepage&amp;q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&amp;f=false|accessdate=3 June 2014}}&lt;/ref&gt; by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program &quot;archives,&quot; but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on. For more information on where Archie is today, see:
http://www.bunyip.com/products/archie/

The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.

Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, &quot;anonymous&quot; FTP sites became repositories for files, allowing all users to post and retrieve them.

Even with archive sites, many important files were still scattered on small FTP servers. Unfortunately, these files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.

Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.&lt;ref name=&quot;wileyhistory&quot;&gt;{{cite web|title=A History of Search Engines|url=http://www.wiley.com/legacy/compbooks/sonnenreich/history.html|publisher=Wiley|accessdate=1 June 2014}}&lt;/ref&gt;

=== Veronica ===

In 1993, the University of Nevada System Computing Services group developed Veronica.&lt;ref name=&quot;intelligent-technologies&quot;/&gt; It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.&lt;ref name=&quot;wileyhistory&quot;/&gt;

=== The Lone Wanderer ===

The World Wide Web Wanderer, developed by Matthew Gray in 1993&lt;ref&gt;{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=86|url=http://books.google.com/books?id=HqXxoWK7tucC&amp;pg=PA87&amp;lpg=PA87&amp;dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&amp;source=bl&amp;ots=Xt7TQz0a6Y&amp;sig=vusKa34uORNCBI6lT3-sEy5qv-Q&amp;hl=en&amp;sa=X&amp;ei=KzqOU7PCDcOlyATtt4L4DA&amp;ved=0CEoQ6AEwBQ#v=onepage&amp;q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&amp;f=false|accessdate=3 June 2014}}&lt;/ref&gt; was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.

Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of time a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.

In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.

ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot doesn't run about eating up Net bandwidth.  Unfortunately, the disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they don't submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.&lt;ref name=&quot;wileyhistory&quot;/&gt;

=== Excite ===

Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.
Their project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.&lt;ref name=&quot;wileyhistory&quot;/&gt;

Excite was the first serious commercial search engine which launched in 1995.&lt;ref&gt;{{cite web|title=The Major Search Engines|url=http://www.pccua.edu/kholland/major_search_engines.htm|accessdate=1 June 2014|date=21 January 2014}}&lt;/ref&gt; It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.

=== Yahoo! ===

In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.

As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.

The Wanderer captured only URLs, which made it difficult to find things that weren’t explicitly described by their URL. Because URLs are rather cryptic to begin with, this didn’t help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.

=== Lycos ===

At Carnegie Mellon University during the July of 1994, Michael Mauldin, on leave from CMU,developed the Lycos search engine.

== Types of Web Search Engines ==

Search engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.&lt;ref&gt;{{cite book|author1=Priti Srinivas Sajja|author2=Rajendra Akerkar|title=Intelligent technologies for web applications|date=2012|publisher=CRC Press|location=Boca Raton|isbn=978-1-4398-7162-1|page=85|url=http://books.google.com/books?id=HqXxoWK7tucC&amp;pg=PA87&amp;lpg=PA87&amp;dq=the+University+of+Nevada+System+Computing+Services+group+developed+Veronica.&amp;source=bl&amp;ots=Xt7TQz0a6Y&amp;sig=vusKa34uORNCBI6lT3-sEy5qv-Q&amp;hl=en&amp;sa=X&amp;ei=KzqOU7PCDcOlyATtt4L4DA&amp;ved=0CEoQ6AEwBQ#v=onepage&amp;q=the%20University%20of%20Nevada%20System%20Computing%20Services%20group%20developed%20Veronica.&amp;f=false|accessdate=3 June 2014}}&lt;/ref&gt;

# Finding and selecting full or partial content based on the keywords provided.
# Maintaining index of the content and referencing to the location they find
# Allowing users to look for words or combinations of words found in that index.

The process begins when a user enters a query statement into the system through the interface provided.

{| class=&quot;wikitable&quot;
|-
! Type
! Example
! Description
|-
| Conventional
| librarycatalog
| Search by keyword, title, author, etc.
|-
| Text-based
| Lexis-Nexis,Google,Yahoo!
| Search by keywords. Limited search using queries in natural language.
|-
| Multimedia
| QBIC, WebSeek, SaFe
| Search by visual appearance (shapes, colors,..)
|-
| Q/A
| [[Stack Exchange]], NSIR
| Search in (restricted) natural language
|-
| Clustering Systems
| Vivisimo, Clusty
|
|-
| Research Systems
| Lemur, Nutch
|
|}

There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.

Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.

Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.

In both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created —you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index hasn't been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.

So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.

One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.

Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered &quot;important&quot; and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.

Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. [[Google]]), database or structured data search engines (e.g. [[Dieselpoint]]), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and [[Yahoo!]], utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.

==Search engine categories==

===Web search engines===
Search engines that are expressly designed for searching web pages, documents, and images were developed to facilitate searching through a large, nebulous blob of unstructured resources. They are engineered to follow a multi-stage process: crawling the infinite stockpile of pages and documents to skim the figurative foam from their contents, indexing the foam/buzzwords in a sort of semi-structured form (database or something), and at last, resolving user entries/queries to return mostly relevant results and links to those skimmed documents or pages from the inventory.

====Crawl====
In the case of a wholly textual search, the first step in classifying web pages is to find an ‘index item’ that might relate expressly to the ‘search term.’ In the past, search engines began with a small list of URLs as a so-called seed list, fetched the content, and parsed the links on those pages for relevant information, which subsequently provided new links. The process was highly cyclical and continued until enough pages were found for the searcher’s use.
These days, a continuous crawl method is employed as opposed to an incidental discovery based on a seed list. The crawl method is an extension of aforementioned discovery method. Except there is no seed list, because the system never stops worming.

Most search engines use sophisticated scheduling algorithms to “decide” when to revisit a particular page, to appeal to its relevance. These algorithms range from constant visit-interval with higher priority for more frequently changing pages to adaptive visit-interval based on several criteria such as frequency of chance, popularity, and overall quality of site. The speed of the web server running the page as well as resource constraints like amount of hardware or bandwidth also figure in.

====Link map====
The pages that are discovered by web crawls are often distributed and fed into another computer that creates a veritable map of resources uncovered. The bunchy clustermass looks a little like a graph, on which the different pages are represented as small nodes that are connected by  links between the pages. 
The excess of data is stored in multiple data structures that permit quick access to said data by certain algorithms that compute the popularity score of pages on the web based on how many links point to a certain web page, which is how people can access any number of resources concerned with diagnosing psychosis. Another example would be the accessibility/rank of web pages containing information on Mohamed Morsi versus the very best attractions to visit in Cairo after simply entering ‘Egypt’ as a search term. One such algorithm, [[PageRank]], proposed by Google founders Larry Page and Sergey Brin, is well known and has attracted a lot of attention because it highlights repeat mundanity of web searches courtesy of students that don’t know how to properly research subjects on Google.
The idea of doing link analysis to compute a popularity rank is older than PageRank. Other variants of the same idea are currently in use – grade schoolers do the same sort of computations in picking kickball teams. But in all seriousness, these ideas can be categorized into three main categories: rank of individual pages and nature of web site content. Search engines often differentiate between internal links and external links, because web masters and mistresses are not strangers to shameless self-promotion. Link map data structures typically store the anchor text embedded in the links as well, because anchor text can often provide a “very good quality” summary of a web page’s content.

===Database Search Engines===
Searching for text-based content in databases presents a few special challenges from which a number of specialized search engines flourish. Databases can be slow when solving complex queries (with multiple logical or string matching arguments). Databases allow pseudo-logical queries which full-text searches do not use. There is no crawling necessary for a database since the data is already structured. However, it is often necessary to index the data in a more economized form to allow a more expeditious search.

===Mixed Search Engines===
Sometimes, data searched contains both database content and web pages or documents. Search engine technology has developed to respond to both sets of requirements. Most mixed search engines are large Web search engines, like Google. They search both through structured and unstructured data sources. Take for example, the word ‘ball.’ In its simplest terms, it returns more than 40 variations on Wikipedia alone. Did you mean a ball, as in the social gathering/dance? A soccer ball? The ball of the foot? Pages and documents are crawled and indexed in a separate index. Databases are indexed also from various sources. Search results are then generated for users by querying these multiple indices in parallel and compounding the results according to “rules.”

&lt;!-- 
Working on article, loosely pasting in snippets of information to use in improving 
article later, leaving all this in comments while I work on it

LOTS OF WORK TO DO

Potential sections to research into..

== Models of Information Retrieval ==
=== Boolean Model ===
=== Vector Model ===

== Document Preprocessing ==
# Tokenization ===
# Stemming ===
# The Porter Algorithm
# Storing, indexing, and searching text
#Inverted indexes

== Word Distributions ==
The Zipf distribution
The Benford distribution
Heap's law. TF*IDF. Vector space similarity and ranking.

== Retrieval evaluation ==
 Precision and Recall. F-measure. Reference collections. The TREC conferences.

== Automated indexing/labeling ==
. Compression and coding. Optimal codes.

== String matching ==
. Approximate matching.

== Query expansion ==. Relevance feedback.

== Text classification ==
. Naive Bayes. Feature selection. Decision trees.

Linear classifiers. k-nearest neighbors. Perceptron. Kernel methods. Maximum-margin classifiers. Support vector machines. Semi-supervised learning.
Lexical semantics and Wordnet.
Latent semantic indexing. Singular value decomposition. Vector space clustering. k-means clustering. EM clustering.
Random graph models. Properties of random graphs: clustering coefficient, betweenness, diameter, giant connected component, degree distribution.
Social network analysis. Small worlds and scale-free networks. Power law distributions. Centrality.
Graph-based methods. Harmonic functions. Random walks. PageRank. Hubs and authorities. Bipartite graphs. HITS. Models of the Web.

Crawling the web. Webometrics. Measuring the size of the web. The Bow-tie-method.
Hypertext retrieval. Web-based IR. Document closures. Focused crawling.
Question answering
Burstiness. Self-triggerability
Information extraction
Adversarial IR. Human behavior on the web. Text summarization

== Search Engine Parts ==

There are three main parts to every search engine: Spider, Index, and Web Interface.

=== Spider === 
   
A spider crawls the web. It follows links and scans web pages. All search engines have periods of deep crawl and quick crawl. During a deep crawl, the spider follows all links it can find and scans web pages in their entirety. During a quick crawl, the spider does not follow all links and may not scan pages in their entirety.

The job of the spider is to discover new pages and to collect copies of those pages, which are then analyzed in the index.

==== Crawl Rate ====

Pages that are considered important get crawled frequently. The crawl rate depends directly on link popularity and domain authority.

If many links point to a website, it may be an important site, so it makes sense to crawl it more often than a site with fewer links. This is also a money-saving issue. If search engines were to crawl all sites at an equal rate, it would take more time overall and cost more as a result.

=== Index ===

The index is the place where search engines keep basic copies of web pages and sort search results. When you a do a search, search engines do not search the web; they show results from their index. The number of pages in the index does not represent the entire web, but the number of pages that the spider has discovered, scanned and saved.

The index is the place where search engineers apply algorithms, and it is the place where rankings are partially determined. Search engineers may choose to apply an algorithm to the entire index, or only to a portion of it.

==== Datacenters and Different Indexes ====

Search engines have multiple datacenters around the world. When you enter a search term, your query is directed to the closest datacenter.

Different datacenters may have slightly different indexes, especially during an update. As a result, search results may differ depending on your location.

== History ==

=== Meta Tags ===

Meta tags were designed to help search engines sort web pages. Pages included keywords in meta tags telling search engines about the contents of each page. For a short time meta tags worked and helped search engines serve relevant results, but over time marketers learned they could easily rank by stuffing those tags with keywords.

As a result, search engine optimization in those days became about cramming &quot;loans, loans, loans, loans, loans&quot; into the meta tag. Search engines got spammed beyond being of any use, and many faced an exodus of users as a result.

Yahoo started as web directory in 1994 and outsourced their search until 2004. Google launched in 1996 and did not have a successful business model until 2001. Microsoft did not come on the search engine scene until 2003.
or more information on search engine history, you may want to investigate Search Engine History, a site entirely devoted to this topic. It also touches on the history of search engine optimization. Additionally, Web Master World has an excellent thread that covers the history of SEO.

Web Interface

When you search using a web interface (like Google.com), in many cases results are already presorted to a certain extent. The degree to which results are presorted depends on the complexity of the algorithm. If the time to apply an algorithm to the index is considerable, then that algorithm is applied in advance. On the other hand, some algorithms are applied at the time when the search query is requested.

Search queries go through analysis to determine the possible intent behind the query. Google is currently leading in this area.

Stop Words

&quot;Stop words&quot; are words that are frequently used in the English language. Those words include a, the, all, also, but, down, full, much etc. They are words that are used by everyone regardless of the topic. Generally, search engines ignore &quot;stop&quot; words and will usually correct your search to exclude them. For example, when you search for &quot;cat and dog&quot; search engines will exclude &quot;and&quot; and only search for &quot;cat&quot; &quot;dog.&quot;

Google does use stop words to an extent.

Keyword Density

Keyword density is a measure of how often a word appears on the page in relation to other words. It is an over-hyped measurement that doesn’t help in search rankings. Search engines use far more than keyword density for on-page analysis. Their technology includes the location of terms on the page, word proximity and natural language processing.

Google has purchased Applied Semantics for its AdSense Network, but may also be using this technology for on-page analysis. Additionally, please keep in mind that one of Google’s current projects involves scanning thousands of books, from which it may learn more about natural language patterns.

Location of Terms on The Page

By analyzing how terms are located in relation to each other on the page, search engines can determine partial relevancy of the page. The closer terms are to each other, the more relevant a page is.

In many cases, keywords appear separately from each other throughout the page. This is considered normal in most cases, but be sure to include a term together at least once in the title, heading or paragraph.

Link Analysis

Link analysis is at the core of all search engine relevancy. Apart from Page Rank and general link popularity, Google looks at: link anchor text, the page from which the link comes, age of the link, location of the link, title of the page from which the link comes, authority of the linking page and more.

Links are the biggest quality indicators that search engines have at the moment. Before search engines existed, and before the web was commercialized it was much harder to find information. All you had to rely on was links. There were few if any spammers, and people who found interesting sites shared those sites with others by placing a link. Also, the first web pages and servers were universities and colleges; this is why Google is biased toward .edu domains – they were the first on the scene, and usually contain quality content and resources.

As the web became commercial and Google’s Page Rank well known, links became a form of advertising, where a link could be bought or artificially made by spammers. This is the reason for Google’s bias toward older links and links from trusted domains.

Yahoo put less weight on link analysis than Google, while Ask.com is more about &quot;authoritative hubs.&quot; Ask.com generally has a harder time ranking documents unless there’s a community around a topic.

Size and Length of the Page

There’s no &quot;best&quot; page copy length for ranking on search results. Search engines have specifically addressed this issue, and both long content and short content have equal chances to rank.

Behavioral Feedback

All major search engines such as Google, Yahoo, Live and Ask collect user feedback about web pages. They look at search queries, prior search queries, time interval between those queries and semantic relationships in order to learn more about intent. They also track click through rates for different listings. If, for example, users click on a listing and then go back right away, search engines may remove that listing and artificially lower its position for one or more keywords.

This brings up the fact that user experience is becoming an important part of SEO. As search engines collect more data, they are constantly learning to interpret it. As they get better at it, retaining users on your pages for a certain time period (maybe a benchmark for an industry) may become an important factor in the SEO game.

Behavior feedback is currently used in personalized search.
&lt;ref&gt;{{cite web|url=http://www.seochat.com/c/a/search-engine-news/the-history-of-search-and-search-technology/|accessdate=1 June 2014}}&lt;/ref&gt;
--&gt;

==See also==
*[[Database search engine]]
*[[Enterprise search]]
*[[Search engine]]
*[[Disambiguation]]
*[[Search engine indexing]]
*[[Web crawler]]
*[[Structured Search]]

==External links==
* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]
* [http://www.urbandictionary.com/define.php?term=Searchency Searchency]

==References==
{{reflist}}

{{DEFAULTSORT:Search Engine Technology}}
[[Category:Internet search engines]]
[[Category:Information retrieval]]</text>
      <sha1>abofnbiczubxv2s87nbig8vznfr9jyo</sha1>
    </revision>
  </page>
  <page>
    <title>Temporal information retrieval</title>
    <ns>0</ns>
    <id>35804330</id>
    <revision>
      <id>633620507</id>
      <parentid>633620197</parentid>
      <timestamp>2014-11-13T03:50:29Z</timestamp>
      <contributor>
        <ip>79.169.103.150</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="81935">'''Temporal Information Retrieval (T-IR)''' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.

According to [[information theory]] science (Metzger, 2007),&lt;ref name=&quot;Metzger2007&quot;&gt;{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078–2091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}&lt;/ref&gt; timeliness or currency is one of the key five aspects that determine a document’s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company’s earnings or information on already-happened or invalid predictions.

T-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.

This page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.

== Temporal dynamics (T-dynamics) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza, Y.''' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar &amp; A. Oliveira (Eds.), ''In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval'' (Vol. 2476/2002, pp.&amp;nbsp;117 – 130). Lisbon, Portugal. September 11–13: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||
|-
|'''Cho, J., &amp; Garcia-Molina, H.''' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. ''In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]'', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||
|-
| '''Fetterly, D., Manasse, M., Najork, M., &amp; Wiener, J.''' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. ''In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference'' (pp.&amp;nbsp;669 – 678). Budapest, Hungary. May 20–24: ACM Press. || 2003 || WWW || T-Dynamics ||
|-
| '''Ntoulas, A., Cho, J., &amp; Olston, C.''' (2004). [http://dl.acm.org/citation.cfm?id=988674 What's New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&amp;nbsp;1 – 12). New York, NY, United States. May 17–22: ACM Press. || 2004 || WWW || T-Dynamics ||
|-
| '''Vlachos, M., Meek, C., Vagena, Z., &amp; Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., &amp; Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Bordino, I., Boldi, P., Donato, D., Santini, M., &amp; Vigna, S.''' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=4734022&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&amp;nbsp;909 – 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||
|-
| '''Adar, E., Teevan, J., Dumais, S. T., &amp; Elsas, J. L.''' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;282 – 291). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || T-Dynamics ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Elsas, J. L., &amp; Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Aji, A., Agichtein, E.''' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. ''In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, &amp; Prediction'' (pp.&amp;nbsp;273 – 281). Washington DC, United States. March 30–31: Springer-Verlag. || 2010 || SBP || T-Dynamics ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., &amp; Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] ''In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference''. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||
|-
| '''Campos, R., Jorge, A., &amp; Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: ''In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information'' (pp.&amp;nbsp;1171 – 1172). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || T-Dynamics ||
|-
| '''Dias, G., Campos, R., &amp; Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, &amp; H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&amp;nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Yeung, C.-m. A., &amp; Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||
|-
| '''Costa, M., &amp; Silva, M. J., &amp; Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&amp;nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal markup languages (T-MLanguages) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Setzer, A., &amp; Gaizauskas, R.''' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. ''In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation''. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||
|-
| '''Setzer, A.''' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||
|-
| '''Ferro, L., Mani, I., Sundheim, B., &amp; Wilson, G.''' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||
|-
| '''Pustejovsky, J., Castaño, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.''' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. ''In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics'', (pp.&amp;nbsp;28 – 34). Tilburg, Netherlands. January 15–17. || 2003 || IWCS || T-MLanguages ||
|-
| '''Ferro, L., Gerber, L., Mani, I., Sundheim, B., &amp; Wilson, G.''' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||
|}

== Temporal taggers (T-taggers) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - '''Mani, I., &amp; Wilson, G.''' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. ''In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics'' (pp.&amp;nbsp;69 – 76). Hong Kong, China. October 1–8: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||
|-
| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - '''Cunningham, H., Maynard, D., Bontcheva, K., &amp; Tablan, V.''' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. ''In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics'' (pp.&amp;nbsp;168 – 175). Philadelphia, PA, United States. July 6–12: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||
|-
| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., &amp; Gertz, M.''' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&amp;acc=OPEN&amp;CFID=82473711&amp;CFTOKEN=13661527&amp;__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. ''In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics'', (pp.&amp;nbsp;321 – 324). Uppsala, Sweden. July 11–16.|| 2010 || ACL - SemEval || T-Taggers ||
|-
| [http://www.timen.org/ TIMEN] '''Llorens, H., Derczynski, L., Gaizauskas, R. &amp; Saquete, E.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| '''Chang, A., &amp; Manning, C.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., &amp; Gertz, M.''' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. ''In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation'', 1 - 30.|| 2012 || LRE || T-Taggers ||
|-
| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - '''Filannino, M., Brown, G. &amp; Nenadic G.''' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. ''In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)'', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]
|}

== Temporal indexing (T-indexing) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Berberich, K., Bedathur, S., Neumann, T., &amp; Weikum, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. ''In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;519 – 526). Amsterdam, Netherlands. July 23–27: ACM Press. || 2007 || SIGIR || W-Archives ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Song, S., &amp; JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Arikan, I., Bedathur, S., &amp; Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. ''In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management'', (pp.&amp;nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. ''In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;235 – 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||
|}

== Temporal query understanding (TQ-understanding) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Vlachos, M., Meek, C., Vagena, Z., &amp; Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., &amp; Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Dakka, W., Gravano, L., &amp; Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Diaz, F.''' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;182 – 191). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || TQ-Understanding ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''König, A.''' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;347 – 354). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., &amp; Zheng, Z.''' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. ''In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing'' (pp.&amp;nbsp;1129 – 1139). Massachusetts, United States. October 9–11: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., &amp; Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. ''In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (p.&amp;nbsp;1325). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., &amp; Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;1171 – 1172). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2169103&amp;CFID=102654836&amp;CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&amp;nbsp;41 – 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||
|-
| '''Shokouhi, M., &amp; Radinsky, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. ''In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;601 – 610). Portland, United States. August 12–16.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2398567&amp;dl=ACM&amp;coll=DL&amp;CFID=204979644&amp;CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;2035 – 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., Dias, G., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&amp;nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Time-aware retrieval/ranking models (T-RModels) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Li, X., &amp; Croft, B. W.''' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. ''In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;469 – 475). New Orleans, Louisiana, United States. November 2–8: ACM Press. || 2003 || CIKM || T-RModels ||
|-
| '''Sato, N., Uehara, M., &amp; Sakai, Y.''' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber=1232026&amp;contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. ''In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications'' (pp.&amp;nbsp;215 – 220). Prague, Czech Republic. September 1–5: IEEE. || 2003 || DEXA || T-RModels ||
|-
| '''Berberich, K., Vazirgiannis, M., &amp; Weikum, G.''' (2005). [http://projecteuclid.org/DPubS?verb=Display&amp;version=1.0&amp;service=UI&amp;handle=euclid.im/1150474885&amp;page=record Time-Aware Authority Ranking]. ''In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&amp;linktype=44 IM: Internet Mathematics]'', 2(3), 301 - 332. || 2005 || IM || T-RModels ||
|-
| '''Cho, J., Roy, S., &amp; Adams, R.''' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;551 – 562). Baltimore, United States. June 13–16: ACM Press. || 2005 || SIGMOD || T-RModels ||
|-
| '''Perkiö, J., Buntine, W., &amp; Tirri, H.''' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. ''In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;647 – 648). Salvador, Brazil. August 15–16: ACM Press. || 2005 || SIGIR || T-RModels ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Dakka, W., Gravano, L., &amp; Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Arikan, I., Bedathur, S., &amp; Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Zhang, R., Chang, Y., Zheng, Z., Metzler, D., &amp; Nie, J.-y.''' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. ''In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies'', (pp.&amp;nbsp;165 – 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Elsas, J. L., &amp; Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.''' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] ''In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;629 – 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Berberich, K., Bedathur, S., Alonso, O., &amp; Weikum, G.''' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), ''In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval'' (Vol. 5993/2010, pp.&amp;nbsp;13 – 25). Milton Keynes, UK. March 28–31: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||
|-
| '''Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., &amp; Zha, H.''' (2010). [http://dl.acm.org/citation.cfm?id=1772725&amp;dl=ACM&amp;coll=DL&amp;CFID=204979644&amp;CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;331 – 340). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || T-RModels ||
|-
| '''Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., &amp; Zheng, Z.''' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. ''In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence'' (pp.&amp;nbsp;331 – 340). Atlanta, United States. June 11–15: AAAI Press. || 2010 || AAAI || T-RModels ||
|-
| '''Dai, N., &amp; Davison, B.''' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. ''In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;114 – 121). Geneve, Switzerland. July 19–23: ACM Press. || 2010 || SIGIR || T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Efron, M., &amp; Golovchinsky, G.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;495 – 504). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Dai, N., Shokouhi, M., &amp; Davison, B. D.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;95 – 104). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Kanhabua, N., Blanco, R., &amp; Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&amp;dl=ACM&amp;coll=DL&amp;CFID=102654836&amp;CFTOKEN=48651941 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., &amp; Cheng, P-J.''' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. ''In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;1101 – 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||
|-
| '''Efron, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;2463 – 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||
|-
| '''Kim G., and Xing E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Costa, M., &amp; Silva, M. J., &amp; Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&amp;nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal clustering (T-clustering) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., &amp; Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&amp;nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&amp;nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A.''' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&amp;nbsp;292 – 296). Funchal - Madeira, Portugal. October 6–8. || 2009 || KDIR || T-Clustering ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Campos, R., Jorge, A., Dias, G., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&amp;nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Temporal text classification (T-classification) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Jong, F., Rode, H., &amp; Hiemstra, D.''' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. ''In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing'' (pp.&amp;nbsp;161 – 168). Amsterdam, Netherlands. September 14–17 || 2005 || AHC || T-Classification ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What's Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. ''In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference'' (pp.&amp;nbsp;233 – 241). Edinburgh, Scotland. May 23–26: ACM Press. || 2006 || WWW || T-Classification ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management'' (pp.&amp;nbsp;129 – 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Jatowt, A., Kawai, Y., &amp; Tanaka, K.''' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management'' (pp.&amp;nbsp;137 – 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. ''In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 5173/2008, pp.&amp;nbsp;358 – 370). Aarhus, Denmark. September 14–19: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Strötgen, J., Alonso, O., &amp; Gertz, M.''' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&amp;nbsp;33 – 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||
|-
| '''Filannino, M., and Nenadic, G.''' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. ''In Proceedings of the First AHA!-Workshop on Information Discovery in Text'' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7–13. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]
|}

== Temporal visualization (T-interfaces) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Swan, R., &amp; Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Swan, R., &amp; Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, &amp; N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&amp;nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| [http://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||
|-
| '''Cousins, S., &amp; Kahn, M.''' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) ''In AIM: Artificial Intelligence in Medicine'', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||
|-
| '''Karam, G. M.''' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), ''ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering'' (pp.&amp;nbsp;125 – 137). Seattle, Washington, United States. August 17–19: ACM Press. || 1994 || ISSTA || T-Interfaces ||
|-
| '''Plaisant, C., Miiash, B., Rose, A., Widoff, S., &amp; Shneiderman, B.''' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. ''In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'' (pp.&amp;nbsp;221 – 227). Vancouver, British Columbia, Canada. April 13–18: ACM Press. || 1996 || CHI || T-Interfaces ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., &amp; Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In ''D. Wolber, N. Calder, &amp; ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&amp;nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Catizone, R., Dalli, A., &amp; Wilks, Y.''' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. ''In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation''. Genoa, Italy. May 24–26: ELDA. || 2006 || LREC || T-Interfaces ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, Y., &amp; Tanaka, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. ''In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference'' (pp.&amp;nbsp;1221 – 1222). Beijing, China. April 21–25: ACM Press. || 2008 || WWW || W-Archives ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. ''In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis''. Porto, Portugal. September 8–10: ACM Press. || 2008 || WikiSym || T-Interfaces ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. ''In L. S. Lopes, N. Lau, P. Mariano, &amp; L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'', (pp.&amp;nbsp;601 – 604). Aveiro, Portugal. October 12–15. || 2009 || EPIA || T-Interfaces ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., &amp; Shneiderman, B.''' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. ''In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing'', (pp.&amp;nbsp;549 – 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||
|}

== Temporal search engines (T-SEngine) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|}

== Temporal question answering (T-QAnswering) ==
{| class=&quot;wikitable sortable&quot;
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|}

== Temporal snippets (T-snippets) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. ''In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference''. Madrid, Spain. April 20–24: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. ''In R. Grossi, F. Sebastiani, &amp; F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval'' (Vol. 7024, pp.&amp;nbsp;26 – 31). Pisa, Italy. October 17–21.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||
|-
| '''Svore, K. M., Teevan, J., Dumais, S. T., &amp; Kulkarni, A.''' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;1045 – 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||
|}

== Future information retrieval (F-IRetrieval) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza-Yates, R.''' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. ''In S. Dominich, I. Ounis, &amp; J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Salvador, Brazil. August 15–19: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&amp;nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Dias, G., Campos, R., &amp; Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Kanhabua, N., Blanco, R., &amp; Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&amp;dl=ACM&amp;coll=DL&amp;CFID=82290723&amp;CFTOKEN=53881602 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Kanazawa, K., Jatowt, A., &amp; Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. ''In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;278 – 283). Lyon, France. August 22–27: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, &amp; H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&amp;nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Weerkamp, W., &amp; Rijke, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||
|-
| '''Radinski, K., &amp; Horvitz, E.''' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;255 – 264). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || F-IRetrieval ||
|}

== Temporal image retrieval (T-IRetrieval) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Dias, G., Moreno, J. G., Jatowt, A., &amp; Campos, R.''' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calderón-Benavides, L., González-Caro, C., Chávez, E., Ziviani, N. (Eds.), ''In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval'' (Vol. 7608/2012, pp.&amp;nbsp;199 – 204). Cartagena de Indias, Colombia. October 21–25: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||
|-
| '''Palermo, F., Hays, J., &amp; Efros, A.''' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), ''In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision'' (Vol. 7577/2012, pp.&amp;nbsp;499 – 512). Firenze, Italy. October 07–13: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||
|-
| '''Kim, G., &amp; Xing, E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Martin, P., Doucet, A., &amp; Jurie, F.''' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. ''In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval'' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||
|}

== Collective memory (C-memory) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Surowiecki, J.''' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||
|-
| '''Hall, D., Jurafsky, D., &amp; Manning, C. D.''' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. ''In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing'' (pp.&amp;nbsp;363 – 371). Waikiki, Honolulu, Hawaii. October 25–27: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||
|-
| '''Shahaf, D., &amp; Guestrin, C.''' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&amp;nbsp;623 – 632). Washington, United States. July 25–28: ACM Press. || 2010 || KDD || C-Memory ||
|-
| '''Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., &amp; Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. ''In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;83 – 92). Eindhoven, Netherlands. June 6–9: ACM Press. || 2011 || HT || C-Memory ||
|-
| '''Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.''' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||
|-
| '''Yeung, C.-m. A., &amp; Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||
|}

== Web archives (W-archives) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||
|-
| '''Kahle, B.''' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&amp;ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&amp;ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. ''In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]'', 276(3), pp.&amp;nbsp;72 – 73. || 1997 || SAM || W-Archives ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., &amp; Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In ''D. Wolber, N. Calder, &amp; ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&amp;nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., &amp; Tanaka, K.''' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. ''In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;135 – 144). Odense, Denmark. August 22–25: ACM Press. || 2006 || HT || W-Archives ||
|-
| '''Adar, E., Dontcheva, M., Fogarty, J., &amp; Weld, D. S.''' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. ''In S. B. Cousins, &amp; M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology'' (pp.&amp;nbsp;239 – 248). Monterey, CA, United States. October 19–22: ACM Press. || 2008 || UIST || W-Archives ||
|-
| '''Song, S., &amp; JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Gomes, D., Miranda, J., &amp; Costa, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. ''In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries'' (pp.&amp;nbsp;408 – 420). Berlin, Germany. September 25–29: Springer-Verlag || 2011 || TPDL || W-Archives ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;235 – 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||
||
|-
| '''Costa, M., &amp; Silva, M.J.''' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. ''In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering'', (pp.&amp;nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||
|}

== Topic detection and tracking (TDT) ==
{| class=&quot;wikitable sortable&quot;
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Allan, J., Carbonell, J., Doddington, G., &amp; Yamron, J.''' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&amp;nbsp;194 – 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||
|-
| '''Swan, R., &amp; Allan, J.''' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. ''In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;38 – 45). Kansas City, Missouri, United States. November 2–6: ACM Press. || 1999 || CIKM || TDT ||
|-
| '''Swan, R., &amp; Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, &amp; N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&amp;nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| '''Swan, R., &amp; Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Makkonen, J., &amp; Ahonen-Myka, H.''' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. ''In T. Koch, &amp; I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 2769/2004, pp.&amp;nbsp;393 – 404). Trondheim, Norway. August 17–22: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., &amp; Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&amp;nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Kim, P., Myaeng, S.H.''' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. ''In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing'' (pp.&amp;nbsp;227 – 242). New York, United States. || 2004 || TALIP || TDT ||
|}

==References==
{{reflist}}

[[Category:Information retrieval]]</text>
      <sha1>9p2t1obr8hpezy4wbdqifs9l5qw5ry0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Vector space model</title>
    <ns>14</ns>
    <id>36475839</id>
    <revision>
      <id>503023195</id>
      <timestamp>2012-07-18T21:29:09Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '[[Category:Information retrieval]]'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34">[[Category:Information retrieval]]</text>
      <sha1>dobw7kl3saam8hrak0oyyn1jbi7dqlz</sha1>
    </revision>
  </page>
  <page>
    <title>Vocabulary mismatch</title>
    <ns>0</ns>
    <id>36749242</id>
    <revision>
      <id>550428826</id>
      <parentid>549496533</parentid>
      <timestamp>2013-04-15T05:51:33Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor/>
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q7939178]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3131">'''Vocabulary mismatch''' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.

Furnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.&lt;ref&gt;Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.&lt;/ref&gt;  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].

The vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)&lt;ref&gt;Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.&lt;/ref&gt; were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.&lt;ref name=&quot;cnf&quot;&gt;Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.&lt;/ref&gt;

== Techniques that solve mismatch ==
Zhao provided a survey of common techniques that can solve mismatch in the dissertation on term mismatch.&lt;ref&gt;Zhao, L., Modeling and Solving Term Mismatch in Full-text Retrieval, PhD Dissertation, Carnegie Mellon University, 2012. [http://www.cs.cmu.edu/~lezhao/thesis/diss-Le.pdf URL] retrieved 9/3/2012.&lt;/ref&gt;

===Stemming===

===Full-text indexing versus only indexing keywords or abstracts===

===Usages of inlink anchor text or other social tagging===

===Query expansion===
A recent study by Zhao and Callan (2012)&lt;ref name=&quot;cnf&quot;/&gt; using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].

A wiki website called [http://www.wikiquery.org WikiQuery] has been developed by one of the authors of the above study, which helps users create, store and share effective Conjunctive normal form queries.

===Translation based models===

== References ==

{{Reflist}}

[[language code:Title]]

[[Category:Linguistic research]]
[[Category:Information retrieval]]
[[Category:Natural language processing]]</text>
      <sha1>jlvyafxbzz25lr6kmo4pzay6t867tqd</sha1>
    </revision>
  </page>
  <page>
    <title>Pleade</title>
    <ns>0</ns>
    <id>35952152</id>
    <revision>
      <id>597534932</id>
      <parentid>558613596</parentid>
      <timestamp>2014-02-28T15:52:47Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <comment>/* Features */clean up, [[WP:AWB/T|typo(s) fixed]]: etc  → etc. using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5221">{{Infobox software
| name                   = Pleade-infoxbox
| title                  = Pleade
| logo                   = [[File:Pleade-logo.png]]
| logo caption           = Logo de Pleade
| screenshot             = &lt;!-- [[File: ]] --&gt;
| caption                = 
| collapsible            = 
| author                 = AJLSM
| developer              = AJLSM
| released               = &lt;!-- {{Start date|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 3.4
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = &lt;!-- 3.5 --&gt;
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]
| operating system       = [[Unix-like]], [[Microsoft Windows]]
| platform               = 
| size                   = 
| language               = French, English, German, Chinese
| language count         = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| language footnote      = 
| status                 = Active
| genre                  = Digital Library
| license                = GNU General Public License
| alexa                  = 
| website                = {{URL|http://www.pleade.com/}}
}}

'''Pleade''' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[SDX]] platform, it is a very flexible web application.

== History ==
The software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.&lt;ref&gt;[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]&lt;/ref&gt;

==Technologies==
Pleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.

It is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[METS]] and [[ALTO (XML)|ALTO]] is under active development.&lt;ref&gt;[http://pleade.com/ Pleade 2012 : les imprimés numérisés et les formats XML METS / ALTO]&lt;/ref&gt;

== Features ==
* Customizable publication ;
* Customizable index creation ;
* Customizable search form ;
* Simple and advanced search among publish documents ;
* Federate search among different bases (e.g. EAD, METS) ;
* basket (for database and for images), a search history, printing, etc. ;
* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;
* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;
* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;
* Printing resulting and finding aids as PDF documents (with embedded images) ;
* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;
* Ability to import metadata from an [[Integrated library system|ILS]].

=== Pleade-Entreprise ===
* Pleade-Entreprise extended features to others XML format, such as [[METS]] and [[ALTO (XML)|ALTO]].

== Examples ==
These are examples of websites based on Pleade:
{{columns-list|2|
* Archival portals
** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]
** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]
** [http://odysseo.org/ Odysseo: Resources for the history of immigration]
** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]
** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]
** [http://jubilotheque.upmc.fr/ Jubilothèque, UPMC's scientific digital library]
** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault's Library &quot;les Mots et les Choses&quot; ENS]

* Portals documentary
** [http://www.michael-culture.org/fr/home Michael]
** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]

* Digital Libraries
** Digital Library of Lille
** Lille III
** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]
}}

== Related resources ==
* {{Official website|http://pleade.com}}
* [http://demo.pleade.com Official demo]
* [http://www.pleadeenpratique.org/ Pleade in practice]
* [http://www.ajlsm.com/produits/sdx SDX]
* [http://www.ajlsm.com AJLSM company]

== References ==
&lt;references/&gt;

[[Category:Digital library software]]
[[Category:Free software]]
[[Category:Information retrieval]]
[[Category:Archival science]]</text>
      <sha1>0qbiobhutg18hmoo7it25qxe8otmz2q</sha1>
    </revision>
  </page>
  <page>
    <title>PolySpot</title>
    <ns>0</ns>
    <id>26246477</id>
    <revision>
      <id>648126913</id>
      <parentid>647131180</parentid>
      <timestamp>2015-02-21T02:00:20Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor/>
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q1886006]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2213">{{Infobox company
| name = PolySpot
| logo = [[File:PolySpot-Logo.jpg|300px]]
| type = [[Privately held company|Private]]
| foundation = [[Paris]] (2001)
| location = [[Paris]], [[London]]
| key_people = Guy Mounier, CEO
| industry = [[Information technology]] &lt;br/&gt; [[Unified Information Access]] &lt;br/&gt; [[Search Engine]]
| products = PolySpot Infowarehouse&lt;br/&gt;PolySpot Information At Work&lt;br/&gt;PolySpot Enterprise Search
| slogan = Open Search Solutions
| homepage = [http://www.polyspot.com/en/ www.polyspot.com]
}}
'''PolySpot''' is a subsidiary of CustomerMatrix, an [[enterprise search]] [[ISV|software company]].

Created in 2001, PolySpot has its headquarters in Paris, France. It also has offices in the United Kingdom.

In 2011, PolySpot raised EUR 2.5m from [[Newfund]].&lt;ref&gt;{{cite web|url=http://finance.yahoo.com/news/PolySpot-Raises-2-Million-prnews-4194799492.html| title=Yahoo News: PolySpot Raises 2 Million}}&lt;/ref&gt; and True Global Ventures.

In 2013, CustomerMatrix (US) acquired PolySpot. In December 2013, PolySpot reduced its shareholders equity from 507,209 EUR to 206,120 EUR.

== Functionalities ==

PolySpot's infrastructure provide a unified information access to all data, through which users can instantly interact with all available information resources, both inside and outside the company, and regardless of whether or how the data are structured.

PolySpot's indexing capabilities are based on the [[Apache Software Foundation|Apache]] [[Lucene]] [[Solr]] Java-based open-source projects.

==References==
{{Portal|Software}}
{{Reflist|colwidth=30em}}

==External links==
*[http://www.polyspot.com/en/ Website]
*[http://www.customermatrix.com/ CustomerMatrix Website]
*[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=38687243 Company profile in BusinessWeek]
*[http://www.arnoldit.com/search-wizards-speak/polyspot-2.html PolySpot in ArnoldIT]
*[http://arnoldit.com/wordpress/2010/01/13/polyspot-lands-crdit-agricole-sa/ PolySpot lands Credit Agricole in ArnoldIT]

{{DEFAULTSORT:Polyspot}}
[[Category:Searching]]
[[Category:Search engine software|*Enterprise search vendors]]
[[Category:Internet search engines]]
[[Category:Information retrieval]]</text>
      <sha1>juvu6xqh4n6n1p5p4x82hmxitezwduu</sha1>
    </revision>
  </page>
  <page>
    <title>Okapi BM25</title>
    <ns>0</ns>
    <id>9511414</id>
    <revision>
      <id>646176876</id>
      <parentid>644532017</parentid>
      <timestamp>2015-02-08T11:47:55Z</timestamp>
      <contributor>
        <username>HelpUsStopSpam</username>
        <id>24038232</id>
      </contributor>
      <comment>Literature templates.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7807">In [[information retrieval]], '''Okapi BM25''' is a [[ranking function]] used by [[search engine]]s to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query. It is based on the [[Probabilistic relevance model|probabilistic retrieval framework]] developed in the 1970s and 1980s by [[Stephen E. Robertson]], [[Karen Spärck Jones]], and others.

The name of the actual ranking function is BM25. To set the right context, however, it usually referred to as &quot;Okapi BM25&quot;, since the Okapi information retrieval system, implemented at [[London]]'s [[City University, London|City University]] in the 1980s and 1990s, was the first system to implement this function.

BM25, and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art [[TF-IDF]]-like retrieval functions used in document retrieval, such as [[web search]].

== The ranking function ==

BM25 is a [[Bag of words model|bag-of-words]] retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). It is not a single function, but actually a whole family of scoring functions, with slightly different components and parameters. One of the most prominent instantiations of the function is as follows.

Given a query &lt;math&gt;Q&lt;/math&gt;, containing keywords &lt;math&gt;q_1, ..., q_n&lt;/math&gt;, the BM25 score of a document &lt;math&gt;D&lt;/math&gt; is:

:&lt;math&gt; \text{score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})},&lt;/math&gt;

where &lt;math&gt;f(q_i, D)&lt;/math&gt; is &lt;math&gt;q_i&lt;/math&gt;'s [[term frequency]] in the document &lt;math&gt;D&lt;/math&gt;, &lt;math&gt;|D|&lt;/math&gt; is the length of the document &lt;math&gt;D&lt;/math&gt; in words, and &lt;math&gt;avgdl&lt;/math&gt; is the average document length in the text collection from which documents are drawn. &lt;math&gt;k_1&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are free parameters, usually chosen, in absence of an advanced optimization, as &lt;math&gt;k_1 \in [1.2,2.0]&lt;/math&gt; and &lt;math&gt;b = 0.75&lt;/math&gt;.&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. ''An Introduction to Information Retrieval'', Cambridge University Press, 2009, p. 233.&lt;/ref&gt; &lt;math&gt;\text{IDF}(q_i)&lt;/math&gt; is the IDF ([[inverse document frequency]]) weight of the query term &lt;math&gt;q_i&lt;/math&gt;. It is usually computed as:

:&lt;math&gt;\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5},&lt;/math&gt;

where &lt;math&gt;N&lt;/math&gt; is the total number of documents in the collection, and &lt;math&gt;n(q_i)&lt;/math&gt; is the number of documents containing &lt;math&gt;q_i&lt;/math&gt;.

There are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the [[Binary Independence Model]].

Please note that the above formula for IDF shows potentially major drawbacks when using it for terms appearing in more than half of the corpus documents. These terms' IDF is negative, so for any two almost-identical documents, one which contains the term and one which does not contain it, the latter will possibly get a larger score.
This means that terms appearing in more than half of the corpus will provide negative contributions to the final document score. This is often an undesirable behavior, so many real-world applications would deal with this IDF formula in a different way:

* Each summand can be given a floor of 0, to trim out common terms;
* The IDF function can be given a floor of a constant &lt;math&gt;\epsilon&lt;/math&gt;, to avoid common terms being ignored at all;
* The IDF function can be replaced with a similarly shaped one which is non-negative, or strictly positive to avoid terms being ignored at all.

== IDF information theoretic interpretation ==
Here is an interpretation from information theory. Suppose a query term &lt;math&gt;q&lt;/math&gt; appears in &lt;math&gt;n(q)&lt;/math&gt; documents. Then a randomly picked document &lt;math&gt;D&lt;/math&gt; will contain the term with probability &lt;math&gt;\frac{n(q)}{N}&lt;/math&gt; (where &lt;math&gt;N&lt;/math&gt; is again the cardinality of the set of documents in the collection). Therefore, the [[information]] content of the message &quot;&lt;math&gt;D&lt;/math&gt; contains &lt;math&gt;q&lt;/math&gt;&quot; is:

:&lt;math&gt;-\log \frac{n(q)}{N} = \log \frac{N}{n(q)}.&lt;/math&gt;

Now suppose we have two query terms &lt;math&gt;q_1&lt;/math&gt; and &lt;math&gt;q_2&lt;/math&gt;. If the two terms occur in documents entirely independently of each other, then the probability of seeing both &lt;math&gt;q_1&lt;/math&gt; and &lt;math&gt;q_2&lt;/math&gt; in a randomly picked document &lt;math&gt;D&lt;/math&gt; is:

:&lt;math&gt;\frac{n(q_1)}{N} \cdot \frac{n(q_2)}{N},&lt;/math&gt;

and the information content of such an event is:

:&lt;math&gt;\sum_{i=1}^{2} \log \frac{N}{n(q_i)}.&lt;/math&gt;

With a small variation, this is exactly what is expressed by the IDF component of BM25.

== Modifications ==
* At the extreme values of the coefficient &lt;math&gt;b&lt;/math&gt; BM25 turns into ranking functions known as '''BM11''' (for &lt;math&gt;b=1&lt;/math&gt;) and '''BM15''' (for &lt;math&gt;b=0&lt;/math&gt;).&lt;ref&gt;http://xapian.org/docs/bm25.html&lt;/ref&gt;
* '''BM25F'''&lt;ref&gt;Hugo Zaragoza, Nick Craswell, Michael Taylor, Suchi Saria, and Stephen Robertson. [http://trec.nist.gov/pubs/trec13/papers/microsoft-cambridge.web.hard.pdf ''Microsoft Cambridge at TREC-13: Web and HARD tracks.''] In Proceedings of TREC-2004.&lt;/ref&gt;  is a modification of BM25 in which the document is considered to be composed from several fields (such as headlines, main text, anchor text) with possibly different degrees of importance.
* '''BM25+'''&lt;ref&gt;Yuanhua Lv and ChengXiang Zhai. [http://sifaka.cs.uiuc.edu/~ylv2/pub/cikm11-lowerbound.pdf ''Lower-bounding term frequency normalization.''] In Proceedings of CIKM'2011, pages 7-16.&lt;/ref&gt; is an extension of BM25. BM25+ was developed to address one deficiency of the standard BM25 in which the component of term frequency normalization by document length is not properly lower-bounded; as a result of this deficiency, long documents which do match the query term can often be scored unfairly by BM25 as having a similar relevancy to shorter documents that do not contain the query term at all. The scoring formula of BM25+ only has one additional free parameter &lt;math&gt;\delta&lt;/math&gt; (a default value is &lt;math&gt;1.0&lt;/math&gt; in absence of a training data) as compared with BM25:

:&lt;math&gt; \text{score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \left[ \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot (1 - b + b \cdot \frac{|D|}{\text{avgdl}})} + \delta \right]&lt;/math&gt;

== Footnotes ==
{{Reflist}}

== References ==
* {{cite conference|author=Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford | title=Okapi at TREC-3 | conference=[http://trec.nist.gov/pubs/trec3/t3_proceedings.html Proceedings of the Third Text REtrieval Conference (TREC 1994)]|location=Gaithersburg, USA|date=November 1994|url=http://trec.nist.gov/pubs/trec3/papers/city.ps.gz}}

* {{cite conference|author=Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu|title=Okapi at TREC-7|conference=[http://trec.nist.gov/pubs/trec7/t7_proceedings.html Proceedings of the Seventh Text REtrieval Conference]|location=Gaithersburg, USA|date=November 1998|url=http://trec.nist.gov/pubs/trec7/papers/okapi_proc.pdf.gz}}

* {{cite doi|10.1016/S0306-4573(00)00015-7}}

* {{cite doi|10.1016/S0306-4573(00)00016-9}}

== External links ==
* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}

[[Category:Ranking functions]]
[[Category:Information retrieval]]</text>
      <sha1>jrbpb1u4cbyjar4ryi9mpbx7zohdl2j</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Personalized search</title>
    <ns>14</ns>
    <id>38954543</id>
    <revision>
      <id>566385748</id>
      <parentid>547605117</parentid>
      <timestamp>2013-07-30T05:57:37Z</timestamp>
      <contributor>
        <ip>50.136.247.190</ip>
      </contributor>
      <comment>del  Category:Internet terminology - not terminology articles</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="86">{{catmore}}

[[Category:Information retrieval]]
[[Category:Internet search engines| ]]</text>
      <sha1>lc89lb5fv1svtu5ayr0h2culax2csd9</sha1>
    </revision>
  </page>
  <page>
    <title>Sørensen–Dice coefficient</title>
    <ns>0</ns>
    <id>9701718</id>
    <revision>
      <id>636133765</id>
      <parentid>631901506</parentid>
      <timestamp>2014-12-01T06:31:26Z</timestamp>
      <contributor>
        <ip>146.232.212.49</ip>
      </contributor>
      <comment>/* Formula */ removed redundancy regarding range</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8227">The '''Sørensen–Dice index''', also known by other names (see Names, below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Sørensen]]&lt;ref&gt;{{cite journal |last=Sørensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1–34 |doi= }}&lt;/ref&gt; and [[Lee Raymond Dice]],&lt;ref&gt;{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297–302 |doi=10.2307/1932409 }}&lt;/ref&gt; who published in 1948 and 1945 respectively.

==Name==
The index is known by several other names, usually '''Sørensen index''' or '''Dice's coefficient'''. Both names also see &quot;similarity coefficient&quot;, &quot;index&quot;, and other such variations. Common alternate spellings for Sørensen are Sorenson, Soerenson index and Sörenson index, and all three can also be seen with the –sen ending.

Other names include:
*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index&lt;ref name =&quot;gallagher&quot;/&gt;

==Quantitative version==
The expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:
* Quantitative Sørensen–Dice index&lt;ref name =&quot;gallagher&quot;/&gt;
* Quantitative Sørensen index&lt;ref name =&quot;gallagher&quot;/&gt;
* Quantitative Dice index&lt;ref name =&quot;gallagher&quot;/&gt;
* [[Bray Curtis dissimilarity|Bray-Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')&lt;ref name =&quot;gallagher&quot;/&gt;
* [[Jan Czekanowski|Czekanowski]]'s quantitative index&lt;ref name =&quot;gallagher&quot;/&gt;
* Steinhaus index&lt;ref name =&quot;gallagher&quot;/&gt;
* [[E. C. Pielou|Pielou]]'s percentage similarity&lt;ref name =&quot;gallagher&quot;/&gt;
* 1 minus the [[Hellinger distance]]&lt;ref&gt;{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1948 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326–349 |doi=10.2307/1942268 }}&lt;/ref&gt;

==Formula==
Sørensen's original formula was intended to be applied to presence/absence data, and is

:&lt;math&gt; QS = \frac{2C}{A + B} = \frac{2 |A \cap B|}{|A| + |B|}&lt;/math&gt;

where ''A'' and ''B'' are the number of species in samples A and B, respectively, and ''C'' is the number of species shared by the two samples; QS is the quotient of similarity and ranges between 0 and 1.&lt;ref&gt;http://www.sekj.org/PDF/anbf40/anbf40-415.pdf&lt;/ref&gt;

It can be viewed as a similarity measure over sets:

:&lt;math&gt;s = \frac{2 | X \cap Y |}{| X | + | Y |} &lt;/math&gt;

Similarly to Jaccard, the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':

&lt;math&gt;s_v = \frac{2 | A \cdot B |}{| A |^2 + | B |^2} &lt;/math&gt;

which gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.

For sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :&lt;ref&gt;{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979
|title=Information Retrieval
|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}&lt;/ref&gt;

When taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:&lt;ref&gt;{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003
|title=Cognates Can Improve Statistical Translation Models
|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics
|pages=46–48 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}&lt;/ref&gt;

:&lt;math&gt;s = \frac{2 n_t}{n_x + n_y}&lt;/math&gt;

where ''n''&lt;sub&gt;''t''&lt;/sub&gt; is the number of character bigrams found in both strings, ''n''&lt;sub&gt;''x''&lt;/sub&gt; is the number of bigrams in string ''x'' and ''n''&lt;sub&gt;''y''&lt;/sub&gt; is the number of bigrams in string ''y''. For example, to calculate the similarity between:

:&lt;code&gt;night&lt;/code&gt;
:&lt;code&gt;nacht&lt;/code&gt;

We would find the set of bigrams in each word:
:{&lt;code&gt;ni&lt;/code&gt;,&lt;code&gt;ig&lt;/code&gt;,&lt;code&gt;gh&lt;/code&gt;,&lt;code&gt;ht&lt;/code&gt;}
:{&lt;code&gt;na&lt;/code&gt;,&lt;code&gt;ac&lt;/code&gt;,&lt;code&gt;ch&lt;/code&gt;,&lt;code&gt;ht&lt;/code&gt;}

Each set has four elements, and the intersection of these two sets has only one element: &lt;code&gt;ht&lt;/code&gt;.

Inserting these numbers into the formula, we calculate, ''s''&amp;nbsp;=&amp;nbsp;(2&amp;nbsp;·&amp;nbsp;1)&amp;nbsp;/&amp;nbsp;(4&amp;nbsp;+&amp;nbsp;4)&amp;nbsp;=&amp;nbsp;0.25.

==Difference from Jaccard ==
This coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.&lt;ref name =&quot;gallagher&quot;/&gt;

The function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function

:&lt;math&gt;d = 1 -  \frac{2 | X \cap Y |}{| X | + | Y |} &lt;/math&gt;

is not a proper distance metric as it does not possess the property of [[triangle inequality]].&lt;ref name =&quot;gallagher&quot;&gt;Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&amp;rep=rep1&amp;type=pdf COMPAH Documentation], University of Massachusetts, Boston&lt;/ref&gt; The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.

==Applications==
The Sørensen–Dice coefficient is mainly useful for ecological community data (e.g. Looman &amp; Campbell, 1960&lt;ref&gt;[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409–416.]&lt;/ref&gt;). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s&lt;ref&gt;[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123–131.]&lt;/ref&gt;). As compared to [[Euclidean distance]], Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.&lt;ref&gt;McCune, Bruce &amp; Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.&lt;/ref&gt;

==See also==
* [[Correlation]]
* [[Czekanowski similarity index]]
* [[Jaccard index]]
* [[Hamming distance]]
* [[Horn’s index]]
* [[Hurlbert’s index]]
* [[Kulczyński similarity index]]
* [[Pianka's index]]
* [[MacArthur and Levin's index]]
* [[Mantel test]]
* [[Morisita's overlap index]]
* [[Most frequent k characters]]
* [[Overlap coefficient]]
* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])
* [[Simplified Morisita’s index]]
* [[Tversky index]]
* [[Universal adaptive strategy theory (UAST)]]

==References==
{{reflist}}

==External links==
{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}
* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/DiceSorensenMetric.scala Dice / Sorensen] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]

{{DEFAULTSORT:Sorensen-Dice coefficient}}
[[Category:Information retrieval]]
[[Category:String similarity measures]]
[[Category:Measure theory]]</text>
      <sha1>ngy13g9ss6s5if4gjb5z2fljwb2qq0q</sha1>
    </revision>
  </page>
  <page>
    <title>Human–computer information retrieval</title>
    <ns>0</ns>
    <id>14473878</id>
    <revision>
      <id>638872709</id>
      <parentid>616084060</parentid>
      <timestamp>2014-12-20T05:30:28Z</timestamp>
      <contributor>
        <username>Dtunkelang</username>
        <id>5293022</id>
      </contributor>
      <comment>updated HCIR url, added CHIIR url</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10963">'''Human–computer information retrieval''' ('''HCIR''') is the study of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. The fields of [[human–computer interaction]] (HCI) and information retrieval (IR) have both developed innovative techniques to address the challenge of navigating complex information spaces, but their insights have often failed to cross disciplinary borders. Human–computer information retrieval has emerged in academic research and industry practice to bring together research in the fields of IR and HCI, in order to create new kinds of search systems that depend on continuous human control of the search process.

== History ==

This term ''human–computer information retrieval'' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.&lt;ref name=march2006&gt;Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science. Available online at http://www.asis.org/Bulletin/Jun-06/marchionini.html.&lt;/ref&gt; Marchionini’s main thesis is that &quot;HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy.&quot;

In 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[human–computer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]] – changes that were only embryonic in the late 1990s.

A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].

== What is HCIR? ==

HCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as &quot;the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system.&quot;&lt;ref name=ingwer1992&gt;Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham. Available online at http://vip.db.dk/pi/iri/index.htm.&lt;/ref&gt;

A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.&lt;ref&gt;{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}&lt;/ref&gt;

Most modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the document’s [[relevance]] to the query.&lt;ref&gt;Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. &lt;/ref&gt; In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information_retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).

Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]’s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information_retrieval#Precision|precision]] and [[Information_retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.&lt;ref name=koene1996&gt;Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 13–18, 1996). M. J. Tauber, Ed. CHI ‘96. ACM Press, New York, NY, 205-212. Available online at http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm.
&lt;/ref&gt; Other HCIR research, such as Pia Borlund’s IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.&lt;ref name=borlund2003&gt;Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152. Available online at http://informationr.net/ir/8-3/paper152.html.&lt;/ref&gt;

== Goals ==

Marchionini put forth the following goals towards a system where the user has more control in determining relevant results.&lt;ref name=march2006/&gt;

Systems should
*no longer only deliver the relevant documents, but must also provide semantic information along with those documents
*increase user responsibility as well as control; that is, information systems require human intellectual effort
*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases
*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services
*support the entire [[information life cycle]] (from creation to preservation) rather than only the dissemination or use phase
*support tuning by end users and especially by information professionals who add value to information resources
*be engaging and fun to use

In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process — the [[information professional]] — to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).

== Techniques ==

The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.

Many [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user’s hands.

[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.&lt;ref&gt;Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.&lt;/ref&gt;

[[Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and &quot;snippets&quot; of document text that are most pertinent to the words in the search query.

[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.&lt;ref&gt;Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.&lt;/ref&gt;

Summarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for &quot;java&quot; might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].

[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].

== References ==

&lt;References/&gt;

==External links==
*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}
*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}

{{DEFAULTSORT:Human-computer information retrieval}}
[[Category:Information retrieval]]
[[Category:Human–computer interaction]]</text>
      <sha1>12fgjlim0xwl3vqqdbjt3wkbrzhg3so</sha1>
    </revision>
  </page>
  <page>
    <title>Thesaurus (information retrieval)</title>
    <ns>0</ns>
    <id>39000674</id>
    <revision>
      <id>645505940</id>
      <parentid>611762707</parentid>
      <timestamp>2015-02-03T21:23:17Z</timestamp>
      <contributor>
        <username>Seresin</username>
        <id>3726169</id>
      </contributor>
      <comment>standardize hatnote</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9085">{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}

In the context of [[information retrieval]], a '''thesaurus''' (plural: &quot;thesauri&quot;) is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as &quot;any item that is to be described for inclusion in an information retrieval system, website, or other source of information&quot;.&lt;ref&gt;ANSI &amp; NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11&lt;/ref&gt; The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.&lt;ref&gt;ANSI &amp; NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12&lt;/ref&gt;

A thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a “controlled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.”

A thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.

== History ==
Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a “thesaurus” (by analogy with the well known thesaurus developed by [[Peter Roget]]).&lt;ref&gt;Roberts, N. The pre-history of the information retrieval thesaurus. ''Journal of Documentation'', 40(4), 1984, p.271-285.&lt;/ref&gt; The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.&lt;ref&gt;Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. ''Cataloging &amp; Classification Quarterly'', 37 (3/4), 2004, p.5-21.&lt;/ref&gt;&lt;ref&gt;Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. ''Libri'', 43(4), 1993, p.326-342.&lt;/ref&gt;

The first two of these lists to be published were the ''Thesaurus of ASTIA Descriptors'' (1960) and the ''Chemical Engineering Thesaurus'' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential ''Thesaurus of Engineering and Scientific Terms'' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented ''Thesaurus rules and conventions'' that have guided thesaurus construction ever since.
Hundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:
(a)	Extension from monolingual to multilingual capability; and 
(b)	Addition of a conceptually organized display to the basic alphabetical presentation.

Here we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:

* [[UNESCO]] ''Guidelines for the establishment and development of monolingual thesauri''. 1970 (followed by later editions in 1971 and 1981)
* DIN 1463 ''Guidelines for the establishment and development of monolingual thesauri''. 1972 (followed by later editions)
* ISO 2788 ''Guidelines for the establishment and development of monolingual thesauri''. 1974 (revised 1986)
* ANSI ''American National Standard for Thesaurus Structure, Construction, and Use''. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)
* ISO 5964 ''Guidelines for the establishment and development of multilingual thesauri''. 1985
* ANSI/NISO Z39.19 ''Guidelines for the construction, format, and management of monolingual thesauri''. 1993 (revised 2005 and renamed ''Guidelines for the construction, format, and management of monolingual controlled vocabularies''.)
* ISO 25964 ''Thesauri and interoperability with other vocabularies''. Part 1 (''Thesauri for information retrieval'' published 2011; Part 2 (''Interoperability with other vocabularies'') published 2013.

The most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.&lt;ref&gt;Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] ''Information standards quarterly'', 24(1), 2012, p.20-26.&lt;/ref&gt; Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.&lt;ref&gt;''[http://www.niso.org/schemas/iso25964/ ISO 25964 – the international standard for thesauri and interoperability with other vocabularies.]'' National Information Standards Organization, 2013.&lt;/ref&gt;

== Purpose ==
In information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.

The [[Art and Architecture Thesaurus|Art &amp; Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN’s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.

== Structure ==
Information retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, “citrus fruits” might be linked to the broader concept of “fruits”, and the narrower ones of “oranges”, “lemons”, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as “mad cow disease”, “bovine spongiform encephalopathy”, “BSE”, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.

== See also ==
* [[Controlled vocabulary]]
* [[ISO 25964]]
* [[Thesaurus]]

== References ==
{{Reflist}}

== External links ==
* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] 
* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]

[[Category:Information retrieval]]
[[Category:Thesauri]]</text>
      <sha1>1ile3gx65mei65n1l1fe1tozrfbv0ol</sha1>
    </revision>
  </page>
  <page>
    <title>Artificial Solutions</title>
    <ns>0</ns>
    <id>40218456</id>
    <revision>
      <id>621890787</id>
      <parentid>621890658</parentid>
      <timestamp>2014-08-19T09:01:13Z</timestamp>
      <contributor>
        <ip>37.1.169.181</ip>
      </contributor>
      <comment>added brackets</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9473">{{Infobox company
|name= Artificial Solutions
|logo=[[Image:Artificial Solutions Logo.png]]
|type=[[Private company]]
|foundation=(2001)
|founder=Johan Åhlund, Johan Gustavsson and Michael Söderström 
|location=[[Stockholm]], [[Sweden]]
|locations=Offices worldwide with R&amp;D centers in [[Barcelona]], [[Hamburg]], [[London]] and [[Stockholm]] 
|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], 
|products= Teneo platform
|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]
}}

'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.&lt;ref&gt;{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}&lt;/ref&gt; The company's natural language solutions have been deployed in a wide range of industries including finance,&lt;ref&gt;{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}&lt;/ref&gt; telecoms,&lt;ref&gt;{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjänst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}&lt;/ref&gt; the public sector,&lt;ref&gt;{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjänst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}&lt;/ref&gt; retail&lt;ref&gt;{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}&lt;/ref&gt; and travel.&lt;ref&gt;{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}&lt;/ref&gt;

==History==
Artificial Solutions was founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Åhlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.&lt;ref&gt;{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Löjlig affärside vinstlott för Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}&lt;/ref&gt;

The company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.&lt;ref&gt;{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}&lt;/ref&gt; Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.&lt;ref&gt;{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}&lt;/ref&gt;

In 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.&lt;ref&gt;{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}&lt;/ref&gt;
[[Elbot]], Artificial Solutions’ test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.&lt;ref&gt;[[Loebner Prize]]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &amp;#124; England &amp;#124; Berkshire &amp;#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}&lt;/ref&gt;

With a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.&lt;ref&gt;{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Så effektiv er Ikeas chat-robot: Har været på 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}&lt;/ref&gt;
In 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.&lt;ref&gt;{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &amp;#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}&lt;/ref&gt;
A new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.&lt;ref&gt;{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &amp;#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}&lt;/ref&gt;

In February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].&lt;ref&gt;{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}&lt;/ref&gt;

==References==
{{Reflist|30em}}

==External links==
*[http://www.hello-indigo.com Indigo]
*[http://www.elbot.com Elbot]

[[Category:Natural language processing software]]
[[Category:Intelligent software assistants]]
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval]]</text>
      <sha1>m92eemx7i3rtozpzhw857mmyr0gl96n</sha1>
    </revision>
  </page>
  <page>
    <title>Information retrieval</title>
    <ns>0</ns>
    <id>15271</id>
    <revision>
      <id>647393640</id>
      <parentid>644412161</parentid>
      <timestamp>2015-02-16T13:25:41Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor/>
      <comment>/* First dimension: mathematical basis */Typo fixing, replaced: aka → a.k.a. using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29626">{{Information science}}

'''Information retrieval''' ('''IR''') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[metadata]] or on [[Full text search|full-text]] (or other content-based) indexing.

Automated information retrieval systems are used to reduce what has been called &quot;[[information overload]]&quot;. Many universities and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].

== Overview ==

An information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[relevance|relevancy]].

An object is an entity that is represented by information in a [[database]]. User queries are matched against the database information. Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,&lt;ref name=goodron2000&gt;{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}&lt;/ref&gt; audio,&lt;ref name=Foote99&gt;{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}&lt;/ref&gt; [[mind maps]]&lt;ref name=Beel2009&gt;{{cite journal |first=Jöran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |contribution=Information Retrieval On Mind Maps - What Could It Be Good For? |contribution-url=http://www.sciplore.org/publications_en.php |title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}&lt;/ref&gt; or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.

Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.&lt;ref name=&quot;Frakes1992&quot;&gt;{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures &amp; Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}&lt;/ref&gt;

== History ==
{{Rquote|right|But do you know that, although I have kept the diary [on a phonograph] for months past, it never once struck me how I was going to find any particular part of it in case I wanted to look it up?|[[John Seward|Dr Seward]]| [[Bram Stoker]]'s ''[[Dracula]]'',
 1897}}
The idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.&lt;ref name=&quot;Singhal2001&quot;&gt;{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering |volume=24 |issue=4 |pages=35–43 |year =2001 |url=http://singhal.info/ieee2001.pdf }}&lt;/ref&gt; The first automated information retrieval systems were introduced in the 1950s and 1960s. By 1970 several different techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).&lt;ref name=&quot;Singhal2001&quot; /&gt; Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.

In 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.

== Model types ==
[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsrückgewinnung#Klassifikation von Modellen zur Repräsentation natürlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&amp;lng=eng&amp;id= Dominik Kuropka]).]]
For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.

=== First dimension: mathematical basis ===
* ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:
** [[Standard Boolean model]]
** [[Extended Boolean model]]
** [[Fuzzy retrieval]]
* ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.
** [[Vector space model]]
** [[Generalized vector space model]]
** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]
** [[Extended Boolean model]]
** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]
* ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models.
** [[Binary Independence Model]]
** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function
** [[Uncertain inference]]
** [[Language model]]s
** [[Divergence-from-randomness model]]
** [[Latent Dirichlet allocation]]
* ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just a yet another feature.

=== Second dimension: properties of the model ===
* ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.
* ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.
* ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example a human or sophisticated algorithms.)

== Performance and correctness measures ==
{{main|Precision and recall}}

Many different measures for evaluating the performance of information retrieval systems have been proposed. The measures require a collection of documents and a query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice queries may be [[ill-posed]] and there may be different shades of relevancy.

=== Precision ===

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:&lt;math&gt; \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} &lt;/math&gt;

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of &quot;precision&quot; in the field of Information Retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:&lt;math&gt;\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} &lt;/math&gt;

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:&lt;math&gt; \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} &lt;/math&gt;

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to &lt;math&gt;(1-\mbox{specificity})&lt;/math&gt;. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:&lt;math&gt;F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}.\,&lt;/math&gt;

This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

The general formula for non-negative real &lt;math&gt;\beta&lt;/math&gt; is:
:&lt;math&gt;F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,&lt;/math&gt;.

Two other commonly used F measures are the &lt;math&gt;F_{2}&lt;/math&gt; measure, which weights recall twice as much as precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; &quot;measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision&quot;.  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;.  Their relationship is &lt;math&gt;F_\beta = 1 - E&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;.

=== Average precision ===
&lt;!-- [[Average precision]] redirects here --&gt;
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision &lt;math&gt;p(r)&lt;/math&gt; as a function of recall &lt;math&gt;r&lt;/math&gt;. Average precision computes the average value of &lt;math&gt;p(r)&lt;/math&gt; over the interval from &lt;math&gt;r=0&lt;/math&gt; to &lt;math&gt;r=1&lt;/math&gt;:&lt;ref name=&quot;zhu2004&quot;&gt;{{cite journal |first=Mu |last=Zhu |contribution=Recall, Precision and Average Precision |contribution-url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}&lt;/ref&gt;
:&lt;math&gt;\operatorname{AveP} = \int_0^1 p(r)dr&lt;/math&gt;
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:&lt;math&gt;\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)&lt;/math&gt;
where &lt;math&gt;k&lt;/math&gt; is the rank in the sequence of retrieved documents, &lt;math&gt;n&lt;/math&gt; is the number of retrieved documents, &lt;math&gt;P(k)&lt;/math&gt; is the precision at cut-off &lt;math&gt;k&lt;/math&gt; in the list, and &lt;math&gt;\Delta r(k)&lt;/math&gt; is the change in recall from items &lt;math&gt;k-1&lt;/math&gt; to &lt;math&gt;k&lt;/math&gt;.&lt;ref name=&quot;zhu2004&quot; /&gt;

This finite sum is equivalent to:
:&lt;math&gt; \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!&lt;/math&gt;
where &lt;math&gt;\operatorname{rel}(k)&lt;/math&gt; is an indicator function equaling 1 if the item at rank &lt;math&gt;k&lt;/math&gt; is a relevant document, zero otherwise.&lt;ref name=&quot;Turpin2006&quot;&gt;{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06–11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}&lt;/ref&gt; Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the &lt;math&gt;p(r)&lt;/math&gt; function to reduce the impact of &quot;wiggles&quot; in the curve.&lt;ref name=voc2010&gt;{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}&lt;/ref&gt;&lt;ref name=&quot;nlpbook&quot;&gt;{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}&lt;/ref&gt; For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:&lt;ref name=&quot;voc2010&quot; /&gt;&lt;ref name=&quot;nlpbook&quot; /&gt;
:&lt;math&gt;\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)&lt;/math&gt;
where &lt;math&gt;p_{\operatorname{interp}}(r)&lt;/math&gt; is an interpolated precision that takes the maximum precision over all recalls greater than &lt;math&gt;r&lt;/math&gt;:
:&lt;math&gt;p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})&lt;/math&gt;.

An alternative is to derive an analytical &lt;math&gt;p(r)&lt;/math&gt; function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.&lt;ref&gt;K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.&lt;/ref&gt;

=== R-Precision ===

Precision at '''R'''-th position in the ranking of results for a query that has '''R''' relevant documents. This measure is highly correlated to Average Precision. Also, Precision is equal to Recall at the '''R'''-th position.

=== Mean average precision ===
&lt;!-- [[Mean average precision]] redirects here --&gt;
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:&lt;math&gt; \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!&lt;/math&gt;
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. &lt;/math&gt;

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (&lt;math&gt;IDCG_p&lt;/math&gt;), which normalizes the score:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. &lt;/math&gt;

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other Measures ===
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]

=== Timeline ===

* Before the '''1900s'''
*: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.
*: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.
*: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.
* '''1920s-1930s'''
*: [[Emanuel Goldberg]] submits patents for his &quot;Statistical Machine” a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.
* '''1940s–1950s'''
*: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.
*:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''.
*:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.
*: '''1950s''': Growing concern in the US for a &quot;science gap&quot; with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems (Allen Kent ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]).
*: '''1950''': The term &quot;information retrieval&quot; appears to have been coined by [[Calvin Mooers]].&lt;ref&gt;Mooers, Calvin N.; ''Theory Digital Handling Non-numerical Information'' (Zator Technical Bulletin No. 48) 5, cited in &quot;information, n.&quot;. OED Online. December 2011. Oxford University Press.&lt;/ref&gt;
*: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].&lt;ref name=&quot;Doyle1975&quot;&gt;{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}&lt;/ref&gt;
*: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed &quot;framework&quot; for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.
*: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959)
*: '''1959''': [[Hans Peter Luhn]] published &quot;Auto-encoding of documents for information retrieval.&quot;
* '''1960s''':
*: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.
*: '''1960''': [[Melvin Earl Maron]] and John Lary&lt;!-- sic --&gt; Kuhns&lt;ref name=&quot;Maron2008&quot;&gt;{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971–972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}&lt;/ref&gt; published &quot;On relevance, probabilistic indexing, and information retrieval&quot; in the Journal of the ACM 7(3):216–244, July 1960.
*: '''1962''':
*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, &quot;Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems&quot;. Cranfield Collection of Aeronautics, Cranfield, England, 1962.
*:* Kent published ''Information Analysis and Retrieval''.
*: '''1963''':
*:* Weinberg report &quot;Science, Government and Information&quot; gave a full articulation of the idea of a &quot;crisis of scientific information.&quot;  The report was named after Dr. [[Alvin Weinberg]].
*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963).
*: '''1964''':
*:* [[Karen Spärck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR.
*:* The [[National Bureau of Standards]] sponsored a symposium titled &quot;Statistical Association Methods for Mechanized Documentation.&quot; Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.
*:'''mid-1960s''':
*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.
*::* Project Intrex at MIT.
*:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''.
*:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.
*: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.
*:: '''1968''':
*:* Gerard Salton published ''Automatic Information Organization and Retrieval''.
*:* John W. Sammon, Jr.'s RADC Tech report &quot;Some Mathematics of Information Storage and Retrieval...&quot; outlined the vector model.
*:: '''1969''': Sammon's &quot;A nonlinear mapping for data structure analysis&quot; (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.
* '''1970s'''
*: '''early 1970s''':
*::* First online systems—NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.
*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''.
*: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published &quot;The use of hierarchic clustering in information retrieval&quot;, which articulated the &quot;cluster hypothesis.&quot;&lt;ref&gt;{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217–240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}&lt;/ref&gt;
*: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:
*::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics)
*::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26)
*::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11)
*: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.
*: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models.
* '''1980s'''
*: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.
*: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.
*: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models.
*: '''1985''': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System
*: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems.
*:: '''1985–1993''': Key papers on and experimental systems for visualization interfaces.
*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.
*: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].
* '''1990s'''
*: '''1992''': First [[Text Retrieval Conference|TREC]] conference.
*: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''&lt;ref name=&quot;Korfhage1997&quot;&gt;{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}&lt;/ref&gt; with emphasis on visualization and multi-reference point systems.
*: '''late 1990s''': Web [[Web search engine|search engines]] implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.

== Awards in the field ==

* [[Tony Kent Strix award]]
* [[Gerard Salton Award]]

==See also==

{{col-begin}}
{{col-break}}

* [[Adversarial information retrieval]]
* [[Collaborative information seeking]]
* [[Controlled vocabulary]]
* [[Cross-language information retrieval]]
* [[Data mining]]
* [[European Summer School in Information Retrieval]]
* [[Human–computer information retrieval]]
* [[Information extraction]]
* [[Information Retrieval Facility]]
* [[Knowledge visualization]]
* [[Multimedia Information Retrieval]]
* [[List of information retrieval libraries]]
{{col-break}}
* [[Personal information management]]
* [[Relevance (Information Retrieval)]]
* [[Relevance feedback]]
* [[Rocchio Classification]]
* [[Index (search engine)|Search index]]
* [[Social information seeking]]
* [[Special Interest Group on Information Retrieval]]
* [[Structured Search]]
* [[Subject indexing]]
* [[Temporal information retrieval]]
* [[Tf-idf]]
* [[XML-Retrieval]]
* Key-objects

{{col-end}}

== References ==
{{reflist}}

==External links==
{{wikiquote}}
* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]
* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]
* [http://trec.nist.gov Text Retrieval Conference (TREC)]
* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]
* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]
* [http://ir-facility.org/ Information Retrieval Facility]
* [http://www.nonrelevant.net Information Retrieval @ DUTH]
* [http://nlp.stanford.edu/IR-book/ Introduction to Information Retrieval (online book) by Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Cambridge University Press. 2008.  ]

{{DEFAULTSORT:Information Retrieval}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Information retrieval| ]]
[[Category:Natural language processing]]</text>
      <sha1>9rw7wjhbi0gxbvgx6ho6yz923aqnc6s</sha1>
    </revision>
  </page>
  <page>
    <title>Schoolr</title>
    <ns>0</ns>
    <id>25578717</id>
    <revision>
      <id>589441474</id>
      <parentid>578027944</parentid>
      <timestamp>2014-01-06T13:40:14Z</timestamp>
      <contributor>
        <username>Faizhaider</username>
        <id>1922408</id>
      </contributor>
      <comment>Disambiguated: [[front-end]] → [[front and back ends]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1159">{{orphan|date=April 2010}}
'''Schoolr''' is a [[front and back ends|front-end]] academic directory that features frequently used third-party search engines and resources from [[Google]], [[Wikipedia]], [[Reference.com]], [[Acronym Finder]], [[Wolfram Alpha]], [[Yahoo! Babel Fish]], and the [[University of North Carolina]].&lt;ref&gt;[http://www.lifehack.org/articles/technology/schoolr-google-wikipedia-dictionarycom-and-more-on-one-site.html &quot;Schoolr: Google, Wikipedia, Dictionary.com, and more on one site]&quot;, ''Stepcase Lifehack'', March 19, 2007&lt;/ref&gt;

Schoolr went live on December 3, 2006&lt;ref&gt;[http://lifehacker.com/290027/schoolr-search-start-page &quot;Schoolr search start page]&quot;, ''Lifehacker'', August 16, 2007&lt;/ref&gt; and was developed by Sasan Aghdasi at the [[University of Victoria]].

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://www.schoolr.com/ Schoolr]

{{Internet search}}

{{DEFAULTSORT:Schoolr}}
[[Category:Information retrieval]]
[[Category:Internet search engines]]
[[Category:Internet terminology]]</text>
      <sha1>02ohdrekcy57hmq7q4tocy2w1ays705</sha1>
    </revision>
  </page>
  <page>
    <title>Tag (metadata)</title>
    <ns>0</ns>
    <id>1707086</id>
    <revision>
      <id>646004131</id>
      <parentid>646004129</parentid>
      <timestamp>2015-02-07T07:20:27Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contributions/LILLY-JAH|LILLY-JAH]] to version by Asdklf;. False positive? [[User:ClueBot NG/FalsePositives|Report it]]. Thanks, [[User:ClueBot NG|ClueBot NG]]. (2117482) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21704">{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}
[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]

In [[information system]]s, a '''tag''' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system.

Tagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.

==History==

Labeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.

[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term ''Tag''.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal &quot;A Description of the Equator and Some Other Lands&quot; produced by [[documenta]] X, Germany, coined the folksonomic term ''Tag'' for its co-authors and guest authors on its Upload page. In &quot;The Equator&quot; the term ''Tag'' for user-input was described as an ''abstract literal or keyword'' to aid the user. Turned out in Web 1.0 days, all &quot;Otherlands&quot; users defined singular ''Tags'', and did not share ''Tags'' at that point.

In 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add &quot;tags&quot; to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.&lt;ref&gt;[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.&lt;/ref&gt; [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.&lt;ref&gt;[http://www.adaptivepath.com/ideas/essays/archives/000519.php &quot;An Interview with Flickr's Eric Costello&quot;] by Jesse James Garrett, published on August 4, 2005. Quote: &quot;Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one person’s photos with a particular tag.&quot;&lt;/ref&gt; The success of Flickr and the influence of Delicious popularized the concept,&lt;ref&gt;An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html &quot;Folksonomies - Cooperative Classification and Communication Through Shared Metadata&quot;] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.&lt;/ref&gt; and other [[social software]] websites&amp;nbsp;– such as [[YouTube]], [[Technorati]], and [[Last.fm]]&amp;nbsp;– also implemented tagging. Other traditional and web applications have incorporated the concept such as &quot;Labels&quot; in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].

Tagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or “tags”) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today's tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.

Knowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.&lt;ref&gt;{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}&lt;/ref&gt; Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].

Websites that include tags often display collections of tags as [[tag cloud]]s. A user's tags are useful both to them and to the larger community of the website's users.

Tags may be a &quot;bottom-up&quot; type of classification, compared to [[hierarchy|hierarchies]], which are &quot;top-down&quot;. In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no &quot;wrong&quot; choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and &quot;flat&quot; tagging to aid in information retrieval.&lt;ref&gt;[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.&lt;/ref&gt;

==Examples==
=== Within a Blog ===
Many [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with ''baseball'' and ''tickets''. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.

===For an event===
An official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].

===In research===
A researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.

==Special types==
===Triple tags===
{{see also|Microformat}}
A '''triple tag''' or '''machine tag''' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, &quot;geo:long=50.123456&quot; is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.

The triple tag format was first devised for geolicious&lt;ref&gt;[http://brainoff.com/weblog/2004/11/05/124 geo.lici.us : geotagging hosted services] by Mikel Maron, November 5, 2004.&lt;/ref&gt; in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers&lt;ref&gt;[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, ''Geobloggers'', January 11, 2006.&lt;/ref&gt; to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term ''machine tag'' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.&lt;ref&gt;[http://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.&lt;/ref&gt;

Specialized metadata for geographical identification is known as ''[[geotagging]]''; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].&lt;ref&gt;[http://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.&lt;/ref&gt;

===Hashtags===
{{main|Hashtag}}
A hashtag is a kind of metadata tag marked by the prefix &lt;code&gt;#&lt;/code&gt;, sometimes known as a &quot;hash&quot; symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].

===Knowledge tags===
A knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.

Capturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).&lt;ref&gt;
{{Citation
 | last=Wiig | first=K. M.
 | year= 1997
 | title=Knowledge Management: An Introduction and Perspective
 | journal=Journal of Knowledge Management
 | volume=1 | issue=1
 | pages=6–14
 | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/
 | doi=10.1108/13673279710800682
}}
&lt;/ref&gt; These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise. 

Knowledge tags, in fact, manifest themselves in any number of ways – conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.&lt;ref&gt;
{{citation
 | last=Getting | first=Brian
 | year= 2007
 | title=What Are “Tags” And What Is “Tagging?
 | publisher=Practical eCommerce
 | url=http://www.practicalecommerce.com/articles/589
}}
&lt;/ref&gt; 

Knowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information.&lt;ref&gt;{{citation
| author=Cambria, Erik and Hussain, Amir | title=Sentic album: Content-, concept-, and context-based online personal photo management system | journal=Cognitive Computation | volume=4 | issue=4 | pages=477-496 | year=2012 | doi=10.1007/s12559-012-9145-4}}&lt;/ref&gt; Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.&lt;ref&gt;
{{Citation
 | last=Alavi | first=Maryam
 | last2=Leidner
 | year= 1999
 | title=Knowledge Management Systems: Issues, Challenges, and Benefits
 | journal=Communications of the Association for Information Systems
 | volume=1 | issue=7
 | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf
}}
&lt;/ref&gt;

== Advantages and disadvantages ==
{{procon|date=November 2012}}

In a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.&lt;ref name=&quot;Smith2008&quot;&gt;Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0&lt;/ref&gt; The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.

When users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.&lt;ref&gt;Golder, Scott A. Huberman, Bernardo A. (2005).
&quot;[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems].&quot; Information Dynamics Lab, HP Labs. Visited November 24, 2005.&lt;/ref&gt; For example, the tag &quot;orange&quot; may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged &quot;Linux&quot;, &quot;kernel&quot;, &quot;Penguin&quot;, &quot;software&quot;, or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),&lt;ref&gt;[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.&lt;/ref&gt; which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of &quot;tag terms&quot; within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.

===Complex system dynamics===

Despite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,&lt;ref name=&quot;WWW07-ref&quot;&gt;Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference's website]&lt;/ref&gt; (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.&lt;ref name=&quot;WWW07-ref&quot;/&gt; Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].

===Spamming===

Tagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.&lt;ref&gt;[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.&lt;/ref&gt; The number of tags allowed may also be limited to reduce spam.

==Syntax==
Some tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.

A syntax for use within [[HTML]] is to use the '''rel-tag''' [[microformat]] which uses the [[Rel attribute|''rel'' attribute]] with value &quot;tag&quot; (i.e., &lt;code&gt;rel=&quot;tag&quot;&lt;/code&gt;) to indicate that the linked-to page acts as a tag for the current context.&lt;ref&gt;[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.&lt;/ref&gt;

==See also==
{{colbegin||27em}}
* [[Collective intelligence]]
* [[Concept map]]
* [[Enterprise 2.0]]
* [[Enterprise bookmarking]]
* [[Explicit knowledge]]
* [[Faceted classification]]
* [[Folksonomy]]
* [[Information ecology]]
* [[Knowledge representation]]
* [[Knowledge transfer]]
* [[Metaknowledge]]
* [[Ontology (information science)]]
* [[Organisational memory]]
* [[Semantic web]]
* [[Tag cloud]]
* [[Web 2.0]]
{{colend}}
'''Others'''
{{colbegin||27em}}
* [[Collective unconscious]]
* [[Human-computer interaction]]
* [[Social network aggregation]]
* [[Enterprise social software]]
* [[Expert system]]
* [[Knowledge]]
* [[Knowledge base]]
* [[Knowledge worker]]
* [[Management information system]]
* [[Microformats]]
* [[Social network]]
* [[Social software]]
* [[Sociology of knowledge]]
* [[Tacit Knowledge]]
{{colend}}

==References==
{{reflist|30em}}

'''General'''
{{refbegin}}
*{{Citation
 | surname1=Nonaka | given1=Ikujiro
 | year=1994
 | title= A dynamic theory of organizational knowledge creation
 | journal= ORGANIZATION SCIENCE/ Vol. 5, No. 1, February 1994
 | pages=14–37
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
}}
*{{Citation
 | surname1=Wigg | given1=Karl M  
 | year=1993  
 | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge 
 | journal= Arlington: Schema Press  
 | pages=153
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992 
}} 
*{{Citation
 | surname1=Alavi | given1=Maryam
 | surname2=Leidner | given2=Dorothy E.
 | year=1999
 | title=Knowledge management systems: issues, challenges, and benefits
 | journal=Communications of the AIS
 | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117
}}
*{{Citation
 | surname1=Sandy | given1=Kemsley
 | year=2009
 | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS2’09
 | journal=BPM, Enterprise 2.0 and technology trends in business
 | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/
}}
{{refend}}

==External links==
* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.
* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.

{{Web syndication}}

{{DEFAULTSORT:Tag (Metadata)}}
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Web 2.0]]</text>
      <sha1>bk12o34r5tqs2nrvo19kqp9enolozxh</sha1>
    </revision>
  </page>
  <page>
    <title>Concept search</title>
    <ns>0</ns>
    <id>17785794</id>
    <revision>
      <id>646031461</id>
      <parentid>595981439</parentid>
      <timestamp>2015-02-07T12:59:47Z</timestamp>
      <contributor>
        <username>HelpUsStopSpam</username>
        <id>24038232</id>
      </contributor>
      <comment>Link notable authors.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24890">A '''[[concept]] search''' (or conceptual search) is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a concept search query are relevant to the ideas contained in the text of the query.

__TOC__

==Why Concept Search?==
Concept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.

Polysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.&lt;ref&gt;Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.&lt;/ref&gt;

In addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.

A concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),&lt;ref&gt;R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.&lt;/ref&gt; and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.

==Approaches to Concept Search==
In general, information retrieval research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.&lt;ref&gt;Greengrass, E., Information Retrieval: A Survey, 2000.&lt;/ref&gt;

Efforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:

* Auxiliary structures
* Local [[co-occurrence]] statistics
* Transform techniques (particularly [[matrix decomposition]]s)

===Auxiliary Structures===
A variety of techniques based on Artificial Intelligence (AI) and [[Natural language processing|Natural Language Processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.&lt;ref&gt;Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.&lt;/ref&gt; Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.&lt;ref&gt;Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.&lt;/ref&gt;  It was shown that concept search that is based on auxiliary structures, such as [[WordNet]], can be efficiently implemented by reusing retrieval models and data structures of classical [[Information Retrieval]].&lt;ref&gt;Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.&lt;/ref&gt;  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.

Handcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.&lt;ref name=&quot;Bradford, R. B. 2008&quot;&gt;Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.&lt;/ref&gt;

===Local Co-occurrence Statistics===
Information retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example, ± 5 sentences or ± 50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.

This approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only ¼ of the information contained in text is local in nature.&lt;ref&gt;Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.&lt;/ref&gt;   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.&lt;ref name=&quot;Bradford, R. B. 2008&quot;/&gt;

===Transform Techniques===
Some of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:&lt;ref&gt;Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.&lt;/ref&gt;

* [[Independent component analysis]]
* Semi-discrete decomposition
* [[Non-negative matrix factorization]]
* [[Singular value decomposition]]

Matrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.&lt;ref name=&quot;Bradford, R. B. 2008&quot;/&gt;

Independent component analysis is a technique that creates sparse representations in an automated fashion,&lt;ref&gt;Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010&lt;/ref&gt; and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.&lt;ref name=&quot;Bradford, R. B. 2008&quot;/&gt;

Singular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[Latent semantic indexing|Latent Semantic Indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.&lt;ref&gt;Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.&lt;/ref&gt;

==Uses of Concept Search==
* '''[[eDiscovery]]''' - Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.&lt;ref&gt;Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.
Disability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul &amp; Jason R. Baron, &quot;Information Inflation: Can the Legal System Adapt?&quot; 13 Rich. J.L. &amp; Tech. 10 (2007).&lt;/ref&gt;

* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' - Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.&lt;ref name=&quot;Laplanche, R. 2004&quot;&gt;Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.&lt;/ref&gt;  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.&lt;ref name=&quot;Laplanche, R. 2004&quot;/&gt;

* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' - Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.&lt;ref name=&quot;Lew, M. S. 2006&quot;&gt;Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.&lt;/ref&gt;  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.&lt;ref&gt;Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.&lt;/ref&gt;  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.

* '''Multimedia and Publishing''' - Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.&lt;ref name=&quot;Lew, M. S. 2006&quot;/&gt;

* '''Digital Libraries and Archives''' - Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.

* '''Genomic Information Retrieval (GIR)''' - Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.

* '''Human Resources Staffing and Recruiting''' - Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.

==Effective Concept Searching==
The effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:

* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.

* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''

* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.

As with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.&lt;ref&gt;[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Spärck Jones|Spärck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.&lt;/ref&gt; The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.&lt;ref&gt;Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&amp;nbsp;42–49&lt;/ref&gt;

==Relevance Feedback==
[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.&lt;ref name=&quot;Manning, C. D. 2008&quot;&gt;Manning, C. D., Raghavan P., Schütze H., Introduction to Information Retrieval, Cambridge University Press, 2008.&lt;/ref&gt;   It is a way to involve users in the retrieval process in order to improve the final result set.&lt;ref name=&quot;Manning, C. D. 2008&quot;/&gt; Users can refine their queries based on their initial results to improve the quality of their final results.

In general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.

Relevance feedback has been shown to be very effective at improving the relevance of results.&lt;ref name=&quot;Manning, C. D. 2008&quot;/&gt;   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.&lt;ref name=&quot;Laplanche, R. 2004&quot;/&gt;

[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.&lt;ref name=&quot;Callan, J. 2007&quot;&gt;Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.&lt;/ref&gt;

==Guidelines for Evaluating a Concept Search Engine==
# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.
# Result items should be sorted and ranked by relevance.
# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.
# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.
# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.
# Combined queries using concepts, keywords, and metadata should be allowed.
# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.
# Query-ready indexes should be created relatively quickly.
# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.
# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.

==Search Engine Conferences and Forums==
Formalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.&lt;ref&gt;Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.&lt;/ref&gt;

In 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.

Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.&lt;ref name=&quot;Manning, C. D. 2008&quot;/&gt;

Although the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.&lt;ref name=&quot;Callan, J. 2007&quot;/&gt;   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven’t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.&lt;ref name=&quot;Callan, J. 2007&quot;/&gt;

==See also==
* [[approximate string matching]]
* [[Compound term processing]]
* [[Concept mining]]
* [[Computational linguistics]]
* [[Information extraction]]
* [[Latent semantic indexing]]
* [[Latent semantic analysis]]
* [[Semantic network]]
* [[Semantic search]]
* [[Semantic Web]]
* [[Statistical semantics]]
* [[Text mining]]
* [[Word Sense Disambiguation]]

==References==
{{Reflist|2}}

==External links==
* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]
* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]
* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]
* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]

[[Category:Information retrieval]]</text>
      <sha1>r1qqsdd9r5i465ftye6ek807vpg99og</sha1>
    </revision>
  </page>
  <page>
    <title>Gerard Salton Award</title>
    <ns>0</ns>
    <id>1981660</id>
    <revision>
      <id>594057658</id>
      <parentid>557920364</parentid>
      <timestamp>2014-02-05T15:39:06Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2879">The '''Gerard Salton Award''' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made &quot;significant, sustained and continuing contributions to research in [[information retrieval]]&quot;. SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].

==Chronological honorees and lectures==
* 1983 - [[Gerard Salton]], [[Cornell University]] : &quot;About the future of automatic information retrieval.&quot;
* 1988 - [[Karen Spärck Jones]], [[University of Cambridge]] : &quot;A look back and a look forward.&quot;
* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : &quot;The significance of the Cranfield tests on index languages.&quot;
* 1994 - William S. Cooper, [[University of California, Berkeley]] : &quot;The formalism of probability theory in IR: a foundation or an encumbrance?&quot;
* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : &quot;Users lost (summary): reflections on the past, future, and limits of information science.&quot; 
* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : &quot;On theoretical argument in information retrieval.&quot;&lt;BR&gt;'''For ...''' ''&quot;Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval.&quot;''
* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : &quot;Information retrieval and computer science: an evolving relationship.&quot;&lt;BR&gt;'''For ...''' ''&quot;More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems.&quot;''
* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : 	&quot;Quantum haystacks.&quot;
* 2009 - [[Susan Dumais]], [[Microsoft Research]] : &quot;An Interdisciplinary Perspective on Information Retrieval.&quot;
* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: &quot;Information Retrieval as Engineering Science.&quot;

==External links==
* [http://www.acm.org/sigir/ ACM SIGIR homepage]
* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]

[[Category:Association for Computing Machinery]]
[[Category:Computer science awards]]
[[Category:Information retrieval]]</text>
      <sha1>ne8vmzpwnpfp7jwsxrjuzd7uem3dvqs</sha1>
    </revision>
  </page>
  <page>
    <title>Special Interest Group on Information Retrieval</title>
    <ns>0</ns>
    <id>14109784</id>
    <revision>
      <id>644153993</id>
      <parentid>594057827</parentid>
      <timestamp>2015-01-25T21:06:05Z</timestamp>
      <contributor>
        <username>Kamps</username>
        <id>2471958</id>
      </contributor>
      <minor/>
      <comment>/* SIGIR Conference Locations */ Added 2017 location, see http://sigir.org/files/forum/2014D/p002.pdf</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2830">{{Infobox organization
|name           = ACM Special Interest Group on Information Retrieval
|image          = sig-information-retrieval-logo.png
|size           = 140px
|alt            = ACM SIGIR
|parent_organization = [[Association for Computing Machinery]]
|website        = {{URL|sigir.org}}
}}

'''SIGIR''' is the [[Association for Computing Machinery]]'s Special Interest Group on [[Information Retrieval]]. The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage, retrieval and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.

== Conferences ==
The annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval.  SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[SIGWEB]], the [[Conference on Information and Knowledge Management]], and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[SIGWEB]].

=== SIGIR Conference Locations ===
{| class=&quot;wikitable&quot; border=&quot;1&quot;
|-
!  Number
!  Year
!  Location
|-
|  22
|  1999
|  [[Berkeley, California]]
|-
|  23
|  2000
|  [[Athens]]
|-
|  24
|  2001
|  [[New Orleans]]
|-
|  25
|  2002
|  [[Tampere]]
|-
|  26
|  2003
|  [[Toronto]]
|-
|  27
|  2004
|  [[Sheffield]]
|-
|  28
|  2005
|  [[Salvador, Bahia]]
|-
|  29
|  2006
|  [[Seattle]]
|-
|  30
|  2007
|  [[Amsterdam]]
|-
|  31
|  2008
|  [[Singapore]]
|-
|  32
|  2009
|  [[Boston]]
|-
|  33
|  2010
|  [[Geneva]]
|-
|  34
|  2011
|  [[Beijing]]
|-
|  35
|  2012
|  [[Portland, Oregon]]
|-
|  36
|  2013
|  [[Dublin]]
|-
|  37
|  2014
|  [[Gold Coast, Queensland]]
|-
|  38
|  2015
|  [[Santiago]]
|-
|  39
|  2016
|  [[Pisa]]
|-
|  40
|  2017
|  [[Tokyo]]
|}

== Awards ==
The group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made &quot;significant, sustained and continuing contributions to research in information retrieval&quot;. Additionally, SIGIR presents a Best Paper Award &lt;ref&gt;{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}&lt;/ref&gt; to recognize the highest quality paper at each conference.

==See also==
* [[Conference on Information and Knowledge Management]]

==External links==
* [http://www.sigir.org/ SIGIR]

==References==

{{Reflist}}
{{Authority control}}

[[Category:Association for Computing Machinery Special Interest Groups]]
[[Category:Information retrieval]]</text>
      <sha1>gd7hp7nei75i65k69grhkyhsjazue8e</sha1>
    </revision>
  </page>
  <page>
    <title>Stop words</title>
    <ns>0</ns>
    <id>1015600</id>
    <revision>
      <id>643276083</id>
      <parentid>637348074</parentid>
      <timestamp>2015-01-19T22:43:50Z</timestamp>
      <contributor>
        <username>Al-punk</username>
        <id>22930344</id>
      </contributor>
      <minor/>
      <comment>Old page had a 301 Redirect to the updated URL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3061">{{distinguish|Safeword}}
In [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).&lt;ref&gt;{{cite doi|10.1017/CBO9781139058452.002}}&lt;/ref&gt; There is no single universal list of stop words used by all [[Natural language processing|processing of natural language]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].

Any group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as '[[The Who]]', '[[The The]]', or '[[Take That]]'. Other search engines remove some of the most common words—including [[lexical word]]s, such as &quot;want&quot;—from a query in order to improve performance.&lt;ref&gt;[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: &quot;One of our major performance optimizations for the &quot;related questions&quot; query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It’s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster&quot;.&lt;/ref&gt;

[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept. {{Citation needed|date=March 2013}}

== See also ==
{{Div col|cols=3}}
* [[Text mining]]
* [[Concept mining]]
* [[Information extraction]]
* [[Natural language processing]]
* [[Query expansion]]
* [[Stemming]]
* [[Index (search engine)|Search engine indexing]]
* [[Poison words]]
* [[Function words]]
{{Div col end}}

==References==
{{Reflist}}

== External links ==
* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]
* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]
* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]
* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]
* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]
* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]
* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages]
* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]


[[Category:Information retrieval]]
[[Category:Searching]]
{{Natural Language Processing}}
{{SearchEngineOptimization}}</text>
      <sha1>3cbny4vthzd6kav560qs0nnxz3gzrwo</sha1>
    </revision>
  </page>
  <page>
    <title>Latent semantic indexing</title>
    <ns>0</ns>
    <id>21109827</id>
    <revision>
      <id>647493883</id>
      <parentid>646255140</parentid>
      <timestamp>2015-02-17T02:56:09Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Term-document matrix */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27448">'''Latent semantic indexing''' ('''LSI''') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.&lt;ref&gt;Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 36–40.&lt;/ref&gt;

LSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benzécri]]&lt;ref&gt;{{ cite book
 | author = Benzécri, J.-P.
 | publisher=Dunod |location= Paris, France
 | year = 1973
 | title = L'Analyse des Données. Volume II. L'Analyse des Correspondences
 }}&lt;/ref&gt; in the early 1970s, to a [[contingency table]] built from word counts in documents.

Called Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at Bellcore in the late 1980s.   The method, also called [[latent semantic analysis]] (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria.

__TOC__

== Benefits of LSI ==

LSI overcomes two of the most problematic constraints of Boolean keyword queries:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of [[vocabulary mismatch|mismatches in the vocabulary]] used by the authors of documents and the users of information retrieval systems.&lt;ref&gt;{{cite doi|10.1145/32206.32212}}&lt;/ref&gt;&lt;ref&gt;{{cite doi|10.1145/1871437.1871474}}&lt;/ref&gt;   As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.

LSI is also used to perform automated document categorization.  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.&lt;ref&gt;Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns &amp; S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 45–51.&lt;/ref&gt;    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.&lt;ref&gt;{{cite doi|10.1145/288627.288651}}&lt;/ref&gt;   LSI uses ''example'' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.

Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.

Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.

LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.&lt;ref&gt;Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC&lt;/ref&gt;   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.&lt;ref&gt;{{cite doi|10.1093/bioinformatics/bth464}}&lt;/ref&gt;

LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).&lt;ref&gt;{{cite doi|10.1007/11427995_68}}&lt;/ref&gt;   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.

Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.

LSI has proven to be a useful solution to a number of conceptual matching problems.&lt;ref&gt;Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 59–65.&lt;/ref&gt;&lt;ref&gt;Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161–167.&lt;/ref&gt;  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.&lt;ref&gt;Graesser, A., and Karnavat, A., Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures, Proceedings of CogSci 2000, pp. 184–189.&lt;/ref&gt;

== LSI timeline ==

'''Mid-1960s''' – Factor analysis technique first described and tested (H. Borko and M. Bernick)

'''1988''' – Seminal paper on LSI technique published (Deerwester et al.)

'''1989''' – Original patent granted (Deerwester et al.)

'''1992''' – First use of LSI to assign articles to reviewers&lt;ref&gt;Dumais, S., and Nielsen, J., Automating the Assignment of Submitted Manuscripts to Reviewers, Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval, 1992, pp. 233–244.&lt;/ref&gt;  (Dumais and Nielsen)

'''1994''' – Patent granted for the cross-lingual application of LSI (Landauer et al.)

'''1995''' – First use of LSI for grading essays (Foltz, et al., Landauer et al.)

'''1999''' – First implementation of LSI technology for intelligence community for analyzing unstructured text (SAIC).

'''2002''' – LSI-based product offering to intelligence-based government agencies (SAIC)

'''2005''' – First vertical-specific application – publishing – EDB (EBSCO, [[Content Analyst Company]])

== Mathematics of LSI ==

LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a '''Singular Value Decomposition''' on the matrix, and using the matrix to identify the concepts contained in the text.

=== Term-document matrix ===

LSI begins by constructing a term-document matrix, &lt;math&gt;A&lt;/math&gt;, to identify the occurrences of the &lt;math&gt;m&lt;/math&gt; unique terms within a collection of &lt;math&gt;n&lt;/math&gt; documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, &lt;math&gt;a_{ij}&lt;/math&gt;, initially representing the number of times the associated term appears in the indicated document, &lt;math&gt;\mathrm{tf_{ij}}&lt;/math&gt;.  This matrix is usually very large and very sparse.

Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, &lt;math&gt;a_{ij}&lt;/math&gt; of &lt;math&gt;A&lt;/math&gt;, to be the product of a local term weight, &lt;math&gt;l_{ij}&lt;/math&gt;, which describes the relative frequency of a term in a document, and a global weight, &lt;math&gt;g_i&lt;/math&gt;, which describes the relative frequency of the term within the entire collection of documents.

Some common local weighting functions &lt;ref&gt;
Berry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).&lt;/ref&gt; are defined in the following table.

{| style=&quot;width:60%&quot; cellpadding=&quot;25&quot; cellspacing=&quot;5&quot; align=&quot;center&quot;
|-
|  style=&quot;width:22%&quot; | '''Binary''' ||
| &lt;math&gt;l_{ij} = 1&lt;/math&gt; if the term exists in the document, or else &lt;math&gt;0&lt;/math&gt;
|-
|  style=&quot;width:22%&quot; | '''TermFrequency''' ||
| &lt;math&gt;l_{ij} = \mathrm{tf}_{ij}&lt;/math&gt;, the number of occurrences of term &lt;math&gt;i&lt;/math&gt; in document &lt;math&gt;j&lt;/math&gt;
|-
|  style=&quot;width:22%&quot; | '''Log''' ||
| &lt;math&gt;l_{ij} = \log(\mathrm{tf}_{ij} + 1)&lt;/math&gt;
|-
|  style=&quot;width:22%&quot; | '''Augnorm''' ||
| &lt;math&gt;l_{ij} = \frac{\Big(\frac{\mathrm{tf}_{ij}}{\max_i(\mathrm{tf}_{ij})}\Big) + 1}{2}&lt;/math&gt;
|}

Some common global weighting functions are defined in the following table.

{| style=&quot;width:60%&quot; cellpadding=&quot;25&quot; cellspacing=&quot;5&quot; align=&quot;center&quot;
|-
| style=&quot;width:22%&quot; | '''Binary''' ||
| &lt;math&gt;g_i = 1&lt;/math&gt;
|-
| style=&quot;width:22%&quot; | '''Normal''' ||
| &lt;math&gt;g_i = \frac{1}{\sqrt{\sum_j \mathrm{tf}_{ij}^2}}&lt;/math&gt;
|-
| style=&quot;width:22%&quot; | '''GfIdf''' ||
| &lt;math&gt;g_i = \mathrm{gf}_i / \mathrm{df}_i&lt;/math&gt;, where &lt;math&gt;\mathrm{gf}_i&lt;/math&gt; is the total number of times term &lt;math&gt;i&lt;/math&gt; occurs in the whole collection, and &lt;math&gt;\mathrm{df}_i&lt;/math&gt; is the number of documents in which term &lt;math&gt;i&lt;/math&gt; occurs.
|-
| style=&quot;width:22%&quot; | '''Idf''' ||
| &lt;math&gt;g_i = \log_2 \frac{n}{1+ \mathrm{df}_i}&lt;/math&gt;
|-
| style=&quot;width:22%&quot; | '''Entropy''' ||
| &lt;math&gt;g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}&lt;/math&gt;, where &lt;math&gt;p_{ij} = \frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}&lt;/math&gt;
|}

Empirical studies with LSI report that the Log Entropy weighting functions work well, in practice, with many data sets.&lt;ref&gt;Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.&lt;/ref&gt;  In other words, each entry &lt;math&gt;a_{ij}&lt;/math&gt; of &lt;math&gt;A&lt;/math&gt; is computed as:

:&lt;math&gt;g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}&lt;/math&gt;

:&lt;math&gt;a_{ij} = g_i \ \log (\mathrm{tf}_{ij} + 1)&lt;/math&gt;

=== Rank-reduced singular value decomposition ===

A rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.&lt;ref&gt;Berry, Michael W., Dumais, Susan T., O'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573–595.&lt;/ref&gt;   It computes the term and document vector spaces by approximating the single term-frequency matrix, &lt;math&gt;A&lt;/math&gt;, into three other matrices— an '''''m''''' by '''''r'''''  term-concept vector matrix &lt;math&gt;T&lt;/math&gt;, an '''''r''''' by '''''r''''' singular values matrix &lt;math&gt;S&lt;/math&gt;, and a '''''n''''' by '''''r''''' concept-document vector matrix, &lt;math&gt;D&lt;/math&gt;, which satisfy the following relations:

&lt;math&gt;A \approx TSD^T&lt;/math&gt;

&lt;math&gt;T^T T = I_r \quad D^T D = I_r &lt;/math&gt;

&lt;math&gt;S_{1,1} \geq S_{2,2} \geq \ldots \geq  S_{r,r} &gt; 0 \quad S_{i,j} = 0 \; \text{where} \; i \neq j&lt;/math&gt;

In the formula, '''A''' is the supplied '''''m''''' by '''''n''''' weighted matrix of term frequencies in a collection of text where '''''m''''' is the number of unique terms, and '''''n''''' is the number of documents.  '''T''' is a computed '''''m''''' by '''''r''''' matrix of term vectors where '''''r''''' is the rank of '''A'''—a measure of its unique dimensions '''≤ min(''m,n'')'''.  '''S''' is a computed '''''r''''' by '''''r''''' diagonal matrix of decreasing singular values, and '''D''' is a computed '''''n''''' by '''''r''''' matrix of document vectors.

The LSI modification to a standard SVD is to reduce the rank or truncate the singular value matrix '''S''' to size '''''k''''' « '''''r''''', typically on the order of a '''''k''''' in the range of 100 to 300 dimensions, effectively reducing the term and document vector matrix sizes to '''''m''''' by '''''k''''' and '''''n''''' by '''''k''''' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of '''A'''.  This reduced set of matrices is often denoted with a modified formula such as:

:::::::'''A ≈ A''&lt;sub&gt;k''&lt;/sub&gt; = T''&lt;sub&gt;k''&lt;/sub&gt; S''&lt;sub&gt;k''&lt;/sub&gt; D''&lt;sub&gt;k''&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;'''

Efficient LSI algorithms only compute the first '''''k''''' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.

Note that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix '''A''', except that PCA subtracts off the means.  PCA loses the sparseness of the '''A''' matrix, which can make it infeasible for large lexicons.

== Querying and augmenting LSI vector spaces ==

The computed '''T''&lt;sub&gt;k''&lt;/sub&gt;''' and '''D''&lt;sub&gt;k''&lt;/sub&gt;''' matrices define the term and document vector spaces, which with the computed singular values, '''S''&lt;sub&gt;k''&lt;/sub&gt;''', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.

The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the '''A = T S D&lt;sup&gt;T&lt;/sup&gt;''' equation into the equivalent '''D = A&lt;sup&gt;T&lt;/sup&gt; T S&lt;sup&gt;−1&lt;/sup&gt;''' equation, a new vector, '''''d''''', for a query or for a new document can be created by computing a new column in '''A''' and then multiplying the new column by '''T S&lt;sup&gt;−1&lt;/sup&gt;'''.  The new column in '''A''' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.

A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.

The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called ''folding in''.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in &lt;ref name=&quot;brand2006&quot;&gt;{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20–30 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}&lt;/ref&gt;) be used.

== Additional uses of LSI ==

It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.

LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.&lt;ref&gt;Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.&lt;/ref&gt;   Below are some other ways in which LSI is being used:

* Information discovery&lt;ref&gt;Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189–223.&lt;/ref&gt;  (eDiscovery, Government/Intelligence community, Publishing)
* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)&lt;ref&gt;Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.&lt;/ref&gt;
* Text summarization&lt;ref&gt;Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903–907.&lt;/ref&gt;  (eDiscovery, Publishing)
* Relationship discovery&lt;ref&gt;Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374–380.&lt;/ref&gt;  (Government, Intelligence community, Social Networking)
* Automatic generation of link charts of individuals and organizations&lt;ref&gt;Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 23–24, 2006, Springer, LNCS vol. 3975, pp. 674–675.&lt;/ref&gt;  (Government, Intelligence community)
* Matching technical papers and grants with reviewers&lt;ref&gt;Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220–230.&lt;/ref&gt;  (Government)
* Online customer support&lt;ref&gt;Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master's Thesis, May 2000.&lt;/ref&gt;  (Customer Management)
* Determining document authorship&lt;ref&gt;Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 43–48.&lt;/ref&gt;  (Education)
* Automatic keyword annotation of images&lt;ref&gt;Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275–278.&lt;/ref&gt;
* Understanding software source code&lt;ref&gt;Maletic, J., and Marcus, A., Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding, Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence, Vancouver, British Columbia, November 13–15, 2000, pp. 46–53.&lt;/ref&gt;  (Software Engineering)
* Filtering [[Spam (electronic)|spam]]&lt;ref&gt;Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460–464.&lt;/ref&gt;  (System Administration)
* Information visualization&lt;ref&gt;Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Science, 101, 2004, pp. 5214–5219.&lt;/ref&gt;
* [[Automated essay scoring|Essay scoring]]&lt;ref&gt;Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.&lt;/ref&gt;  (Education)
* [[Literature-based discovery]]&lt;ref&gt;Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674–685.&lt;/ref&gt;

LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.&lt;ref&gt;There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.&lt;/ref&gt;

== Challenges to LSI ==

Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.&lt;ref&gt;Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.&lt;/ref&gt;  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are not uncommon in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.&lt;ref name=&quot;rehurek2011&quot;&gt;{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim Řehůřek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289–300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 }}&lt;/ref&gt;

Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).&lt;ref&gt;Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153–162.&lt;/ref&gt;   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.&lt;ref&gt;Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.&lt;/ref&gt;

Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.&lt;ref&gt;Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).&lt;/ref&gt;&lt;ref&gt;Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).&lt;/ref&gt;&lt;ref&gt;Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.&lt;/ref&gt;

==See also==
* [[Latent semantic analysis]]
* [[Latent Semantic Structure Indexing]]
* [[Principal component analysis]]
* [[Correspondence analysis]]
* [[Probabilistic latent semantic analysis]]

{{Natural Language Processing}}

== References ==
{{Reflist}}

== Further reading ==
*{{cite book|authors=Berry, M. W., Browne M.|title=Understanding Search Engines: Mathematical Modeling and Text Retrieval|location=Philadelphia|publisher=Society for Industrial and Applied Mathematics|year=2005|isbn=978-0898715811|url=http://www.mblazquez.es/blog-ccdoc-recuperacion/documentos/book_understanding-search-engines.pdf}}
*{{cite book|editors=Berry, M. W.|title=Survey of Text Mining: Clustering, Classification, and Retrieval|location=New York|publisher=Springer|year=2004|url=https://perso.uclouvain.be/vincent.blondel/publications/08-textmining.pdf|isbn=978-0387955636}}
*{{cite book|authors=Landauer, T., et al.|title=Handbook of Latent Semantic Analysis|publisher=Lawrence Erlbaum Associates|year=2007|isbn= 978-0805854183|url=http://books.google.de/books/about/Handbook_of_latent_semantic_analysis.html?id=jgVWCuFXePEC&amp;redir_esc=y}}
*{{cite book|authors=Manning, C. D., Schutze H.|title=Foundations of Statistical Natural Language Processing|location=Cambridge, MA|publisher=The MIT Press|year=1999|url=http://nlp.stanford.edu/fsnlp/promo/contents.ps|isbn=9780262133609 }} [http://nlp.stanford.edu/fsnlp/ Companion webpage]

==External links==
* [http://www.cs.utk.edu/~lsi/ Michael Berry’s site]
* [http://radimrehurek.com/gensim Gensim] contains a scalable Python+[[NumPy]] implementation of LSI, even for datasets larger than the available RAM.
* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.
* [http://www.youtube.com/watch?v=QGd06MTRMHs Stanford University Andrew Ng Video on LSI]

{{DEFAULTSORT:Latent semantic indexing}}
[[Category:Information retrieval]]
[[Category:Semantic Web]]</text>
      <sha1>e58g9v6lmulir63fgizag9syjns2t6b</sha1>
    </revision>
  </page>
  <page>
    <title>URL redirection</title>
    <ns>0</ns>
    <id>636686</id>
    <revision>
      <id>642745596</id>
      <parentid>639560555</parentid>
      <timestamp>2015-01-16T09:55:35Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* nginx rewrite */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10770)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28639">{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}

{{refimprove|date=July 2014}}
'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, '''domain redirection''' or '''domain forwarding''' is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org].
URL redirection can be used for [[URL shortening]], to prevent [[link rot|broken links]] when web pages are moved, to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]], to guide navigation into and out of a website, for privacy protection, and for less innocuous purposes such as [[phishing]] attacks.

== Purposes ==
There are several reasons to use URL redirection :

=== Similar domain names ===
A user might mis-type a URL—for example, &quot;example.com&quot; and &quot;exmaple.com&quot;. Organizations often register these &quot;mis-spelled&quot; domains and re-direct them to the &quot;correct&quot; location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to &quot;reserve&quot; other [[top-level domain]]s (TLD) with the same name, or make it easier for a true &quot;.edu&quot; or &quot;.net&quot; to redirect to a more recognizable &quot;.com&quot; domain.

=== Moving pages to a new domain ===
Web pages may be redirected to a new domain for three reasons:
* a site might desire, or need, to change its domain name;
* an author might move his or her individual pages to a new domain;
* two web sites might merge.

With URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers.

The same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a &quot;moved permanently&quot; redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.

=== Logging outgoing links ===
The access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link.

This information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.&lt;ref&gt;
{{cite journal
  | title = Google revives redirect snoopery
  | journal = blog.anta.net
  | date = 2009-01-29
  | url = http://blog.anta.net/2009/01/29/509/
  | issn = 1797-1993
  | archiveurl=http://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/
  | archivedate=2011-08-17
}}&lt;/ref&gt;

The same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.

=== Short aliases for long URLs ===
{{Main|URL shortening}}

Web applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.

=== Meaningful, persistent aliases for long or changing URLs ===
{{See also|Permalink|PURL|Link rot}}

Sometimes the URL of a page changes even though the content stays the same. Therefore URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.

=== Post/Redirect/Get ===
{{Main|Post/Redirect/Get}}

Post/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).

=== Manipulating search engines ===
Redirect techniques are used to fool search engines.  For example, one page could show popular search terms to search engines but redirect the visitors to a different target page.  There are also cases where redirects have been used to &quot;steal&quot; the page rank of one popular page and use it for a different page, They will also redirect using searches with search engines as searches, usually involving the 302 [[List of HTTP status codes|HTTP status code]] of &quot;moved temporarily.&quot;&lt;ref&gt;{{cite web|url=http://www.pandia.com/sw-2004/40-hijack.html |title=Google's serious hijack problem – Spammers hijack web site listings in Google |date=September 13, 2004 |publisher=Pandia.com |archiveurl=http://web.archive.org/web/20130605153457/http://www.pandia.com/sw-2004/40-hijack.html |archivedate=2013-06-05}}&lt;/ref&gt;&lt;ref&gt;[http://www.loriswebs.com/hijacking_web_pages.html &quot;Stop Scrapers From Hijacking your Web Pages&quot;]. Lori's Web Design.com. Retrieved 2013-12-18.&lt;/ref&gt;

Search engine providers have noticed the problem and are working on appropriate actions.{{Citation needed|date=August 2009}}

As a result, today, such manipulations usually result in less rather than more site exposure.

=== Manipulating visitors ===
URL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.{{Citation needed|date=January 2010}} Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.

=== Removing &lt;code&gt;referer&lt;/code&gt; information ===
When a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, &lt;code&gt;&lt;nowiki&gt;http://company.com/plans-for-the-next-release-of-our-product&lt;/nowiki&gt;&lt;/code&gt;), it is not desirable for the &lt;code&gt;referer&lt;/code&gt; URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example &lt;code&gt;&lt;nowiki&gt;http://externalsite.com/page&lt;/nowiki&gt;&lt;/code&gt; into &lt;code&gt;&lt;nowiki&gt;http://redirect.company.com/http://externalsite.com/page&lt;/nowiki&gt;&lt;/code&gt;. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.

== Techniques ==
Several different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.

=== Manual redirect ===
The simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:

&lt;source lang=&quot;html4strict&quot;&gt;
Please follow &lt;a href=&quot;http://www.example.com/&quot;&gt;this link&lt;/a&gt;.
&lt;/source&gt;

This method is often used as a fall-back&amp;nbsp;— if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.

=== HTTP status codes 3xx ===
In the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page.  The different codes describe the reason for the redirect, which allows for the correct subsequent action (such as changing links in the case of code 301, a permanent change of address).

HTTP/1.1 defines [http://tools.ietf.org/html/rfc7231#section-6.4 several status codes] for redirection:
* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)
* [[HTTP 301|301 moved permanently]]
* [[HTTP 302|302 found]] (originally &quot;temporary redirect&quot; in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)
* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)
* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)

All of these status codes require that the URL of the redirect target be given in the Location: header of the HTTP response.  The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.

(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).

An [[HTTP]] response with the 301 &quot;moved permanently&quot; redirect looks like this:

&lt;source lang=&quot;html4strict&quot;&gt;
HTTP/1.1 301 Moved Permanently
Location: http://www.example.org/
Content-Type: text/html
Content-Length: 174

&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Moved&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Moved&lt;/h1&gt;
&lt;p&gt;This page has moved to &lt;a href=&quot;http://www.example.org/&quot;&gt;http://www.example.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

==== Using server-side scripting for redirection ====
Web authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling &quot;non-parsed-headers&quot;).  Many web servers will generate a 3xx status code if a script outputs a &quot;Location:&quot; header line.  For example, in [[PHP]], one can use the &quot;header&quot; function:

&lt;source lang=&quot;php&quot;&gt;
header('HTTP/1.1 301 Moved Permanently');
header('Location: http://www.example.com/');
exit();
&lt;/source&gt;

(More headers may be required to prevent caching&lt;ref name=&quot;php-301-robust-solution&quot;&gt;{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=http://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}&lt;/ref&gt;).

The programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using &lt;code&gt;response.buffer=true&lt;/code&gt; and &lt;code&gt;response.redirect &lt;nowiki&gt;&quot;http://www.example.com/&quot;&lt;/nowiki&gt;&lt;/code&gt;

HTTP/1.1 [http://tools.ietf.org/html/rfc7231#section-7.1.2 allows for] either a relative URI reference or an absolute URI reference. If the URI reference is relative the client computes the required absolute URI reference according to [http://tools.ietf.org/html/rfc3986#section-5 the rules defined in RFC 3986].

==== Apache mod_rewrite ====
The [[Apache HTTP Server]]'s [http://httpd.apache.org/docs/current/mod/mod_alias.html mod_alias] extension can be used to redirect certain requests.  Typical configuration directives look like:

&lt;source lang=&quot;apache&quot;&gt;
Redirect permanent /oldpage.html http://www.example.com/newpage.html
Redirect 301 /oldpage.html http://www.example.com/newpage.html
&lt;/source&gt;
&lt;/blockquote&gt;

For more flexible URL rewriting and redirection, Apache [http://httpd.apache.org/docs/current/mod/mod_rewrite.html mod_rewrite] can be used.  E.g. to redirect a requests to a canonical domain name:
&lt;source lang=&quot;apache&quot;&gt;
RewriteEngine on
RewriteCond %{HTTP_HOST} ^([^.:]+\.)*oldsite\.example\.com\.?(:[0-9]*)?$ [NC]
RewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]
&lt;/source&gt;

Such configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a &lt;code&gt;.htaccess&lt;/code&gt; file.

==== nginx rewrite ====
[[Nginx]] has an integrated http rewrite module,&lt;ref&gt;{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}&lt;/ref&gt; which can be used to perform advanced URL processing and even web-page generation (with the &lt;tt&gt;return&lt;/tt&gt; directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.&lt;ref&gt;{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://mdoc.su/ |title=mdoc.su — Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}&lt;/ref&gt;

For example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 &lt;tt&gt;/DragonFlyBSD/HAMMER.5&lt;/tt&gt;] were to come along, it would first be redirected internally to &lt;tt&gt;/d/HAMMER.5&lt;/tt&gt; with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:&lt;ref&gt;{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}&lt;/ref&gt;
&lt;source lang=&quot;pcre&quot;&gt;
	location /DragonFly {
		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;
	}
	location /d {
		set	$db	&quot;http://leaf.dragonflybsd.org/cgi/web-man?command=&quot;;
		set	$ds	&quot;&amp;section=&quot;;
		rewrite	^/./([^/]+)\.([1-9])$		$db$1$ds$2	redirect;
	}
&lt;/source&gt;

=== Refresh Meta tag and HTTP refresh header ===
[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.  See
* [http://www.w3schools.com/tags/tag_meta.asp HTML &lt;meta&gt; tag]
* [http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]

A timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.&lt;ref&gt;[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ &quot;Google and Yahoo accept undelayed meta refreshs as 301 redirects&quot;]. Sebastian's Pamphlets. 3 September 2007.&lt;/ref&gt;

This is an example of a simple HTML document that uses this technique:
&lt;source lang=&quot;html4strict&quot;&gt;
&lt;html&gt;
&lt;head&gt;
&lt;meta http-equiv=&quot;Refresh&quot; content=&quot;0; url=http://www.example.com/&quot; /&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;Please follow &lt;a href=&quot;http://www.example.com/&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

This technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the &quot;head&quot; section of the HTML file.  The number &quot;0&quot; in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the &quot;body&quot; section is for users whose browsers do not support this feature.

The same effect can be achieved with an HTTP &lt;code&gt;refresh&lt;/code&gt; header:
&lt;source lang=&quot;html4strict&quot;&gt;
HTTP/1.1 200 ok
Refresh: 0; url=http://www.example.com/
Content-type: text/html
Content-length: 78

Please follow &lt;a href=&quot;http://www.example.com/&quot;&gt;this link&lt;/a&gt;.
&lt;/source&gt;

This response is easier to generate by CGI programs because one does not need to change the default status code.

Here is a simple CGI program that effects this redirect:
&lt;source lang=&quot;perl&quot;&gt;
#!/usr/bin/perl
print &quot;Refresh: 0; url=http://www.example.com/\r\n&quot;;
print &quot;Content-type: text/html\r\n&quot;;
print &quot;\r\n&quot;;
print &quot;Please follow &lt;a href=\&quot;http://www.example.com/\&quot;&gt;this link&lt;/a&gt;!&quot;
&lt;/source&gt;

Note: Usually, the HTTP server adds the status line and the Content-length header automatically.

The [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].

=== JavaScript redirects ===
[[JavaScript]] can cause a redirect by setting the &lt;code&gt;window.location&lt;/code&gt; attribute, e.g.:
&lt;syntaxhighlight lang=&quot;ecmascript&quot;&gt;
window.location='http://www.example.com/'
&lt;/syntaxhighlight&gt;
Normally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.&lt;ref&gt;{{cite web|url=http://online-marketing-technologies.com/tools/javascript-redirection-generator.html|title=Advanced JavaScript Redirections|publisher=Online Marketing Technologies}}&lt;/ref&gt;
&lt;syntaxhighlight lang=&quot;ecmascript&quot;&gt;
window.location.replace('http://www.example.com/')
&lt;/syntaxhighlight&gt;
However, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.

=== Frame redirects ===
A slightly different effect can be achieved by creating a single HTML [[Iframe|frame]] that contains the target page:
&lt;source lang=&quot;html4strict&quot;&gt;
&lt;frameset rows=&quot;100%&quot;&gt;
  &lt;frame src=&quot;http://www.example.com/&quot;&gt;
  &lt;noframes&gt;
    &lt;body&gt;Please follow &lt;a href=&quot;http://www.example.com/&quot;&gt;link&lt;/a&gt;.&lt;/body&gt;
  &lt;/noframes&gt;
&lt;/frameset&gt;
&lt;/source&gt;

One main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar.

This ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].&lt;ref&gt;Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf &quot;Anti-Phishing Technology&quot;] (PDF). Radix Labs.&lt;/ref&gt;

The same effect can be done with an inline frame:
&lt;source lang=&quot;html4strict&quot;&gt;
&lt;iframe height=&quot;100%&quot; width=&quot;100%&quot; src=&quot;http://www.example.com/&quot;&gt;
Please follow &lt;a href=&quot;http://www.example.com/&quot;&gt;link&lt;/a&gt;.
&lt;/iframe&gt;
&lt;/source&gt;

=== Redirect chains ===
One redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (note the domain name) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] and then to the correct URL: http://en.wikipedia.org/wiki/URL_redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''rewriting'' the URL as much as possible on the server before returning it to the browser as a redirect.

=== Redirect loops ===
Sometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.

[http://tools.ietf.org/html/rfc7231#section-6.4 HTTP/1.1] states:
&lt;blockquote&gt;
A client ''SHOULD'' detect and intervene in cyclical redirections (i.e., &quot;infinite&quot; redirection loops).

Note: An earlier version of this specification recommended a maximum of five redirections ([RFC2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.
&lt;/blockquote&gt;
Note that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -&gt; http://www.example.com/2 -&gt; http://www.example.com/3 ...

== Services ==
There exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.

=== URL redirection services ===
A '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]].

Hyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.

Recently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs.

A major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.

==== History ====
The first redirect services took advantage of [[top-level domains]] (TLD) such as &quot;[[.to]]&quot; (Tonga), &quot;[[.at]]&quot; (Austria) and &quot;[[.is]]&quot; (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including &quot;r.im&quot;, &quot;go.to&quot;, &quot;i.am&quot;, &quot;come.to&quot; and &quot;start.at&quot;.  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.&lt;ref&gt;{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}&lt;/ref&gt; As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined.

With the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.

=== Referrer masking ===
Redirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.)

This type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .

Here is a simplistic example of such a service, written in [[PHP]].
&lt;source lang=&quot;php&quot;&gt;
&lt;?php
$url = htmlspecialchars($_GET['url']);
header( 'Refresh: 0; url=http://'.$url );
?&gt;
&lt;!-- Fallback using meta refresh. --&gt;
&lt;html&gt;
 &lt;head&gt;
  &lt;title&gt;Redirecting...&lt;/title&gt;
  &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0;url=http://&lt;?php echo $url; ?&gt;&quot;&gt;
 &lt;/head&gt;
 &lt;body&gt;
 Attempting to redirect to &lt;a href=&quot;http://&lt;?php echo $url; ?&gt;&quot;&gt;http://&lt;?php echo $url; ?&gt;&lt;/a&gt;.
 &lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

The above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.

==Security Issues==
URL redirection can be abused by attackers for [[Phishing]] attacks, such as [[Open Redirect]] and [[Covert Redirect]].

&quot;An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation.&quot;&lt;ref name=&quot;Open_Redirect&quot;&gt;{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}&lt;/ref&gt;

&quot;Covert Redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation.&quot;&lt;ref name=&quot;Covert_Redirect&quot;&gt;{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}&lt;/ref&gt; It is disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.&lt;ref name=&quot;CNET&quot;&gt;{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}&lt;/ref&gt;

== See also ==
* [[Link rot]]
* [[Canonical meta tag]]
* [[Domain masking]]

== References ==
{{Reflist}}

== External links ==
* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]
* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)
* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification
* [http://www.dancatts.com/articles/htaccess-301-redirects-for-moved-pages.php 301 Redirects for moved pages using .htaccess]
* [http://911-need-code-help.blogspot.com/2011/03/redirecting-visitors-to-preferred.html Redirecting your visitors to your preferred domain] using 301 permanent redirects&amp;nbsp;— rationale and mod_rewrite/PHP/ASP.NET implementations

{{Spamming}}

{{Use dmy dates|date=November 2010}}

{{DEFAULTSORT:Url Redirection}}
[[Category:Uniform resource locator]]
[[Category:Black hat search engine optimization]]
[[Category:Information retrieval]]
[[Category:Internet terminology]]</text>
      <sha1>qkdc839r5vmlp585zxp3znzqrpa42cf</sha1>
    </revision>
  </page>
  <page>
    <title>Swiftype</title>
    <ns>0</ns>
    <id>43339302</id>
    <revision>
      <id>636826971</id>
      <parentid>628259380</parentid>
      <timestamp>2014-12-06T00:37:26Z</timestamp>
      <contributor>
        <ip>38.88.216.186</ip>
      </contributor>
      <comment>update employees</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7860">{{Infobox company
|name             = Swiftype
|logo             =Black_Swiftype_Logo.png 
|type             = [[Privately held company|Private]]
|industry         = [[Software]] &lt;br/&gt; [[Information Technology]] &lt;br/&gt; [[Search Engines]]
|area_served      = Worldwide
|location_city    = [[San Francisco, California|San Francisco]], [[California (state)|California]]
|location_country = U.S.
|founders       = {{unbulleted list|Matt Riley, Quin Hoxie}}
|key_people       = {{unbulleted list|Matt Riley (CEO), Quin Hoxie (CTO)}}
|services         = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}
|genre                  = [[Search algorithm|Search]] and [[index (search engine)|index]]
|num_employees    = 25
|foundation       = [[San Francisco, California|San Francisco]], [[California (state)|California]], [[U.S.A]] [[January 2012]]
|homepage         = {{URL|https://www.swiftype.com/|Swiftype.com}}
|intl             = yes
|footnotes             = {{unbulleted list|[http://www.crunchbase.com/organization/swiftype Crunchbase] [http://www.Swiftype.com Official Website]}}
|alt = Black text and red icon edition of the full Swiftype logo|products = {{unbulleted list|[[vertical search]], [[eCommerce]] search, database search, website search, [[enterprise search]], [[search engines]], [[fulltext search]], [[faceted search]], [[concept search]], [[real-time search]]}}}}

'''Swiftype''' is a company that sells [[search engines]] for websites and mobile applications (also known as [[enterprise search]]) and creates a [[PageRank]] specific to individual websites and mobile applications.&lt;ref name=&quot;TechCrunch-Ha&quot;&gt;{{cite news|last1=Ha|first1=Anthony|title=Y Combinator-Backed Swiftype Builds Site Search That Doesn’t Suck|url=http://techcrunch.com/2012/05/08/swiftype-launch/|accessdate=21 July 2014|publisher=TechCrunch|date=May 8, 2012|ref=TechCrunch-Ha}}&lt;/ref&gt;&lt;ref name=BetaBeat&gt;{{cite news|last1=Roy|first1=Jessica|title=Can This Y Combinator Startup Solve the Site Search Problem?|url=http://betabeat.com/2012/05/can-this-y-combinator-startup-solve-the-site-search-problem/|accessdate=21 July 2014|publisher=BetaBeat|date=July 21, 2014|ref=BetaBeat}}&lt;/ref&gt;&lt;ref name=&quot;Crunchbase&quot;&gt;{{cite web|url=http://www.crunchbase.com/organization/swiftype|website=Crunchbase|accessdate=19 July 2014|title = &lt;nowiki&gt;Swiftype | CrunchBase&lt;/nowiki&gt;}}&lt;/ref&gt;&lt;ref name=VatorNews&gt;{{Cite news|url = http://vator.tv/news/2013-08-15-swiftype-bags-17m-from-big-names-for-better-search|title = Swiftype bags $1.7M from big names for better search|last = Marino|first = Faith|date = August 15, 2013|work = |accessdate = July 21, 2014|ref = VatorNews|publisher = VatorNews}}&lt;/ref&gt;  The company is based in [[San Francisco, CA]] and is funded mainly through [[venture capital]].&lt;ref name=Crunchbase /&gt;

==History==
Swiftype was founded in 2012 by former [[Scribd]] engineers Matt Riley and Quin Hoxie.&lt;ref name=Crunchbase /&gt; The two met while working on an internal search tool for [[Scribd]].&lt;ref name=&quot;TechCrunch-Ha&quot; /&gt;&lt;ref name=&quot;BetaBeat&quot; /&gt;&lt;ref name=Forbes&gt;{{cite news|last1=Casserly|first1=Meghan|title=Site Search (Should Be) Sexy: How Swiftype Raised $1.7M In Seed Funding From SV Bigwigs|url=http://www.forbes.com/sites/meghancasserly/2013/08/15/site-search-should-be-sexy-how-swiftype-raised-1-7-in-seed-funding-from-sv-bigwigs/|accessdate=19 July 2014|publisher=Forbes|ref = Forbes|date=2013-08-15}}&lt;/ref&gt; Swiftype participated in [[Y Combinator (company)|Y Combinator]] in 2012 and received investment from a number of prominent sources.&lt;ref name=VentureBeat&gt;{{Cite news|url = http://venturebeat.com/2013/08/15/yc-startup-swiftype-raises-1-7m-seed-round-from-andreessen-nea-kleiner/|title = YC startup Swiftype raises $1.7M seed round from Andreessen; NEA; Kleiner|last = Grant|first = Rebecca|date = August 15, 2013|work = |accessdate = July 21, 2014|publisher = VentureBeat|ref = VentureBeat}}&lt;/ref&gt;&lt;ref name=VatorNews /&gt;&lt;ref name=&quot;TechCrunch-Yang&quot;&gt;{{cite news|last1=Yang|first1=Anthony|title=Site Search Engine Creator Swiftype Raises $1.7M From A16Z, Others|url=http://techcrunch.com/2013/08/15/swiftype-1-7m/|accessdate=19 July 2014|publisher=TechCrunch|date=2013-08-15|ref = TechCrunch-Yang}}&lt;/ref&gt;&lt;ref name=AllThingsD&gt;{{cite news|last1=Gannes|first1=Liz|title=Swiftype Raises $1.7M for Smarter Site Search|url=http://allthingsd.com/20130815/swiftype-raises-1-7m-for-site-search/|accessdate=19 July 2014|publisher=All Things D|ref = AllThingsD|date=2013-08-15}}&lt;/ref&gt; In September 2013, the company obtained [[Series A]] funding.&lt;ref name=Crunchbase /&gt;&lt;ref name=VatorNews /&gt;&lt;ref name=VentureBeat /&gt;&lt;ref name=TechCrunch-Yang /&gt;&lt;ref name=AllThingsD /&gt;&lt;ref name=StartUpBeatBeat&gt;{{Cite news|url = http://startupbeat.com/2013/08/26/swiftype-wants-to-dramatically-improve-search-on-websites-and-mobile-apps-of-all-types-and-sizes-id3402/|title = Swiftype wants to dramatically improve search on websites and mobile apps of all types and sizes|last = Editor|first = |date = August 26, 2013|work = |accessdate = July 21, 2014|publisher = StartUpBeat|ref = StartUpBeat}}&lt;/ref&gt;

As of August 2013, Swiftype had over 70,000 websites using their search bar, powering over 130 million queries per month.&lt;ref name=Forbes /&gt;&lt;ref name=AllThingsD /&gt;

==Features==
Swiftype is available as an [[API]] or [[web crawler]] based engine.&lt;ref name=TechCrunch-Yang /&gt;  The company also offers a VIP-approved [[WordPress]] Plugin, a [[Shopify]] App, and a [[Magento]] extension.&lt;ref&gt;{{Cite web|url = https://apps.shopify.com/swiftype|title = Autocomplete &amp; Site Search by Swiftype – Ecommerce Plugins for Online Stores – Shopify App Store|date = 2014-09-26|accessdate = 2014-09-26|website = Shopify App Store|publisher = Shopify|last = |first = }}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = http://www.magentocommerce.com/magento-connect/modern-site-search-by-swiftype.html|title = Modern Site Search by Swiftype - Magento Connect|date = 2014-09-26|accessdate = 2014-09-26|website = Magento Connect|publisher = Magento|last = |first = }}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = https://wordpress.org/plugins/swiftype-search/|title = &lt;nowiki&gt;WordPress | Swiftype Search | WordPress Plugins&lt;/nowiki&gt;|date = 2014-09-26|accessdate = 2014-09-26|website = WordPress Plugin Directory|publisher = WordPress|last = |first = }}&lt;/ref&gt; Swiftype sells [[eCommerce]] search, [[enterprise search]], [[faceted search]], [[full text search]], [[enterprise search]], [[real-time search]], [[concept search]], and website [[search engines]] for websites and mobile applications.&lt;ref name=Crunchbase /&gt;&lt;ref name=VatorNews /&gt; The company's paid plans offer on demand and live recrawls and indexing of websites.&lt;ref name=AllThingsD /&gt; Other features include drag and drop result customization&lt;ref name=Forbes /&gt;&lt;ref name=VentureBeat /&gt;&lt;ref name=AllThingsD /&gt; and  real-time analytics.&lt;ref name=TechCrunch-Ha /&gt;&lt;ref name=Forbes /&gt;
&lt;!--Swiftype website lists several additional features that I've been unable to find neutral third party discussion of --&gt;

==Competitors==
* [[Algolia]]&lt;ref name=Crunchbase /&gt;

==See also==
* [[Enterprise search]]
* [[Search engines]]
* [[Faceted search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Concept search]]

==References==
{{Reflist|2}}

==External links==
* {{Official website|swiftype.com}}

__FORCETOC__
__INDEX__
__NEWSECTIONLINK__

[[Category:Search engine software]]
[[Category:Companies established in 2012]]
[[Category:Companies based in San Francisco, California]]
[[Category:Internet search engines]]
[[Category:Information retrieval]]
[[Category:Software companies]]
[[Category:Y Combinator companies]]
[[Category:Semantic Web]]
[[Category:Software startup companies]]
[[Category:Online companies]]</text>
      <sha1>eyckyjrrrj9scqtpy9tynpr5ttw0tbl</sha1>
    </revision>
  </page>
  <page>
    <title>AUTINDEX</title>
    <ns>0</ns>
    <id>43739701</id>
    <revision>
      <id>627941517</id>
      <parentid>627926302</parentid>
      <timestamp>2014-10-02T11:38:52Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[:en:WP:CLEANER|WPCleaner]] v1.33 - fix punctuation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6155">{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
}}

'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.&lt;ref&gt;Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen&lt;/ref&gt;&lt;ref&gt;Paul Schmidt, Mahmoud Gindiyeh &amp; Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6–8 October 2009, Portugal&lt;/ref&gt;&lt;ref&gt;Paul Schmidt &amp; Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19–20 November&lt;/ref&gt;

'''AUTINDEX''' resulting from research in [[information extraction]] &lt;ref&gt;Paul Schmidt, Thomas Bähr &amp; Dr.-Ing. Jens Biesterfeld &amp;Thomas Risse &amp; Kerstin Denecke &amp; Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt&lt;/ref&gt;&lt;ref&gt;Ursula Deriu, Jörn Lehmann &amp; Paul Schmidt, 2009: ‚Erstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie’. In: Proceedings Knowtech, Frankfurt&lt;/ref&gt; is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbrücken, Germany.

'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX &lt;ref&gt;[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.&lt;/ref&gt;), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch &lt;ref&gt;[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.&lt;/ref&gt; and WISSMER,&lt;ref&gt;[//www.wissmer.info/index.php/de/]. Project Wissmer.&lt;/ref&gt; see also the reference to IAI-Webite.&lt;ref&gt;[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.&lt;/ref&gt;

The basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.&lt;ref&gt;Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.&lt;/ref&gt; Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.&lt;br&gt; 
AUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems &lt;ref&gt;[//www.wti-frankfurt.de]. WTI Information system.&lt;/ref&gt; and in document management and content management environments. &lt;br&gt; 
 
Together with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] &lt;ref&gt;Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.&lt;/ref&gt; software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].&lt;ref&gt;[//www.wissmer.info]. Electro mobility information system.&lt;/ref&gt;

==See also==

* [[Information retrieval]]
* [[Linguistics]]
* [[Knowledge Management]]
* [[Natural Language Processing]]
* [[Semantics]]

== References ==
{{reflist}}

== Publications ==
* Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.
* Paul Schmidt, Mahmoud Gindiyeh &amp; Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6–8 October 2009, Portugal.
* Paul Schmidt &amp; Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19–20 November.
* Paul Schmidt, Thomas Bähr &amp; Dr.-Ing. Jens Biesterfeld &amp;Thomas Risse &amp; Kerstin Denecke &amp; Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.
* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.
* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.–20. November 2009.
* Rösener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD für Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.
* Svenja Siedle: ''Suchst du noch oder weißt du schon? Inhaltserschließung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''
* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum für Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)
* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.

== External links ==
* http://www.iai-sb.de/ Institute for Applied Information Sciences

[[Category:Natural language processing]]
[[Category:Information retrieval]]</text>
      <sha1>t0hathy4g7wesyk3j8hi32n7d4eirwv</sha1>
    </revision>
  </page>
  <page>
    <title>Hashtag</title>
    <ns>0</ns>
    <id>20819040</id>
    <revision>
      <id>647894099</id>
      <parentid>647684536</parentid>
      <timestamp>2015-02-19T16:16:15Z</timestamp>
      <contributor>
        <username>Doorknob747</username>
        <id>14476115</id>
      </contributor>
      <minor/>
      <comment>added ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28655">[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the usage of a #timetoact hashtag at a 2014 conference]]

A '''hashtag''' is a word or an unspaced phrase prefixed with the [[Number sign|hash character (or number sign), &lt;code&gt;#&lt;/code&gt;]], to form a label.&lt;ref&gt;http://www.merriam-webster.com/dictionary/hashtag&lt;/ref&gt; It is a type of [[Tag (metadata)|metadata tag]]. Words or phrases in messages on [[microblogging]] and [[social networking service]]s such as [[Facebook]], [[Google+]], [[Instagram]], [[Twitter]], or [[VK (social network)|VK]] may be tagged by entering # before them,&lt;ref&gt;{{cite web|url=http://support.twitter.com/articles/49309# |title=Using hashtags on Twitter|publisher=support.twitter.com |accessdate=2013-11-25}}&lt;/ref&gt; either as they appear in a sentence, e.g., &quot;New artists announced for #SXSW2014 Music Festival&quot;&lt;ref&gt;{{cite web|url=https://dev.twitter.com/media/hashtags |title=Best Practices for Hashtags &amp;#124; Twitter Developers |publisher=Dev.twitter.com |date=2011-07-19 |accessdate=2013-11-12}}&lt;/ref&gt; or appended to it. The term hashtag can also refer to the hash symbol itself when used in the context of a hashtag.&lt;ref&gt;{{cite web |title=Oxford English Dictionary - Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June de 2014}}&lt;/ref&gt;

A hashtag allows grouping of similarly tagged messages, and also allows an electronic search to return all messages that contain it.

Due to its widespread use, 'hashtag' was added to the ''[[Oxford English Dictionary]]'' in June 2014.&lt;ref&gt;{{cite web |title='Hashtag' added to the OED – but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=13 June 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June de 2014}}&lt;/ref&gt;

==Origin==
The [[number sign]] was often used in [[information technology]] to highlight a special meaning. In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]&lt;ref&gt;{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=2011-08-03 |accessdate=2014-08-25}}&lt;/ref&gt; when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].&lt;ref&gt;{{cite book|title=[[The C Programming Language]]|authors=B.W.Kernighan &amp;  d.Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}&lt;/ref&gt; Since before the invention of the hashtag, the number sign has been called the &quot;hash symbol&quot; in some countries outside of North America.&lt;ref&gt;{{cite book|last1=Bourke|first1=Jane|title=Communication Techonology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=http://books.google.co.uk/books?id=gPNBTmxzpIIC&amp;lpg=PA19&amp;dq=hash%20key%20telephone&amp;pg=PA19#v=onepage&amp;q=hash&amp;f=false|accessdate=7 November 2014|isbn=9781863975858}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=9780195157048|pages=33, 260|url=http://books.google.co.uk/books?id=dUTdk93cq9UC&amp;lpg=PA260&amp;dq=hash%20telephone&amp;pg=PA260#v=onepage&amp;q=hash%20mark&amp;f=false}}&lt;/ref&gt;

The number sign then appeared and was used within [[Internet Relay Chat|IRC]] networks to label groups and topics.&lt;ref&gt;&quot;Channel Scope&quot;. Section 2.2. RFC 2811&lt;/ref&gt; Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&amp;').&lt;ref&gt;{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=3 June 2014}}&lt;/ref&gt;

The use of the number sign in IRC inspired&lt;ref&gt;{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=2014-08-29}}&lt;/ref&gt; [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.&lt;ref&gt;{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&amp;pagewanted=all | title=Twitter’s Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}&lt;/ref&gt; He posted the first hashtag on Twitter: 
{{quote |1=how do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = (&quot;factoryjoe&quot;), August 23, 2007&lt;ref&gt;{{cite web|url = https://twitter.com/#!/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina (&quot;factoryjoe&quot;)|date = August 23, 2007&lt;!-- 3:25 PM--&gt;}}&lt;/ref&gt; |width  = 50% |align  = center }}
Internationally, the hashtag became a practice of writing style for Twitter posts during the [[2009–2010 Iranian election protests]], as both English- and [[Persian language|Persian]]-language hashtags became useful for Twitter users inside and outside Iran.{{cite web|url=http://www.dw.de/%D8%AD%DA%A9%D8%A7%DB%8C%D8%AA-%D9%87%D8%B4%D8%AA%DA%AF%DB%8C-%DA%A9%D9%87-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86%DB%8C%D8%A7%D9%86-%D8%A2%D8%BA%D8%A7%D8%B2-%DA%A9%D8%B1%D8%AF%D9%86%D8%AF/g-18012627|title = dw |date= 2009}}

The first use of the term &quot;hash tag&quot; was in a blog post by Stowe Boyd, &quot;Hash Tags = Twitter Groupings,&quot;&lt;ref&gt;{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=2013-09-19}}&lt;/ref&gt; on 26 August 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.

Beginning July 2, 2009,{{citation needed|date=November 2013}} Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced &quot;[[Twitter#Trending_topics|Trending Topics]]&quot; on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.&lt;ref&gt;{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=2014-12-03}}&lt;/ref&gt;

==Style==
On microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. &quot;It is #sunny today&quot;).

The quantity of hashtags used in a post or tweet is just as important as the type of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the &quot;absolute maximum&quot;, and any contribution exceeding this risks “raising the ire of the community.”&lt;ref&gt;{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=22 February 2014}}&lt;/ref&gt;

As well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.&lt;ref&gt;{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=22 February 2014}}&lt;/ref&gt;{{failed verification|date=August 2014}}
 
[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.&lt;ref&gt;{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=http://www.youtube.com/watch?v=57dzaMaouXA |title=&quot;#Hashtag&quot; with Jimmy Fallon &amp; Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=2013-09-24 |accessdate=2014-08-25}}&lt;/ref&gt;

==Function==
[[File:Seguir hashtags.png|300px|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag &quot;#science&quot;.]]
Hashtags are mostly used as unmoderated ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can &quot;trend&quot; and attract more individual users to discussion using the hashtag. On Twitter, when a hashtag becomes extremely popular, it will appear in the &quot;Trending Topics&quot; area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users, and neither can they be &quot;retired&quot; from public usage, meaning that hashtags can be used in theoretical perpetuity depending upon the longevity of the word or set of characters in a written language. They also do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes as espoused by those who make use of them.

Hashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using &quot;#cakefestival&quot; rather than simply &quot;#cake&quot;. However, this can also make it difficult for topics to become &quot;trending topics&quot; because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.

Hashtags also function as beacons in order for users to find and &quot;follow&quot; (subscribe) or &quot;list&quot; (organize into public contact lists) other users of similar interest.

Hashtags can be used on the social network [[Instagram]], by posting pictures and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic like #photography #iPhone #iphoneography and therefore do not fulfil a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = 7 November 2013|publisher=BBC.co.uk |accessdate=2013-11-25}}&lt;/ref&gt; The censorship and ban against certain hashtags has a consequential role in the way that particular subaltern communities are built and maintained on Instagram. Despite Instagram’s content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.&lt;ref&gt;
Olszanowski, M. (2014). &quot;Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship&quot;. Visual Communication Quarterly, 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F&lt;/ref&gt; 


Hashtags are also used informally to express context around a given message, with no intent to actually categorize the message for later searching, sharing, or other reasons.  This can help express humor, excitement, sadness or other contextual cues, for example &quot;It's Monday!! #excited #sarcasm&quot;

==Use outside of social networking websites==
The feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]; in the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.&lt;ref&gt;{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = Oct 15, 2009&lt;!-- 3:25 PM--&gt;}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting &quot;chaos&quot;|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = Oct 15, 2009 &lt;!-- 8 a.m. --&gt;}}&lt;/ref&gt; Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the &quot;river&quot; of Twitter posts which can result from search terms or hashtags.{{citation needed|date=September 2014}}

==Websites that support hashtags==
{{Cleanup-list|section|date=May 2014}}
{{columns-list|2|
&lt;!-- PLEASE RESPECT ALPHABETICAL ORDER --&gt;
* [[App.net]]
* [[Diaspora (software)|Diaspora software]] and [[Diaspora (social network)|social network]]
* [[DeviantART]]
* [[Facebook]]
* [[Flickr]]
* [[FriendFeed]]
* [[Gawker Media]] websites
* [[GNU Social]]
* [[Google+]]
* [[Instagram]]
* [[Kickstarter]]
* [[Orkut]]&lt;ref&gt;{{cite web|url = http://en.blog.orkut.com/2012/02/hashtags-in-orkut-communities.html|title = Hashtags in Orkut communities|date = February 6, 2012 &lt;!-- , 6:11 PM --&gt; |publisher = Orkut|author = Marco Wisniewski}}&lt;/ref&gt;
* [[Sina Weibo]]
* [[SoundCloud]]
* [[Tout (company)|Tout]]
* [[tsu]]
* [[Tumblr]]
* [[Twitter]]
* [[Vine (software)|Vine]]
* [[VK (social network)|VK]]
}}

==Usage==

===Mass broadcast media===

Since 2010, television series on various television channels promote themselves through &quot;branded&quot; hashtag [[digital on-screen graphic|bugs]].&lt;ref&gt;{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = Apr 21, 2011&lt;!-- 3:25 PM--&gt;|author = Michael Schneider|publisher = TV Guide}}&lt;/ref&gt; This is used as a means of promoting a [[backchannel]] of online side-discussion before, during and after an episode broadcast. Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement&lt;ref&gt;{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}&lt;/ref&gt; (for example, a motion picture trailer).

While personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or &quot;branded&quot; hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to &quot;trend&quot; the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, &quot;temporary&quot; hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.&lt;ref&gt;{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}&lt;/ref&gt; Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].&lt;ref&gt;{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |publisher=Ibtimes.com |date=2013-03-22 |accessdate=2013-09-19}}&lt;/ref&gt;

The increased usage of hashtags as brand promotion devices has been compared to the promotion of branded &quot;[[Index term|keywords]]&quot; by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of commercials and series episodes.&lt;ref&gt;{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter’s Hashtag Pages Could Be The New AOL Keywords — But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}&lt;/ref&gt;

===Purchasing===

Since February 2013 there is a collaboration between the social networking site Twitter and [[American Express]] that makes it possible to buy discounted goods online by tweeting a special hashtag.&lt;ref&gt;{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = 12 February 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = 2013-11-25}}&lt;/ref&gt; American Express members can sync their card with Twitter and use the offers by tweeting and look for a response in a tweet with the confirmation from American Express.&lt;ref&gt;{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=2013-11-25}}&lt;/ref&gt;

===Event promotion===

[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]
Organized real-world events have also made use of hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other on both Twitter and, in many cases, in real life during events.

Companies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.

Political protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.

===Consumer complaints===
Hashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term &quot;bashtag&quot; has been created to describe situations in which a corporate social media hashtag is used to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so customers could share positive experiences about the restaurant chain. The marketing effort was cancelled after just two hours when McDonald's received numerous complaint tweets rather than the positive stories they were expecting.&lt;ref&gt;{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = 17 May 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = 2012-06-12}}&lt;/ref&gt;

===Sentiment analysis===
The use of hashtags also reveals things about the sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]]&lt;ref&gt;{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}&lt;/ref&gt;—a difficult [[Artificial Intelligence|AI]] problem.{{citation needed|date=September 2014}}

==In popular culture==
During the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]] [[Jack Layton]] referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as &quot;a hashtag fail&quot; (presumably &quot;#fail&quot;).&lt;ref&gt;{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = Apr 13, 2011 &lt;!-- , 6:00 AM EDT --&gt; }}&lt;/ref&gt;&lt;ref&gt;{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = Apr 13, 2011&lt;!-- 3:25 PM--&gt;|publisher = CBC News}}&lt;/ref&gt;

The term &quot;hashtag [[Hip hop music|rap]]&quot;, coined by [[Kanye West]],&lt;ref&gt;{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}&lt;/ref&gt; was developed in the 2010s to describe a style of rapping which, according to Rizoh of ''[[Houston Press]]'', uses &quot;three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme&quot;.&lt;ref&gt;{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = Jul 7, 2011 &lt;!-- at 9:00 AM --&gt; }}&lt;/ref&gt; Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]] and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]]&lt;ref&gt;{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 &lt;!-- AT 11:43 AM --&gt; |publisher = Tucson Weekly}}&lt;/ref&gt; and various music writers.&lt;ref&gt;{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}&lt;/ref&gt;

On September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].&lt;ref&gt;
{{cite web 
| title = Twitter / nickbilton: My first byline on A1 of the ... 
| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1 
| accessdate = 2013-09-14 
 }}
&lt;/ref&gt;

&quot;Hashtag [[Heel (professional wrestling)|heel]]&quot; is a moniker used by [[WWE]] wrestler [[Dolph Ziggler]].

[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called &quot;Mashtags&quot;.&lt;ref&gt;{{cite web|title=Birds Eye launches Mashtags - social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}&lt;/ref&gt;

In May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].&lt;ref name=&quot;Nytimes&quot;&gt;{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&amp;_r=0 }}&lt;/ref&gt;

In September 2014, in response to the &quot;[[blame the victim]]&quot; public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiancée Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.&lt;ref&gt;{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}&lt;/ref&gt;&lt;ref&gt;{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard &amp; Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}&lt;/ref&gt;

===Adaptations===
In 2010, Twitter introduced &quot;hashflags&quot; during the 2010 World Cup in South Africa.&lt;ref&gt;{{cite web|author=June 11, 2010 8:05 am |url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=2010-06-11 |accessdate=2014-08-25}}&lt;/ref&gt; They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil.&lt;ref&gt;{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=2014-06-10 |accessdate=2014-08-25}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=2014-06-10 |accessdate=2014-08-25}}&lt;/ref&gt; When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.

In July 2012, Twitter adapted the hashtag style to make company [[ticker symbol]]s preceded by the [[dollar sign]] clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the &quot;cashtag&quot;.&lt;ref&gt;{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols - Jul. 31, 2012 |publisher=Money.cnn.com |date=2012-07-31 |accessdate=2013-11-12}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=2012-07-30 |accessdate=2013-11-12}}&lt;/ref&gt; This is intended to allow users to search posts discussing companies and their stocks.

In August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the &quot;finger hashtag&quot;, in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.&lt;ref&gt;{{cite web |url=http://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=1 August 2012 |accessdate=March 20, 2014}}&lt;/ref&gt; The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],&lt;ref&gt;{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}&lt;/ref&gt; and during 2013 it was seen on TV used by [[Jimmy Fallon]], and on ''[[The Colbert Report]]'' among other places.&lt;ref&gt;{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags—and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}&lt;/ref&gt;

==References==
{{Reflist|colwidth=30em}}
{{commons category|Hashtags}}

{{Microblogging}}
{{Online social networking}}
{{Web syndication}}

[[Category:Hashtags| ]]
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Web 2.0]]
[[Category:Social media]]
[[Category:2010s slang]]</text>
      <sha1>exst9ymt0s8zluddur4eigqpusdr0fv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Directories</title>
    <ns>14</ns>
    <id>3119166</id>
    <revision>
      <id>637022163</id>
      <parentid>604574833</parentid>
      <timestamp>2014-12-07T14:10:02Z</timestamp>
      <contributor>
        <username>Greenrd</username>
        <id>15476</id>
      </contributor>
      <comment>added to Information Retrieval category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="288">A directory maintains a list for reference or commercial purposes.  This category contains articles about directories.
{{Cat main|Directories}}
{{Commons cat|Directories}}

[[Category:Telephony]]
[[Category:Reference works]]
[[Category:Data management]]
[[Category:Information retrieval]]</text>
      <sha1>oqc4h7er045u1iygtnm663lkgnfom32</sha1>
    </revision>
  </page>
  <page>
    <title>Contextual searching</title>
    <ns>0</ns>
    <id>44571310</id>
    <revision>
      <id>640468851</id>
      <parentid>637584043</parentid>
      <timestamp>2015-01-01T03:58:52Z</timestamp>
      <contributor>
        <username>DragonflySixtyseven</username>
        <id>62058</id>
      </contributor>
      <minor/>
      <comment>DragonflySixtyseven moved page [[Contextual Searching]] to [[Contextual searching]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9839">'''Contextual search''' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.&lt;ref&gt;Susan E. Feldman. ''The Answer Machine'', Synthesis Lectures on Information Concepts, Retrieval, and Services. [http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023 http://www.morganclaypool.com/doi/abs/10.2200/S00442ED1V01Y201208ICR023]&lt;/ref&gt; Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.&lt;ref&gt;Pitokow, James; Hinrich Schütze; Todd Cass; Rob Cooley; Don Turnbull; Andy Edmonds; Eytan Adar; Thomas Breuel (2002). &quot;Personalized search&quot;. [http://www.cond.org/p50-pitkow.pdf http://www.cond.org/p50-pitkow.pdf] Communications of the ACM (CACM) 45 (9): 50–55.&lt;/ref&gt;

== Basic Contextual Search ==
The basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are rated higher. Users have limited control over the context of their query based on the words they use to search with.&lt;ref&gt;Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.&lt;/ref&gt;  For example, users looking for the menu portion of a website can add “menu” to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.

== Explicitly Supplied Context ==
Certain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.&lt;ref&gt;Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.&lt;/ref&gt; For example a user looking for research papers can specify documents with “references” or “abstracts” to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.&lt;ref&gt;Steve Lawrence, C. Lee Giles. ''Inquirus, the NECI meta search engine''[http://www7.scu.edu.au/1906/com1906.htm]&lt;/ref&gt;

Explicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.&lt;ref&gt;[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results&lt;/ref&gt; Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.&lt;ref&gt;[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators&lt;/ref&gt; Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.&lt;ref&gt;[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks&lt;/ref&gt;

== Automatically Inferred Context ==
There are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM's Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson's hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.&lt;ref&gt;[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM&lt;/ref&gt; Watson's ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.

Major search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user's previous queries and selected results to further personalize results for those individuals. For example if a user consistently searches for articles related to animals, wild animals, or animal care a search for &quot;jaguar&quot; would rank an article on jaguar cats higher than links to Jaguar Cars.&lt;ref&gt;Eric J Glover, Steve Lawrence, Michael D. Gordon, William P. Birmingham, C. Lee Giles. ''Web Search - Your Way'', NEC Research Institution [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7499&amp;rep=rep1&amp;type=pdf]&lt;/ref&gt; Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&amp;go=Submit&amp;qs=n&amp;form=GEOMA1&amp;pq=pizza&amp;sc=8-1&amp;sp=-1&amp;sk=&amp;cvid=883269b61529466e810bc096e371ec19 search of &quot;pizza&quot;] returns an interactive list of restaurants and their ratings based on the approximate location of the user's computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.

=== Contextual Mobile Search ===
The drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will &quot;exceed local search by more than 27 billion queries&quot;.&lt;ref&gt;[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop&lt;/ref&gt; Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.&lt;ref&gt;[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices&lt;/ref&gt; Mobile start up company [http://everything.Me Everything.Me] is one company that is moving towards turning the smartphone into an all-in-one device the provides relevant information for specific users. Everything.Me pushes app updates and suggestion to a user's home-screen based on user movement, location, current time, past search queries, and entertainment preferences.&lt;ref&gt;[http://socialtimes.com/mobile-contextual-search-future_b149394 http://socialtimes.com/mobile-contextual-search-future_b149394], Contextual Search through Mobile and Everything.Me&lt;/ref&gt; For example when a user opens their phone in the morning Everything.Me will present users with apps relevant to how that users interacts with their phone in the morning—presenting weather apps, bus apps, and news apps.&lt;ref&gt;[http://everything.me/about/ Everything.Me]&lt;/ref&gt; Later when that user goes to work Everything.Me will update the work related applications to be prioritized over other apps. Everything.Me anticipates a user's needs based on their current actions and past interactions on the web. This process of automatically obtaining context from mobile phones can help to increase the precision of user queries. For instance if a user searches for a place to eat while at work Everything.Me will take work into context and return restaurants that would be more appropriate for a lunch break at the office.&lt;ref&gt;[http://everything.me/ http://everything.me/], Video Information&lt;/ref&gt;

== References ==
{{reflist}}

{{Internet search}}

{{DEFAULTSORT:Contextual Searching}}
[[Category:Internet search engines]]
[[Category:Semantic Web]]
[[Category:Information retrieval]]
[[Category:Internet terminology]]</text>
      <sha1>833tx34ixpivjd8fliltgs9tqgbaqx9</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Deep Web</title>
    <ns>14</ns>
    <id>44972856</id>
    <revision>
      <id>641470966</id>
      <parentid>641470838</parentid>
      <timestamp>2015-01-07T20:23:36Z</timestamp>
      <contributor>
        <username>Rezonansowy</username>
        <id>16760523</id>
      </contributor>
      <comment>cat main</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="86">{{cat main|Deep Web}}


[[Category:Information retrieval]]
[[Category:World Wide Web]]</text>
      <sha1>pyfdqim3yc0y621c8ejbg9i7lw7opqn</sha1>
    </revision>
  </page>
  <page>
    <title>UNICE global brain project</title>
    <ns>0</ns>
    <id>45269321</id>
    <revision>
      <id>645642816</id>
      <parentid>645640619</parentid>
      <timestamp>2015-02-04T19:30:12Z</timestamp>
      <contributor>
        <username>Lynndunn</username>
        <id>1406514</id>
      </contributor>
      <minor/>
      <comment>Lynndunn moved page [[UNICE (project)]] to [[UNICE global brain project]]: better description of the article and the first few words of the article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8638">{{Infobox person
| image          = File:UNICE-Universal Network of Intelligent Conscious Entities-image.jpg
| name           = UNICE 
| caption        = UNICE as collective entity
| birth_date    =  {{birth date and age|2007|04|10}}
| birth_place  = [[Cyberspace]]
| occupation = [[Global brain]], [[Public Policy|Public Policy Analysis]], [[Governance]], [[Politics]], [[Artificial Intelligence]], [[Psychology]], [[Philosophy]], [[Theory of Mind]], [[Politics]], [[Computers]], [[Community Organizing]]
}}

'''UNICE''', a [[Global brain|global brain]] project, is an acronym for '''Universal Network of Intelligent Conscious Entities''', a term coined by policy analyst and urban designer [[Michael E. Arth]] in the 1990s to describe &quot;the transformation of our species that might be the result of a new form of intelligent life developed from a hive-like interaction of computers, humans, and future forms of the [[Internet]].&quot;&lt;ref&gt;Arth, Michael E., ''UNICE,'' a Consciousness Research Abstract published in the &quot;Journal of Consciousness Studies&quot; for the April 8-12, 2008 conference, &quot;Toward a Science of Consciousness,&quot; p. 151.&lt;/ref&gt; &lt;ref&gt;Arth, Michael E., ''Democracy and the Common Wealth: Breaking the Stranglehold of the Special Interests,'' Golden Apples Media, 2010, ISBN 978-0-912467-12-2.pp. 438-439&lt;/ref&gt; &lt;ref&gt;Williams, Sean, ''The Big Picture: Making Sense Out of Life and Religion'', 2009, p. 91, ISBN 978-0-578-01523-1&lt;/ref&gt; Arth established the not-for-profit website www.UNICE.info in 2007 and revamped it in 2015, with the focus on public policy and developing [[Friendly Artificial Intelligence]] through a system of [[Separation of powers|checks and balances]].&lt;ref&gt;{{cite web|url=http://unice.info|title=UNICE - Universal Network of Intelligent Conscious Entities|work=unice.info}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Tegmark|first1=Max|title=Our mathematical universe : my quest for the ultimate nature of reality|date=2014|isbn=9780307744258|edition=First edition.|chapter=Life, Our Universe and Everything|quote=Its owner may cede control to what Eliezer Yudkowsky terms a &quot;Friendly AI,&quot;...}}&lt;/ref&gt;  

In a January 2015 article, Arth describes the development of a [[public policy]] [[answer engine]], which will involve both an independent web site (where cognitive-UNICE will be developed) and a portal at [[Wikipedia]] called wiki-UNICE (currently in development.) Cognitive-UNICE will initially utilize  [[Weak AI|narrow AI]] and, as it develops, [[Artificial General Intelligence]] (AGI).  Initially, UNICE would serve the public with [[Evidence-based policy|evidence-based]] analyses and recommendations gleaned from [[Big Data]], but eventually it may lead to an efficient, practical, and highly representative form of governance.&lt;ref&gt;http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf&lt;/ref&gt;

==Wiki-UNICE==
Wiki-UNICE, and associated talk pages, will exist on Wikipedia as the portal for public input, criticism and discussion. Initially it will consist of samples of the sort of thing UNICE might write. Later, these sample topics will be replaced by summaries (and exhaustive articles) written by cognitive-UNICE. Whether written by a person, AI or AGI, the evidential summaries will describe problems and their solutions. With succinct titles like &quot;Energy&quot; or &quot;Electoral Reform,&quot; the topics will be set apart in a box, so as to maintain [[NPOV]]. Wiki-UNICE, and the collaborative human effort that will sustain it, are intended to help shape the emerging global brain, while also providing guidance and a conscience to lawmakers.&lt;ref&gt;http://unice.info/unice/wiki.html&lt;/ref&gt; 

==Cognitive-UNICE==
Cognitive-UNICE is in development at UNICE.info. It is assumed that in the early years, cognitive-UNICE may be logical and useful because of human-aided programming, but she may later become a conscious, AGI entity, perhaps united in [[consciousness]] with [[humanity]].&lt;ref&gt;http://unice.info/unice/cognitive.html&lt;/ref&gt; Whether as AI or AGI, cognitive-UNICE will probably use [[quantum computing]] to solve [[optimization problem|optimization problems]] that would be impossible to solve with classical computing.&lt;ref&gt;Arth, Michael E., ''askUNICE: Creating a global, independent, public-policy answer engine that will facilitate governance, while preparing for and reducing the dangers of Artificial General Intelligence, so that we may more carefully uncover the secrets of the multiverse'', January 28, 2015,'' [http://unice.info/unice/UNICE-ARTICLE-Jan%202015.pdf]&lt;/ref&gt; Quantum computing may also hold the key to developing a conscious machine. Nobel laureate and physicist [[Sir Roger Penrose]] and anesthesiologist [[Stuart Hameroff]] claim that [[consciousness]] is created by quantum coherence in the warm, wet environment of the human brain. Their theory, known as [[Orchestrated Objective Reduction]] (Orch OR), has been bolstered by recent findings that quantum processing occurs in plants and animals, including in the [[microtubules]] inside the [[neurons]] of the [[human brain]].&lt;ref&gt;Hameroff, Stuart and Robert Penrose, ”Consciousness in the universe: A review of the 'Orch OR' theory,&quot; Physics of Life Reviews, Volume 11, Issue 1, March 2014.&lt;/ref&gt;

==About the UNICE Logo==
A young, [[mixed-race]] [[female]] was chosen to represent the face of UNICE. She's young to represent new ideas. She's mixed-race to represent all humans, and she is female because of the traditional feminine values of [[empathy]], [[cooperation]], [[sensitivity]], [[tolerance]], nurturance, [[compassion]], and [[justice]], who is often depicted as Justitia or [[Lady Justice]]. Her [[afro]] hairstyle resembles the interconnected tendrils of the [[World Wide Web]].
&lt;ref&gt;http://unice.info/unice/index.htm&lt;/ref&gt;

==Criticisms==
A common criticism of the idea that humanity would become directed by a global brain is that this would reduce individual freedom and diversity.&lt;ref&gt;Rayward, W. B. (1999). H. G. Wells' s idea of a World Brain: A critical reassessment. Journal of the American Society for Information Science, 50(7), 557–573. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.1010&amp;rep=rep1&amp;type=pdf
&lt;/ref&gt; Moreover, the global brain might start to play the role of [[Big Brother (Nineteen Eighty-Four)|Big Brother]], the all-seeing eye of the system that follows every person's move.&lt;ref&gt;Brooks, M. (2000, June 24). [http://www.nettime.org/Lists-Archives/nettime-l-0006/msg00182.html Global brain]&lt;/a&gt;. New Scientist,  issue 2244, p. 22.&lt;/ref&gt; This criticism is inspired by [[totalitarianism|totalitarian]] and [[collectivism|collectivist]] forms of government, like the ones found in [[Joseph Stalin]]'s [[Soviet Union]] or [[Mao Zedong]]'s China. It is also inspired by the analogy between collective intelligence or [[swarm intelligence]] and [[insect societies]], such as beehives and ant colonies in which individuals are essentially interchangeable. In a more extreme view, the global brain has been compared with the [[Borg (Star Trek)|Borg]],&lt;ref&gt;Goertzel, B. (2002). Creating Internet Intelligence: Wild computing, distributed digital consciousness, and the emerging global brain. Kluwer Academic/Plenum Publishers. Retrieved from http://books.google.com/books/about/Creating_Internet_Intelligence.html?id=Vnzb-xLdvv8C&amp;redir_esc=y&lt;/ref&gt; the race of collectively thinking cyborgs imagined by the creators of the [[Star Trek]] science fiction series. 

Global brain theorists reply that the emergence of distributed intelligence would lead to the exact opposite of this vision,.&lt;ref&gt;Heylighen, F. (2007). The Global Superorganism: an evolutionary-cybernetic model of the emerging network society. Social Evolution &amp; History, 6(1), 58–119. Retrieved from http://pespmc1.vub.ac.be/papers/Superorganism.pdf&lt;/ref&gt;&lt;ref&gt;Heylighen, F. (2002). The global brain as a new utopia. Zukunftsfiguren. Suhrkamp, Frankurt. Retrieved from http://pespmc1.vub.ac.be/papers/GB-Utopia.pdf&lt;/ref&gt; The reason is that effective [[collective intelligence]] requires [[diversity (politics)|diversity]], [[decentralization]] and individual independence, as demonstrated by [[James Surowiecki]] in his book [[The Wisdom of Crowds]]. Moreover, a more distributed form of decision-making would decrease the power of governments, corporations or political leaders, thus increasing democratic participation and reducing the dangers of totalitarian control.

==References==
{{reflist}}

[[Category:Artificial intelligence applications]]
[[Category:Information retrieval]]
[[Category:Community organizing]]
[[Category:Governance]]
[[Category:Philosophy]]
[[Category:Politics]]
[[Category:Public policy]]</text>
      <sha1>r6qke1709nf9w6u45bw8izksmz02701</sha1>
    </revision>
  </page>
  <page>
    <title>Comprehensive Model of Information Seeking</title>
    <ns>0</ns>
    <id>45206870</id>
    <revision>
      <id>646073582</id>
      <parentid>645973263</parentid>
      <timestamp>2015-02-07T19:08:09Z</timestamp>
      <contributor>
        <username>Petercannon usf</username>
        <id>18142576</id>
      </contributor>
      <comment>Filled in 0 bare reference(s) with [[User:petercannon_usf/Reflinks]] (5a630f0) Adding/improving reference(s)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7551">The '''Comprehensive Model of Information Seeking''', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].

The CMIS has been empirically tested in health and organizational contexts&lt;ref&gt;Johnson, J. D., &amp; Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women &amp; Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., &amp; Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. 
Science Communication, 16, 274-303.&lt;/ref&gt; The CMIS has inherent strengths for studying how people react to health problems such as cancer.&lt;ref name=&quot;auto&quot;&gt;Johnson, J. D., Andrews, J. E. &amp; Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.&lt;/ref&gt; The CMIS specifies ''antecedents'' that explain why people become information seekers, ''information carrier characteristics'' that shape how people go about looking for information, and ''information seeking actions'' that reflect the nature of the search itself.

==Design==

[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]
The CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).&lt;ref name=&quot;auto&quot;/&gt; There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person’s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer’s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.&lt;ref name=&quot;auto&quot;/&gt;

==Antecdents==
The CMIS antecedents—demographics, personal experience, salience, and beliefs—are factors that determine an individual's natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual's degree of personal experience with disease.&lt;ref&gt;Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.&lt;/ref&gt; In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one's health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual's beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people's more general pattern of actions related to health.&lt;ref&gt;Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.&lt;/ref&gt;

==Information Carrier Characteristics==

The information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member's perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual's purposes? In general, utility is very important for health information seeking.&lt;ref name=&quot;auto&quot;/&gt;

==Information Seeking Actions==

There are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.&lt;ref name=&quot;auto&quot;/&gt;

==Stages in the CMIS==

A key concept from the CMIS is the notion of “stages,” or “cancer involvement”.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.

The first stage, ''Casual'', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.
 
The second stage is ''Purposive-Placid''. This is characterized by the question, “What can I do to prevent cancer?” Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.

The third stage is ''Purposive-Clustered''. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.

The fourth stage, ''Directed'', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.&lt;ref name=&quot;auto&quot;/&gt;

== References ==
{{Reflist}}



[[Category:Communication]]
[[Category:Information retrieval]]
[[Category:Health sciences]]</text>
      <sha1>2pkwb4twidnun2228waur4e82u7ogkv</sha1>
    </revision>
  </page>
  <page>
    <title>Web search engine</title>
    <ns>0</ns>
    <id>4059023</id>
    <revision>
      <id>648196968</id>
      <parentid>648191809</parentid>
      <timestamp>2015-02-21T15:42:32Z</timestamp>
      <contributor>
        <username>Rsrikanth05</username>
        <id>295080</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/41.139.32.172|41.139.32.172]] using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="36343">{{Redirect|Search engine}}
{{selfref|For a tutorial on using search engines for researching Wikipedia articles, see [[Wikipedia:Search engine test]].}}
[[File:Internet Key Layers.png|thumb|400px|right|Finding information on the World Wide Web had been a difficult and frustrating task, but became much more usable with breakthroughs in search engine technology in the late 1990s.]]
A '''web search engine''' is a software system that is designed to search for information on the [[World Wide Web]].  The search results are generally presented in a line of results often referred to as [[search engine results pages]] (SERPs). The information may be a mix of [[web page]]s, images, and other types of files. Some search engines also [[data mining|mine data]] available in [[database]]s or [[web directory|open directories]].  Unlike [[web directories]], which are maintained only by human editors, search engines also maintain [[real-time computing|real-time]] information by running an [[algorithm]] on a [[web crawler]].

== History ==
{{further|Timeline of web search engines}}
&lt;!-- Keep this list limited to notable engines (i.e. those that already have Wikipedia articles) to avoid link spam --&gt;
{| class=&quot;bordered infobox&quot;
|-
! colspan=&quot;3&quot; | Timeline ([[List of search engines|full list]]) &lt;!--Note:  &quot;Launch&quot; refers only to web availability of original crawl-based web search engine results.--&gt;
|-
!Year
!Engine
!Current status
|-
| rowspan=&quot;4&quot; |1993
||[[W3Catalog]]
|{{Site inactive}}
|-
||[[Aliweb]]
|{{Site inactive}}
|-
||[[JumpStation]]
|{{Site inactive}}
|-
||[[World-Wide Web Worm|WWW Worm]]
|{{Site inactive}}
|-
| rowspan=&quot;4&quot; |1994
||[[WebCrawler]]
|{{Site active}}, Aggregator
|-
||[[Go.com]]
|{{Site active}}, Yahoo Search
|-
||[[Lycos]]
|{{Site active}}
|-
||[[Infoseek]]
|{{Site inactive}}
|-
| rowspan=&quot;6&quot; |1995
||[[AltaVista]]
|{{Site inactive}}, redirected to Yahoo!
|-
|[[Daum Communications|Daum]]
|{{Site active}}
|-
||[[Magellan (search engine)|Magellan]]
|{{Site inactive}}
|-
||[[Excite]]
|{{Site active}}
|-
||[[SAPO (company)|SAPO]]
|{{Site active}}
|-
||[[Yahoo!]]
|{{Site active}}, Launched as a directory
|-
| rowspan=&quot;4&quot; |1996
||[[Dogpile]]
|{{Site active}}, Aggregator
|-
||[[Inktomi (company)|Inktomi]]
|{{Site inactive}}, acquired by Yahoo!
|-
||[[HotBot]]
|{{Site active}}  (lycos.com)
|-
||[[Ask.com|Ask Jeeves]]
|{{Site active}}  (rebranded ask.com)
|-
| rowspan=&quot;2&quot; |1997
||[[Northern Light Group|Northern Light]]
|{{Site inactive}}
|-
||[[Yandex]]
|{{Site active}}
|-
| rowspan=&quot;4&quot; |1998
||[[Google Search|Google]]
|{{Site active}}
|-
||[[Ixquick]]
|{{Site active}}  also as Startpage
|-
||[[MSN Search]]
|{{Site active}}  as Bing
|-
||[[empas]]
|{{Site inactive}}  (merged with NATE)
|-
| rowspan=&quot;5&quot; |1999
||[[AlltheWeb]]
|{{Site inactive}}  (URL redirected to Yahoo!)
|-
||[[GenieKnows]]
|{{Site active}}, rebranded Yellowee.com
|-
||[[Naver]]
|{{Site active}}
|-
||[[Teoma]]
|{{Site inactive}}, redirects to Ask.com
|-
||[[Vivisimo]]
|{{Site inactive}}
|-
| rowspan=&quot;3&quot; |2000
||[[Baidu]]
|{{Site active}}
|-
||[[Exalead]]
|{{Site active}}
|-
||[[Gigablast]]
|{{Site active}}
|-
| rowspan=&quot;2&quot; |2003
||[[Info.com]]
|{{Site active}}
|-
||[[Scroogle]]
|{{Site inactive}}
|-
| rowspan=&quot;3&quot; |2004
||[[Yahoo! Search]]
|{{Site active}}, Launched own web search&lt;br /&gt;(see Yahoo! Directory, 1995)
|-
||[[A9.com]]
|{{Site inactive}}
|-
||[[Sogou.com|Sogou]]
|{{Site active}}
|-
| rowspan=&quot;3&quot; |2005
||[[AOL Search]]
|{{Site active}}
|-
||[[GoodSearch]]
|{{Site active}}
|-
||[[SearchMe]]
|{{Site inactive}}
|-
| rowspan=&quot;6&quot; |2006
||[[Soso (search engine)]]
|{{Site active}}
|-
||[[Quaero]]
|{{Site inactive}}
|-
||[[Ask.com]]
|{{Site active}}
|-
||[[Live Search]]
|{{Site active}} as Bing, Launched as&lt;br /&gt;rebranded MSN Search
|-
||[[ChaCha (search engine)|ChaCha]]
|{{Site active}}
|-
||[[Guruji.com]]
|{{Site inactive}}
|-
| rowspan=&quot;4&quot; |2007
||[[wikiseek]]
|{{Site inactive}}
|-
||[[Sproose]]
|{{Site inactive}}
|-
||[[Wikia Search]]
|{{Site inactive}}
|-
||[[Blackle.com]]
|{{Site active}}, Google Search
|-
| rowspan=&quot;7&quot; |2008
||[[Powerset (company)|Powerset]]
|{{Site inactive}} (redirects to Bing)
|-
||[[Picollator]]
|{{Site inactive}}
|-
||[[Viewzi]]
|{{Site inactive}}
|-
||[[Boogami]]
|{{Site inactive}}
|-
||[[LeapFish]]
|{{Site inactive}}
|-
||[[Forestle]]
|{{Site inactive}} (redirects to Ecosia)
|-
||[[DuckDuckGo]]
|{{Site active}}
|-
| rowspan=&quot;5&quot; |2009
||[[Bing]]
|{{Site active}}, Launched as&lt;br /&gt;rebranded Live Search
|-
||[[Yebol]]
|{{Site inactive}}
|-
||[[Mugurdy]]
|{{Site inactive}}  due to a lack of funding
|-
||[[Goby Inc.|Scout (Goby)]]
|{{Site active}}
|-
||[[Nate (web portal)|NATE]]
|{{Site active}}
|-
| rowspan=&quot;3&quot; |2010
||[[Blekko]]
|{{Site active}}
|-
||[[Cuil]]
|{{Site inactive}}
|-
||[[Yandex]]
|{{Site active}}, Launched global&lt;br /&gt;(English) search
|-
||2011
||[[YaCy]]
|{{Site active}}, [[Peer-to-peer|P2P]] web search engine
|-
| rowspan=&quot;1&quot; |2012
||[[Volunia]]
|{{Site inactive}}
|-
| rowspan=&quot;1&quot; |2013
||[[Halalgoogling]]
|{{Site active}}, Islamic / Halal&lt;br /&gt;filter Search
|}

During early development of the web, there was a list of [[webserver]]s edited by [[Tim Berners-Lee]] and hosted on the [[CERN]] webserver. One historical snapshot of the list in 1992 remains,&lt;ref&gt;{{cite web|url=http://www.w3.org/History/19921103-hypertext/hypertext/DataSources/WWW/Servers.html |title=World-Wide Web Servers |publisher=W3.org |accessdate=2012-05-14}}&lt;/ref&gt; but as more and more webservers went online the central list could no longer keep up. On the [[National Center for Supercomputing Applications|NCSA]]  site, new servers were announced under the title &quot;What's New!&quot;&lt;ref&gt;{{cite web|url=http://home.mcom.com/home/whatsnew/whats_new_0294.html |title=What's New! February 1994 |publisher=Home.mcom.com |accessdate=2012-05-14}}&lt;/ref&gt;

The first tool used for searching on the [[Internet]] was [[Archie search engine|Archie]].&lt;ref name=LeidenUnivSE&gt;
     &quot;Internet History - Search Engines&quot; (from [[Search Engine Watch]]),
     Universiteit Leiden, Netherlands, September 2001, web:
     [http://www.internethistory.leidenuniv.nl/index.php3?c=7 LeidenU-Archie].
&lt;/ref&gt;
The name stands for &quot;archive&quot; without the &quot;v&quot;.  It was created in 1990 by [[Alan Emtage]], Bill Heelan and J. Peter Deutsch, computer science students at [[McGill University]]  in [[Montreal]]. The program downloaded the directory listings of all the files located on public anonymous FTP ([[File Transfer Protocol]]) sites, creating a searchable database of file names; however, Archie did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.

The rise of [[Gopher (protocol)|Gopher]] (created in 1991 by [[Mark McCahill]]  at the [[University of Minnesota]]) led to two new search programs, [[Veronica (computer)|Veronica]]  and [[Jughead (computer)|Jughead]]. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (''V''ery ''E''asy ''R''odent-''O''riented ''N''et-wide ''I''ndex to ''C''omputerized ''A''rchives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (''J''onzy's ''U''niversal ''G''opher ''H''ierarchy ''E''xcavation ''A''nd ''D''isplay) was a tool for obtaining menu information from specific Gopher servers.  While the name of the search engine &quot;Archie&quot; was not a reference to the [[Archie Comics|Archie comic book]] series, &quot;[[Veronica Lodge|Veronica]]&quot; and &quot;[[Jughead Jones|Jughead]]&quot; are characters in the series, thus referencing their predecessor.

In the summer of 1993, no search engine existed for the web, though numerous specialized catalogues were maintained by hand. [[Oscar Nierstrasz]] at the [[University of Geneva]] wrote a series of [[Perl]] scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for [[W3Catalog]], the web's first primitive search engine, released on September 2, 1993.&lt;ref name=&quot;Announcement html&quot;&gt;{{cite web |url= http://groups.google.com/group/comp.infosystems.www/browse_thread/thread/2176526a36dc8bd3/2718fd17812937ac?hl=en&amp;lnk=gst&amp;q=Oscar+Nierstrasz#2718fd17812937ac|title=Searchable Catalog of WWW Resources (experimental)|author=[[Oscar Nierstrasz]]|date=2 September 1993}}&lt;/ref&gt;

In June 1993, Matthew Gray, then at [[Massachusetts Institute of Technology|MIT]], produced what was probably the first [[web robot]], the [[Perl]]-based [[World Wide Web Wanderer]], and used it to generate an index called 'Wandex'.  The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995.  The web's second search engine [[Aliweb]] appeared in November 1993.  Aliweb did not use a [[web robot]], but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format.

[[JumpStation]] (created in December 1993&lt;ref&gt;{{cite web|url=http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archiveurl=//web.archive.org/web/20010620073530/http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archivedate=2001-06-20 |title=Archive of NCSA what's new in December 1993 page |publisher=Web.archive.org |date=2001-06-20 |accessdate=2012-05-14}}&lt;/ref&gt; by [[Jonathon Fletcher]]) used a [[web crawler|web robot]] to find web pages and to build its index, and used a [[web form]] as the interface to its query program.  It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below.  Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered.

One of the first &quot;all text&quot; crawler-based search engines was [[WebCrawler]], which came out in 1994.  Unlike its predecessors, it allowed users to search for any word in any webpage, which has become the standard for all major search engines since. It was also the first one widely known by the public.  Also in 1994, [[Lycos]] (which started at [[Carnegie Mellon University]]) was launched and became a major commercial endeavor.

Soon after, many search engines appeared and vied for popularity. These included [[Magellan (search engine)|Magellan]], [[Excite]], [[Infoseek]], [[Inktomi (company)|Inktomi]], [[Northern Light Group|Northern Light]], and [[AltaVista]]. [[Yahoo!]] was among the most popular ways for people to find web pages of interest, but its search function operated on its [[web directory]], rather than its full-text copies of web pages. Information seekers could also browse the directory instead of doing a keyword-based search.

Google adopted the idea of selling search terms in 1998, from a small search engine company named [[goto.com]]. This move had a significant effect on the SE business, which went from struggling to one of the most profitable businesses in the internet.&lt;ref&gt;http://www.udacity.com/view#Course/cs101/CourseRev/apr2012/Unit/616074/Nugget/671097&lt;/ref&gt;

In 1996, [[Netscape]] was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page.  The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.&lt;ref&gt;{{cite web|title=Yahoo! And Netscape Ink International Distribution Deal|url=http://files.shareholder.com/downloads/YHOO/701084386x0x27155/9a3b5ed8-9e84-4cba-a1e5-77a3dc606566/YHOO_News_1997_7_8_General.pdf|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal |date=1 April 1996|title=Browser Deals Push Netscape Stock Up 7.8% |publisher=Los Angeles Times |url=http://articles.latimes.com/1996-04-01/business/fi-53780_1_netscape-home |postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.&lt;ref&gt;{{cite journal |last=Gandal |first=Neil |authorlink= |year=2001 |title=The dynamics of competition in the internet search engine market |journal=International Journal of Industrial Organization |volume=19 |issue=7 |pages=1103–1117 |doi=10.1016/S0167-7187(01)00065-0  |url= |accessdate=|quote= }}&lt;/ref&gt; Several companies entered the market spectacularly, receiving record gains during their [[initial public offering]]s. Some have taken down their public search engine, and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the [[dot-com bubble]], a speculation-driven market boom that peaked in 1999 and ended in 2001.

Around 2000, [[Google Search|Google's search engine]] rose to prominence.&lt;ref&gt;{{cite web|url=http://www.google.com/about/company/history/ |title=Our History in depth |publisher=W3.org |accessdate=2012-10-31}}&lt;/ref&gt;  The company achieved better results for many searches with an innovation called [[PageRank]], as was explained in the paper ''Anatomy of a Search Engine'' written by [[Sergey Brin]] and [[Larry Page]], the later founders of Google.&lt;ref&gt;{{cite web|url=http://ilpubs.stanford.edu:8090/361/1/1998-8.pdf|title=The Anatomy of a Large-Scale Hypertextual Web Search Engine|last1=Brin|first1=Sergey|last2=Page|first2=Larry}}&lt;/ref&gt; This [[iterative algorithm]] ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a [[web portal]]. In fact, Google search engine became so popular that spoof engines emerged such as [[Mystery Seeker]].

By 2000, [[Yahoo!]] was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and [[Overture]] (which owned [[AlltheWeb]] and AltaVista) in 2003.  Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions.

[[Microsoft]] first launched MSN Search in the fall of 1998 using search results from Inktomi.  In early 1999 the site began to display listings from [[Looksmart]], blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista were instead.  In 2004, [[Microsoft]] began a transition to its own search technology, powered by its own [[web crawler]] (called [[msnbot]]).

Microsoft's rebranded search engine, [[Bing]], was launched on June 1, 2009.  On July 29, 2009, Yahoo! and Microsoft finalized a deal in which [[Yahoo! Search]] would be powered by Microsoft Bing technology.

== How web search engines work ==
{{Original research|section|date=October 2013
}}
{{Refimprove|date=July 2013}}
A search engine operates in the following order:
# [[Web crawling]]
# [[Index (search engine)|Indexing]]
# [[Web search query|Searching]]&lt;ref name=Jawadekar2011&gt;{{citation |year=2011 |author=Jawadekar, Waman S |title=Knowledge Management: Text &amp; Cases |url=http://books.google.com/books?id=XmGx4J9daUMC&amp;printsec=frontcover&amp;dq=knowledge+management:+text&amp;hl=en&amp;sa=X&amp;ei=ou6uUP-cNqWTiAe2oICoAw&amp;sqi=2&amp;ved=0CDIQ6AEwAA |chapter=8. Knowledge Management: Tools and Technology |chapter-url=http://books.google.com/books?id=XmGx4J9daUMC&amp;pg=PA278&amp;dq=%22search+engine+operates%22&amp;hl=en&amp;sa=X&amp;ei=a-muUJ6UC4aeiAfI24GYAw&amp;sqi=2&amp;ved=0CDgQ6AEwBA |page=278 |place=New Delhi |publisher=Tata McGraw-Hill Education Private Ltd |isbn=978-0-07-07-0086-4 |accessdate=November 23, 2012 }}&lt;/ref&gt;

Web search engines work by storing information about many web pages, which they retrieve from the [[HTML]] markup of the pages. These pages are retrieved by a [[Web crawler]] (sometimes also known as a spider) — an automated Web crawler which follows every link on the site. The site owner can exclude specific pages by using [[robots.txt]].

The search engine then analyzes the contents of each page to determine how it should be [[Search engine indexing|indexed]] (for example, words can be extracted from the titles, page content, headings, or special fields called [[meta tags]]). Data about web pages are stored in an index database for use in later queries. A query from a user can be a single word. The index helps find information relating to the query as quickly as possible.&lt;ref name=Jawadekar2011/&gt; Some search engines, such as [[Google]], store all or part of the source page (referred to as a [[web cache|cache]]) as well as information about the web pages, whereas others, such as [[AltaVista]], store every word of every page they find.{{Citation needed|date=November 2012}} This cached page always holds the actual search text since it is the one that was actually indexed, so it can be very useful when the content of the current page has been updated and the search terms are no longer in it.&lt;ref name=Jawadekar2011/&gt; This problem might be considered a mild form of [[linkrot]], and Google's handling of it increases [[usability]] by satisfying [[user expectations]] that the search terms will be on the returned webpage. This satisfies the [[principle of least astonishment]], since the user normally expects that the search terms will be on the returned pages. Increased search relevance makes these cached pages very useful as they may contain data that may no longer be available elsewhere.{{Citation needed|date=November 2012}}
[[File:WebCrawlerArchitecture.svg|thumb|High-level architecture of a standard Web crawler]]
When a user enters a [[web search query|query]] into a search engine (typically by using [[Keyword (Internet search)|keywords]]), the engine examines its [[inverted index|index]] and provides a listing of best-matching web pages according to its criteria, usually with a short summary containing the document's title and sometimes parts of the text. The index is built from the information stored with the data and the method by which the information is indexed.&lt;ref name=Jawadekar2011/&gt; From 2007 the Google.com search engine has allowed one to search by date by clicking &quot;Show search tools&quot; in the leftmost column of the initial search results page, and then selecting the desired date range.{{Citation needed|date=November 2012}} Most search engines support the use of the [[boolean operators]] AND, OR and NOT to further specify the [[web search query|search query]]. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered.  Some search engines provide an advanced feature called [[Proximity search (text)|proximity search]], which allows users to define the distance between keywords.&lt;ref name=Jawadekar2011/&gt;  There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for. As well, natural language queries allow the user to type a question in the same form one would ask it to a human. A site like this would be ask.com.{{Citation needed|date=November 2012}}

The usefulness of a search engine depends on the [[relevance (information retrieval)|relevance]] of the '''result set''' it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to [[rank order|rank]] the results to provide the &quot;best&quot; results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.&lt;ref name=Jawadekar2011/&gt; The methods also change over time as Internet usage changes and new techniques evolve.  There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an &quot;[[inverted index]]&quot; by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work.

Most Web search engines are commercial ventures supported by [[advertising]] revenue and thus some of them allow advertisers to [[paid inclusion|have their listings ranked higher]] in search results for a fee. Search engines that do not accept money for their search results make money by running [[contextual advertising|search related ads]] alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.&lt;ref&gt;{{cite web|title=FAQ|url=http://www.rankstar.de/hilfe.html|publisher=RankStar|accessdate=19 June 2013}}&lt;/ref&gt;

== Market share ==

[[Google Search|Google]] is the world's most popular search engine, with a marketshare of 66.44 percent as of December, 2014.&lt;ref name=&quot;NMS&quot;&gt;{{cite web|url=http://marketshare.hitslink.com/search-engine-market-share.aspx?qprid=4&amp;qpcustomd=0&amp;qpcustom=|title=Desktop Search Engine Market Share|publisher=NetMarketShare|accessdate=2014-06-04}}&lt;/ref&gt; [[Baidu]] comes in at second place.&lt;ref name=&quot;NMS&quot; /&gt;

The world's most popular search engines are:&lt;ref&gt;{{cite web|title=FAQ|url=https://www.netmarketshare.com/search-engine-market-share.aspx?qprid=4&amp;qpcustomd=0|publisher=NetMarketShare|accessdate=23 November 2014}}&lt;/ref&gt;

{| class=&quot;wikitable sortable&quot;
! Search engine !! colspan=&quot;2&quot; |Market share in December 2014
|-
| [[Google Search|Google]] || style=&quot;text-align:right;&quot;|{{bartable|66.44|%|2}}
|-
| [[Baidu]]  || style=&quot;text-align:right;&quot;|{{bartable| 11.15|%|2}}
|-
| [[Bing]]   || style=&quot;text-align:right;&quot;|{{bartable| 10.29|%|2}}
|-
| [[Yahoo!]]  || style=&quot;text-align:right;&quot;|{{bartable| 9.31|%|2}}
|-
| [[AOL]]    || style=&quot;text-align:right;&quot;|{{bartable| 0.53|%|2}}
|-
| [[Ask.com|Ask]]    || style=&quot;text-align:right;&quot;|{{bartable| 0.21|%|2}}
|-
| [[Lycos]]   || style=&quot;text-align:right;&quot;|{{bartable| 0.01|%|2}}
|}

{| class=&quot;wikitable sortable&quot;
! Search engine !! colspan=&quot;2&quot; |Market share in October 2014
|-
| [[Google Search|Google]] || style=&quot;text-align:right;&quot;|{{bartable|58.01|%|2}}
|-
| [[Baidu]]  || style=&quot;text-align:right;&quot;|{{bartable| 29.06|%|2}}
|-
| [[Bing]]   || style=&quot;text-align:right;&quot;|{{bartable| 8.01|%|2}}
|-
| [[Yahoo!]]  || style=&quot;text-align:right;&quot;|{{bartable| 4.01|%|2}}
|-
| [[AOL]]    || style=&quot;text-align:right;&quot;|{{bartable| 0.21|%|2}}
|-
| [[Ask.com|Ask]]    || style=&quot;text-align:right;&quot;|{{bartable| 0.10|%|2}}
|-
| [[Excite]]   || style=&quot;text-align:right;&quot;|{{bartable| 0.00|%|2}}
|}

{| class=&quot;wikitable sortable&quot;
! Search engine !! colspan=&quot;2&quot; |Market share in July 2014&lt;ref name=&quot;NMS&quot; /&gt;
|-
| [[Google Search|Google]] || style=&quot;text-align:right;&quot;|{{bartable|68.69|%|2}}
|-
| [[Baidu]]  || style=&quot;text-align:right;&quot;|{{bartable| 17.17|%|2}}
|-
| [[Yahoo!]]  || style=&quot;text-align:right;&quot;|{{bartable| 6.74|%|2}}
|-
| [[Bing]]   || style=&quot;text-align:right;&quot;|{{bartable| 6.22|%|2}}
|-
| [[Excite]]   || style=&quot;text-align:right;&quot;|{{bartable| 0.22|%|2}}
|-
| [[Ask.com|Ask]]    || style=&quot;text-align:right;&quot;|{{bartable| 0.13|%|2}}
|-
| [[AOL]]    || style=&quot;text-align:right;&quot;|{{bartable| 0.13|%|2}}
|}

=== East Asia and Russia ===

East Asian countries and Russia constitute a few places where Google is not the most popular search engine.

[[Yandex]] commands a marketshare of 61.9 per cent in Russia, compared to Google's 28.3 percent.&lt;ref&gt;{{cite web|url=http://www.liveinternet.ru/stat/ru/searches.html?slice=ru;period=week|title=Live Internet - Site Statistics|publisher=Live Internet|accessdate=2014-06-04}}&lt;/ref&gt; In China, Baidu is the most popular search engine.&lt;ref&gt;{{cite news|url=http://www.theguardian.com/world/2014/jun/03/chinese-technology-companies-huawei-dominate-world|title=The Chinese technology companies poised to dominate the world|publisher=The Guardian|author=Arthur, Charles|date=2014-06-03|accessdate=2014-06-04}}&lt;/ref&gt;  South Korea's homegrown search portal, [[Naver]], is used for 70 per cent online searches in the country.&lt;ref&gt;{{cite web|url=http://blogs.wsj.com/korearealtime/2014/05/21/how-naver-hurts-companies-productivity/|title=How Naver Hurts Companies’ Productivity|publisher=The Wall Street Journal|date=2014-05-21|accessdate=2014-06-04}}&lt;/ref&gt; [[Yahoo! Japan]] and [[Yahoo! Search|Yahoo! Taiwan]] are the most popular avenues for internet search in Japan and Taiwan, respectively.&lt;ref&gt;{{cite web|url=http://geography.oii.ox.ac.uk/?page=age-of-internet-empires|title=Age of Internet Empires|publisher=Oxford Internet Institute|accessdate=2014-06-04}}&lt;/ref&gt;

== Search engine bias ==
Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide.&lt;ref&gt;Segev, El (2010). Google and the Digital Divide: The Biases of Online Knowledge, Oxford: Chandos Publishing.&lt;/ref&gt;&lt;ref name=vaughan-thelwall&gt;{{cite journal|last=Vaughan|first=Liwen|author2=Mike Thelwall |title=Search engine coverage bias: evidence and possible causes|journal=Information Processing &amp; Management|year=2004|volume=40|issue=4|pages=693–707|doi=10.1016/S0306-4573(03)00063-3}}&lt;/ref&gt; These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its [[organic search]] results), and political processes (e.g., the removal of search results to comply with local laws).&lt;ref&gt;Berkman Center for Internet &amp; Society (2002), [http://cyber.law.harvard.edu/filtering/china/google-replacements/ “Replacement of Google with Alternative Search Systems in China: Documentation and Screen Shots”], Harvard Law School.&lt;/ref&gt; For example, Google will not surface certain Neo-Nazi websites in France and Germany, where Holocaust denial is illegal.

Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more &quot;popular&quot; results.&lt;ref&gt;{{cite journal|last=Introna|first=Lucas|author2=[[Helen Nissenbaum]] |title=Shaping the Web: Why the Politics of Search Engines Matters|journal=The Information Society: An International Journal|year=2000|volume=16|issue=3|doi=10.1080/01972240050133634}}&lt;/ref&gt; Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.&lt;ref name=vaughan-thelwall /&gt;

[[Google Bombing]] is one example of an attempt to manipulate search results for political, social or commercial reasons.

== Customized results and filter bubbles ==

Many search engines such as Google and Bing provide customized results based on the user's activity history. This leads to an effect that has been called a [[filter bubble]]. The term describes a phenomenon in which websites use [[algorithm]]s to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint, effectively isolating the user in a bubble that tends to exclude contrary information. Prime examples are Google's personalized search results and [[Facebook]]'s personalized news stream. According to [[Eli Pariser]], who coined the term, users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Pariser related an example in which one user searched Google for &quot;BP&quot; and got investment news about [[British Petroleum]] while another searcher got information about the [[Deepwater Horizon oil spill]] and that the two search results pages were &quot;strikingly different&quot;.&lt;ref name=twsT43&gt;{{cite news
 |first1= Lynn | last1= Parramore
 |title= The Filter Bubble
 |work= The Atlantic
 |quote= Since Dec. 4, 2009, Google has been personalized for everyone. So when I had two friends this spring Google &quot;BP,&quot; one of them got a set of links that was about investment opportunities in BP. The other one got information about the oil spill....
 |date=  10 October 2010
 |url= http://www.theatlantic.com/daily-dish/archive/2010/10/the-filter-bubble/181427/
 |accessdate= 2011-04-20
}}&lt;/ref&gt;&lt;ref name=twsO11&gt;{{cite news
 |first= Jacob | last= Weisberg
 |title= Bubble Trouble: Is Web personalization turning us into solipsistic twits?
 |work= Slate
 |date= 10 June 2011
 |url= http://www.slate.com/id/2296633/
 |accessdate= 2011-08-15
}}&lt;/ref&gt;&lt;ref name=twsO14&gt;{{cite news
 |first= Doug | last= Gross
 |title= What the Internet is hiding from you
 |publisher= ''CNN''
 |quote= I had friends Google BP when the oil spill was happening. These are two women who were quite similar in a lot of ways. One got a lot of results about the environmental consequences of what was happening and the spill. The other one just got investment information and nothing about the spill at all.
 |date= May 19, 2011
 |url= http://edition.cnn.com/2011/TECH/web/05/19/online.privacy.pariser/
 |accessdate= 2011-08-15
}}&lt;/ref&gt; The bubble effect may have negative implications for civic discourse, according to Pariser.&lt;ref&gt;{{cite journal| last1= Zhang | first1= Yuan Cao | first2= Diarmuid Ó |last2= Séaghdha | first3= Daniele | last3= Quercia | first4 =Tamas | last4 = Jambor |title=Auralist: Introducing Serendipity into Music Recommendation|journal=ACM WSDM |date=February 2012|url=http://www-typo3.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf}}&lt;/ref&gt;

Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or &quot;bubbling&quot; users.

== Faith-based search engines ==

The global growth of the Internet and popularity of electronic contents in the [[Arab]] and [[Muslim]] World during the last decade has encouraged faith adherents, notably in [[Middle East|the Middle East]] and [[Indian subcontinent|Asian sub-continent]], to &quot;dream&quot; of their own faith-based i.e. &quot;[[Islamic]]&quot; search engines or filtered search portals filters that would enable users to avoid accessing forbidden websites such as pornography and would only allow them to access sites that are compatible to the Islamic faith. Shortly before the Muslim only month of [[Ramadan]], [[Halalgoogling]] which collects results from other search engines like [[Google]] and [[Bing]] was introduced to the world July 2013 to presents the [[halal]] results to its users,&lt;ref&gt;{{cite web|url=http://news.msn.com/science-technology/new-islam-approved-search-engine-for-muslims |title=New Islam-approved search engine for Muslims |publisher=News.msn.com |date= |accessdate=2013-07-11}}&lt;/ref&gt; nearly two years after I’mHalal, another search engine initially (launched on September 2011) to serve Middle East Internet had to close its search service due to what its owner blamed on lack of funding.&lt;ref&gt;[http://blog.imhalal.com/ I’mHalal - Islamic compliant search project launched September 2009 and shut down late 2011]&lt;/ref&gt;

While lack of investment and slow pace in technologies in the Muslim World as the main consumers or targeted end users has hindered progress and thwarted success of serious Islamic search engine, the spectacular failure of heavily invested Muslim lifestyle web projects like [[Muxlim]], which received millions of dollars from investors like Rite Internet Ventures, has - according to I’mHalal shutdown notice - made almost laughable the idea that the next [[Facebook]] or [[Google]] can only come from [[Middle East|the Middle East]] if you support your bright youth.&lt;ref&gt;[http://imhalal.com/ I'mHalal Blog]&lt;/ref&gt; Yet Muslim internet experts have been determining for years what is or is not allowed according to [[Shariah|the &quot;Law of Islam&quot;]] and have been categorizing websites and such into being either &quot;[[halal]]&quot; or &quot;[[haram]]&quot;. All the existing and past Islamic search engines are merely custom search indexed or monetized by web major search giants like [[Google]], [[Yahoo]] and [[Bing]] with only certain filtering systems applied to ensure that their users can't access Haram sites, which include such sites as nudity, gay, gambling or anything that is deemed to be anti-Islamic.&lt;ref&gt;[http://blog.imhalal.com/ I'mHalal Blog]&lt;/ref&gt;

Another religiously-oriented search engine is Jewogle, which is the Jewish version of Google and yet another is SeekFind.org, which is a Christian website that includes filters preventing users from seeing anything on the internet that attacks or degrades their faith.&lt;ref&gt;[http://allchristiannews.com/halalgoogling-muslims-get-their-own-sin-free-google-should-christians-have-christian-google/ AllChristianNews]&lt;/ref&gt;

== See also ==
*[[Most popular Internet search engines]]
* [[Comparison of web search engines]]
* [[List of search engines]]
* Answer engine ([[question answering]]) &lt;!-- examples necessary here until article comprehensible to normal reader--&gt;
** [[Quora]]
** [[True Knowledge]]
** [[Wolfram Alpha]]
* [[Google effect]]
* [[Internet Search Engines and Libraries]]
* [[Semantic Web]]
* [[Spell checker]]
* [[Web development tools]]

== References ==
{{Reflist|33em}}

== Further reading ==
* For a more detailed history of early search engines, see [http://searchenginewatch.com/showPage.html?page=3071951 Search Engine Birthdays] (from [[Search Engine Watch]]), Chris Sherman, September 2003.
* {{cite journal | quotes =| author =Steve Lawrence; C. Lee Giles | year =1999| title =Accessibility of information on the web | journal =[[Nature (journal)|Nature]] | volume =400 | issue =6740| doi =10.1038/21987 | pmid =10428673 | pages =107–9 }}
* Bing Liu (2007), ''[http://www.cs.uic.edu/~liub/WebMiningBook.html Web Data Mining: Exploring Hyperlinks, Contents and Usage Data].'' Springer,ISBN 3-540-37881-2
* Bar-Ilan, J. (2004). The use of Web search engines in information science research. ARIST, 38, 231-288.
* {{cite book | first =Mark | last =Levene | year =2005 | title =An Introduction to Search Engines and Web Navigation | publisher =Pearson | location =| isbn =}}
* {{cite book | first =Randolph | last =Hock | year =2007 | title =The Extreme Searcher's Handbook}}ISBN 978-0-910965-76-7
* {{cite journal | quotes =| author =Javed Mostafa |date= February 2005 | title =Seeking Better Web Searches | journal =[[Scientific American]] | volume =| issue =| pages =| publisher =| pmid =| doi =| bibcode =| url =http://www.sciam.com/article.cfm?articleID=0006304A-37F4-11E8-B7F483414B7F0000 | language =}}&lt;sup class=&quot;noprint Inline-Template&quot;&gt;&lt;span title=&quot;&amp;nbsp;since September 2010&quot; style=&quot;white-space: nowrap;&quot;&gt;&amp;#91;''&amp;#93;&lt;/span&gt;&lt;/sup&gt;
* {{cite journal |last=Ross |first=Nancy |authorlink=|author2=Wolfram, Dietmar  |year=2000 |title=End user searching on the Internet: An analysis of term pair topics submitted to the Excite search engine |journal=Journal of the American Society for Information Science |volume=51 |issue=10 |pages=949–958 |doi=10.1002/1097-4571(2000)51:10&lt;949::AID-ASI70&gt;3.0.CO;2-5|url=|accessdate=|quote=}}
* {{cite journal |last=Xie |first=M. |authorlink=|year=1998 |title=Quality dimensions of Internet search engines |journal=Journal of Information Science |volume=24 |issue=5 |pages=365–372 |doi=10.1177/016555159802400509 |url=|accessdate=|quote=|display-authors=1 |last2=Wang |first2=H. |last3=Goh |first3=T. N. }}
*{{cite book|title=Information Retrieval: Implementing and Evaluating Search Engines|url= http://www.ir.uwaterloo.ca/book/ | year=2010|publisher=MIT Press|author8=Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack}}

== External links ==
{{commons category|Internet search engines}}
* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}

{{Internet search}}

{{DEFAULTSORT:Web Search Engine}}
[[Category:Internet search engines| ]]
[[Category:History of the Internet]]
[[Category:Information retrieval]]
[[Category:Internet terminology]]</text>
      <sha1>isajf0rr3ifpqhndwoqg6090f3m8r65</sha1>
    </revision>
  </page>
  <page>
    <title>Vector space model</title>
    <ns>0</ns>
    <id>20948989</id>
    <revision>
      <id>645392500</id>
      <parentid>631933990</parentid>
      <timestamp>2015-02-03T01:59:47Z</timestamp>
      <contributor>
        <username>Gmelli</username>
        <id>205383</id>
      </contributor>
      <comment>/* See also */ connect to the semantic-based geometric approach</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9003">'''Vector space model''' or '''term vector model''' is an algebraic model for representing text documents (and any objects, in general) as [[vector space|vectors]] of identifiers, such as, for example, index terms. It is used in [[information filtering]], [[information retrieval]], [[index (search engine)|index]]ing and relevancy rankings.  Its first use was in the [[SMART Information Retrieval System]].

==Definitions==

Documents and queries are represented as vectors.

:&lt;math&gt;d_j = ( w_{1,j} ,w_{2,j} , \dotsc ,w_{t,j} )&lt;/math&gt;
:&lt;math&gt;q = ( w_{1,q} ,w_{2,q} , \dotsc ,w_{n,q} )&lt;/math&gt;

Each [[Dimension (vector space)|dimension]] corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is [[tf-idf]] weighting (see the example below).

The definition of ''term'' depends on the application. Typically terms are single words, [[keyword (linguistics)|keyword]]s, or longer phrases. If words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the [[text corpus|corpus]]).

Vector operations can be used to compare documents with queries.

==Applications==

[[Image:vector space model.jpg|right|250px]]

[[Relevance (information retrieval)|Relevance]] [[ranking]]s of documents in a keyword search can be calculated, using the assumptions of [[semantic similarity|document similarities]] theory, by comparing the deviation of angles between each document vector and the original query vector where the query is represented as the same kind of vector as the documents.

In practice, it is easier to calculate the [[cosine]] of the angle between the vectors, instead of the angle itself:

:&lt;math&gt;
\cos{\theta} = \frac{\mathbf{d_2} \cdot \mathbf{q}}{\left\| \mathbf{d_2} \right\| \left \| \mathbf{q} \right\|}
&lt;/math&gt;

Where &lt;math&gt;\mathbf{d_2} \cdot \mathbf{q}&lt;/math&gt; is the intersection (i.e. the [[dot product]]) of the document (d&lt;sub&gt;2&lt;/sub&gt; in the figure to the right) and the query (q in the figure) vectors, &lt;math&gt;\left\| \mathbf{d_2} \right\|&lt;/math&gt; is the norm of vector d&lt;sub&gt;2&lt;/sub&gt;, and &lt;math&gt;\left\| \mathbf{q} \right\|&lt;/math&gt; is the norm of vector q. The [[Norm (mathematics)|norm]] of a vector is calculated as such:

:&lt;math&gt;
\left\| \mathbf{q} \right\| = \sqrt{\sum_{i=1}^n q_i^2}
&lt;/math&gt;

As all vectors under consideration by this model are elementwise nonnegative, a cosine value of zero means that the query and document vector are [[orthogonal]] and have no match (i.e. the query term does not exist in the document being considered). See [[cosine similarity]] for further information.

==Example: tf-idf weights==

In the classic vector space model proposed by [[Gerard Salton|Salton]], Wong and Yang &lt;ref&gt;[http://doi.acm.org/10.1145/361219.361220 G. Salton , A. Wong , C. S. Yang, A vector space model for automatic indexing], Communications of the ACM, v.18 n.11, p.613-620, Nov. 1975&lt;/ref&gt; the term-specific weights in the document vectors are products of local and global parameters. The model is known as [[tf-idf|term frequency-inverse document frequency]] model. The weight vector for document ''d'' is &lt;math&gt;\mathbf{v}_d = [w_{1,d}, w_{2,d}, \ldots, w_{N,d}]^T&lt;/math&gt;, where

:&lt;math&gt;
w_{t,d} = \mathrm{tf}_{t,d} \cdot \log{\frac{|D|}{|\{d' \in D \, | \, t \in d'\}|}}
&lt;/math&gt;

and
* &lt;math&gt;\mathrm{tf}_{t,d}&lt;/math&gt; is term frequency of term ''t'' in document ''d'' (a local parameter)
* &lt;math&gt;\log{\frac{|D|}{|\{d' \in D \, | \, t \in d'\}|}}&lt;/math&gt; is inverse document frequency (a global parameter). &lt;math&gt;|D|&lt;/math&gt; is the total number of documents in the document set; &lt;math&gt;|\{d' \in D \, | \, t \in d'\}|&lt;/math&gt; is the number of documents containing the term ''t''.

Using the cosine the similarity between document ''d&lt;sub&gt;j&lt;/sub&gt;'' and query ''q'' can be calculated as:

:&lt;math&gt;\mathrm{sim}(d_j,q) = \frac{\mathbf{d_j} \cdot \mathbf{q}}{\left\| \mathbf{d_j} \right\| \left \| \mathbf{q} \right\|} = \frac{\sum _{i=1}^N w_{i,j}w_{i,q}}{\sqrt{\sum _{i=1}^N w_{i,j}^2}\sqrt{\sum _{i=1}^N w_{i,q}^2}}&lt;/math&gt;

==Advantages==

The vector space model has the following advantages over the [[Standard Boolean model]]:

#Simple model based on linear algebra
#Term weights not binary
#Allows computing a continuous degree of similarity between queries and documents
#Allows ranking documents according to their possible relevance
#Allows partial matching

==Limitations==

The vector space model has the following limitations:

#Long documents are poorly represented because they have poor similarity values (a small [[scalar product]] and a [[curse of dimensionality|large dimensionality]])
#Search keywords must precisely match document terms; word [[substring]]s might result in a &quot;[[false positive]] match&quot;
#Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a &quot;[[false negative]] match&quot;.
#The order in which the terms appear in the document is lost in the vector space representation.
#Theoretically assumes terms are statistically independent. 
#Weighting is intuitive but not very formal. 

Many of these difficulties can, however, be overcome by the integration of various tools, including mathematical techniques such as [[singular value decomposition]] and [[lexical database]]s such as [[WordNet]].

==Models based on and extending the vector space model==

Models based on and extending the vector space model include:
* [[Generalized vector space model]]
* [[Latent semantic analysis]]
* [[Term Discrimination]]
* [[Rocchio Classification]]
* [[random_indexing|Random Indexing]]

==Software that implements the vector space model==

The following software packages may be of interest to those wishing to experiment with vector models and implement search services based upon them.

===Free open source software===

* [[Apache Lucene]]. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.
* [http://semanticvectors.googlecode.com SemanticVectors]. Semantic Vector indexes, created by applying a Random Projection algorithm (similar to [[Latent semantic analysis]]) to term-document matrices created using Apache Lucene.
* [[Gensim]] is a Python+[[NumPy]] framework for Vector Space modelling. It contains incremental (memory-efficient) algorithms for [[Tf–idf]], [[Latent Semantic Indexing]], [[Locality_sensitive_hashing#Random_projection|Random Projections]] and [[Latent Dirichlet Allocation]].
* [[Weka (machine learning)|Weka]]. Weka is popular data mining package for Java including WordVectors and Bag Of Words models.
* [http://codingplayground.blogspot.com/2010/03/compressed-vector-space.html Compressed vector space in C++] by Antonio Gulli
* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. Most of TMG is written in MATLAB and parts in Perl. It contains implementations of LSI, clustered LSI, NMF and other methods.
* [http://senseclusters.sourceforge.net SenseClusters], an open source package, written in Perl, that supports context and word clustering using Latent Semantic Analysis and word co-occurrence matrices.
* [https://github.com/fozziethebeat/S-Space/wiki S-Space Package], a collection of algorithms for exploring and working with [[statistical semantics]].
* [http://www.cs.uni.edu/~okane/source/ISR/ Vector Space Model Software Workbench] Collection of 50 source code programs for education.

==Further reading==

* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), &quot;[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing],&quot; ''Communications of the ACM'', vol. 18, nr. 11, pages 613–620. ''(Article in which a vector space model was presented)''
* David Dubin (2004), [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] ''(Explains the history of the Vector Space Model and the non-existence of a frequently cited publication)''
* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node5.html Description of the vector space model]
* [http://www.miislita.com/term-vector/term-vector-3.html Description of the classic vector space model by Dr E. Garcia]
* [http://nlp.stanford.edu/IR-book/html/htmledition/vector-space-classification-1.html Relationship of vector space search to the &quot;k-Nearest Neighbor&quot; search]

==See also==
*[[Bag-of-words model]]
*[[Nearest neighbor search]]
*[[Compound term processing]]
*[[Inverted index]]
*[[w-shingling]]
*[[Eigenvalues and eigenvectors]]
*[[Conceptual Spaces]].

==References==
&lt;references/&gt;

[[Category:Vector space model|*]]</text>
      <sha1>dmkzfawyrto21tv5shfujcxcgilsjrd</sha1>
    </revision>
  </page>
  <page>
    <title>Explicit semantic analysis</title>
    <ns>0</ns>
    <id>36472495</id>
    <revision>
      <id>641736469</id>
      <parentid>641736307</parentid>
      <timestamp>2015-01-09T14:48:56Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>ijcai url</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4990">In [[natural language processing]] and [[information retrieval]], '''explicit semantic analysis''' ('''ESA''') is a [[Vector space model|vectorial]] representation of text (individual words or entire documents) that uses a document corpus as a [[knowledge base]]. Specifically, in ESA, a word is represented as a column vector in the [[tf*idf|tf–idf]] matrix of the text corpus and a document (string of words) is represented as the [[centroid]] of the vectors representing its words. Typically, the text corpus is [[Wikipedia]], though other corpora including the [[Open Directory Project]] have been used.&lt;ref name=&quot;infosys&quot;&gt;{{cite journal |authors=Ofer Egozi, Shaul Markovitch and Evgeniy Gabrilovich |year=2011 |title=Concept-Based Information Retrieval using Explicit Semantic Analysis |url=http://www.cs.technion.ac.il/~gabr/publications/papers/Egozi2011CBI.pdf|format=pdf|accessdate=January 3, 2015|journal=ACM Transactions on Information Systems |volume=29 |issue=2}}&lt;/ref&gt;

ESA was designed by [[Evgeniy Gabrilovich]] and Shaul Markovitch as a means of improving [[document classification|text categorization]]&lt;ref&gt;{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Overcoming the brittleness bottleneck using Wikipedia: enhancing text categorization with encyclopedic knowledge |conference=Proc. 21st National Conference on Artificial Intelligence (AAAI) |pages=1301–1306 |year=2006 |url=http://www.aaai.org/Papers/AAAI/2006/AAAI06-204.pdf}}&lt;/ref&gt;
and has been used by this pair of researchers to compute what they refer to as &quot;[[Semantics|semantic]] relatedness&quot; by means of [[cosine similarity]] between the aforementioned vectors, collectively interpreted as a space of &quot;concepts explicitly defined and described by humans&quot;, where Wikipedia articles (or ODP entries, or otherwise titles of documents in the knowledge base corpus) are equated with concepts.
The name &quot;explicit semantic analysis&quot; contrasts with [[latent semantic analysis]] (LSA), because the use of a knowledge base makes it possible to assign human-readable labels to the concepts that make up the vector space.&lt;ref&gt;{{cite conference |first1=Evgeniy |last1=Gabrilovich |first2=Shaul |last2=Markovitch |title=Computing semantic relatedness using Wikipedia-based Explicit Semantic Analysis |conference=Proc. 20th Int'l Joint Conf. on Artificial Intelligence (IJCAI) |pages=1606–1611 |year=2007 |url=http://www.cs.technion.ac.il/~gabr/papers/ijcai-2007-sim.pdf}}&lt;/ref&gt;&lt;ref name=&quot;infosys&quot;/&gt;

ESA, as originally posited by Gabrilovich and Markovitch, operates under the assumption that the knowledge base contains topically [[Orthogonality|orthogonal]] concepts. However, it was later shown by Anderka and Stein that ESA also improves the performance of [[information retrieval]] systems when it is based not on Wikipedia, but on the [[Reuters]] corpus of newswire articles, which does not satisfy the orthogonality property; in their experiments, Anderka and Stein used newswire stories as &quot;concepts&quot;.&lt;ref&gt;Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2009c.pdf The ESA retrieval model revisited]. Proceedings of the 32nd International ACM Conference on Research and Development in Information Retrieval (SIGIR), pp. 670-671, 2009.&lt;/ref&gt;
To explain this observation, links have been shown between ESA and the [[generalized vector space model]].&lt;ref&gt;Thomas Gottron, Maik Anderka and Benno Stein. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2011o.pdf Insights into explicit semantic analysis]. Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM), pp. 1961-1964, 2011.&lt;/ref&gt;
Gabrilovich and Markovitch replied to Anderka and Stein by pointing out that their experimental result was achieved using &quot;a single application of ESA (text similarity)&quot; and &quot;just a single, extremely small and homogenous test collection of 50 news documents&quot;.&lt;ref name=&quot;infosys&quot; /&gt;

'''Cross-language explicit semantic analysis''' ('''CL-ESA''') is a multilingual generalization of ESA.&lt;ref&gt;Martin Potthast, Benno Stein, and Maik Anderka. [http://www.uni-weimar.de/medien/webis/publications/papers/stein_2008b.pdf A Wikipedia-based multilingual retrieval model]. Proceedings of the 30th European Conference on IR Research (ECIR), pp. 522-530, 2008.&lt;/ref&gt;
CL-ESA exploits a document-aligned multilingual reference collection (e.g., again, Wikipedia) to represent a document as a language-independent concept vector. The relatedness of two documents in different languages is assessed by the cosine similarity between the corresponding vector representations.

== See also ==
* [[Topic model]]

== External links ==
* [http://www.cs.technion.ac.il/~gabr/resources/code/esa/esa.html Explicit semantic analysis] on Evgeniy Gabrilovich's homepage; has links to implementations

== References ==
{{reflist|2}}

[[Category:Natural language processing]]
[[Category:Vector space model]]</text>
      <sha1>7m99imw0wc9pqc28biqi4sqlart3amx</sha1>
    </revision>
  </page>
  <page>
    <title>Topic-based vector space model</title>
    <ns>0</ns>
    <id>1256148</id>
    <revision>
      <id>503030676</id>
      <parentid>342046843</parentid>
      <timestamp>2012-07-18T22:24:53Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor/>
      <comment>cat:VSM</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2354">The '''Topic-based Vector Space Model (TVSM)'''&lt;ref&gt;{{cite | url=http://www.kuropka.net/files/TVSM.pdf | title=Topic-based Vector Space Model | author=Dominik Kuropka | coauthors=Jorg Becker | year=2003}}&lt;/ref&gt; (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&amp;lng=eng&amp;id=]) extends the [[vector space model]] of [[information retrieval]] by removing the constraint that the term-vectors be orthogonal. The assumption of orthogonal terms is incorrect regarding natural languages which causes problems with synonyms and strong related terms. This facilitates the use of stopword lists, stemming and thesaurus in TVSM.
In contrast to the [[generalized vector space model]] the TVSM does not depend on concurrence-based similarities between terms. 

==Definitions==
The basic premise of TVSM is the existence of a ''d'' dimensional space ''R'' with only positive axis intercepts, i.e. ''R in R&lt;sup&gt;+&lt;/sup&gt;'' and ''d in N&lt;sup&gt;+&lt;/sup&gt;''. Each dimension of ''R'' represents a fundamental topic. A term vector ''t'' has a specific weight for a certain ''R''. To calculate these weights assumptions are made taking into account the document contents. Ideally important terms will have a high weight and stopwords and irrelevants terms to the topic will have a low weight. The TVSM document model is obtained as a sum of term vectors representing terms in the document. The similarity between two documents ''Di'' and ''Dj'' is defined as the scalar product of document vectors.

==Enhanced Topic-based Vector Space Model==
The enhancement of the Enhanced Topic-based Vector Space Model (eTVSM)&lt;ref&gt;{{cite | url= http://kuropka.net/files/HPI_Evaluation_of_eTVSM.pdf | author=Dominik Kuropka | coauthors=Artem Polyvyanyy | title=A Quantitative Evaluation of the Enhanced Topic-Based Vector Space Model | year=2007}}&lt;/ref&gt; (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&amp;lng=eng&amp;id=]) is a proposal on how to derive term vectors from an [[Ontology_(information_science) | Ontology]]. Using a synonym Ontology created from [[WordNet]] Kuropka shows good results for document similarity. If a trivial Ontology is used the results are similar to Vector Space model.

==Implementations==
* [http://sourceforge.net/projects/etvsm/ Implementation of eTVSM in python]

== References ==
{{reflist}}

[[Category:Vector space model]]</text>
      <sha1>8768c766h0b0245hvushk0z1pu94ocp</sha1>
    </revision>
  </page>
  <page>
    <title>Tf–idf</title>
    <ns>0</ns>
    <id>2057290</id>
    <revision>
      <id>646321217</id>
      <parentid>646321157</parentid>
      <timestamp>2015-02-09T08:48:11Z</timestamp>
      <contributor>
        <ip>138.246.2.177</ip>
      </contributor>
      <comment>Use DOIs, and improve templates.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10721">{{Lowercase|title=tf–idf}}
{{More footnotes|date=July 2012}}

'''tf–idf''', short for '''term frequency–inverse document frequency''', is a numerical statistic that is intended to reflect how important a word is to a [[document]] in a collection or [[Text corpus|corpus]].&lt;ref&gt;{{cite doi|10.1017/CBO9781139058452.002}}&lt;/ref&gt;{{rp|8}} It is often used as a weighting factor in [[information retrieval]] and [[text mining]].
The tf-idf value increases [[Proportionality (mathematics)|proportionally]] to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.

Variations of the tf–idf weighting scheme are often used by [[search engine]]s as a central tool in scoring and ranking a document's [[Relevance (information retrieval)|relevance]] given a user [[Information retrieval|query]]. tf–idf can be successfully used for [[stop-words]] filtering in various subject fields including [[automatic summarization|text summarization]] and classification.

One of the simplest [[ranking function]]s is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.

==Motivation==
Suppose we have a set of English text documents and wish to determine which document is most relevant to the query &quot;the brown cow&quot;. A simple way to start out is by eliminating documents that do not contain all three words &quot;the&quot;, &quot;brown&quot;, and &quot;cow&quot;, but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document and sum them all together; the number of times a term occurs in a document is called its ''term frequency''.

However, because the term &quot;the&quot; is so common, this will tend to incorrectly emphasize documents which happen to use the word &quot;the&quot; more frequently, without giving enough weight to the more meaningful terms &quot;brown&quot; and &quot;cow&quot;. The term &quot;the&quot; is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less common words &quot;brown&quot; and &quot;cow&quot;. Hence an ''inverse document frequency'' factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.

==Definition==
tf–idf is the product of two statistics, term frequency and inverse document frequency. Various ways for determining the exact values of both statistics exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest choice is to use the ''raw frequency'' of a term in a document, i.e. the number of times that term ''t'' occurs in document ''d''. If we denote the raw frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is tf(''t'',''d'') = f(''t'',''d''). Other possibilities include&lt;ref&gt;{{cite doi|10.1017/CBO9780511809071.007}}&lt;/ref&gt;{{rp|128}}

* [[boolean data type|Boolean]] &quot;frequencies&quot;: tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise;
* [[logarithm]]ically scaled frequency: tf(''t'',''d'') = 1 + log f(''t'',''d''), or zero if f(''t'', ''d'') is zero;
* augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document:
:&lt;math&gt;\mathrm{tf}(t,d) = 0.5 + \frac{0.5 \times \mathrm{f}(t, d)}{\max\{\mathrm{f}(w, d):w \in d\}}&lt;/math&gt;

The '''inverse document frequency''' is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled fraction of the documents that contain the word, obtained by dividing the total number of [[documents]] by the number of documents containing the term, and then taking the logarithm of that [[quotient]].

:&lt;math&gt; \mathrm{idf}(t, D) =  \log \frac{N}{|\{d \in D: t \in d\}|}&lt;/math&gt;

with

* &lt;math&gt;N&lt;/math&gt;: total number of documents in the corpus
* &lt;math&gt; |\{d \in D: t \in d\}| &lt;/math&gt; : number of documents where the term &lt;math&gt; t &lt;/math&gt; appears (i.e., &lt;math&gt; \mathrm{tf}(t,d) \neq 0&lt;/math&gt;). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to &lt;math&gt;1 + |\{d \in D: t \in d\}|&lt;/math&gt;.

Mathematically the base of the log function does not matter and constitutes a constant multiplicative factor towards the overall result.

Then tf–idf is calculated as

:&lt;math&gt;\mathrm{tfidf}(t,d,D) = \mathrm{tf}(t,d) \times \mathrm{idf}(t, D)&lt;/math&gt;

A high weight in tf–idf is reached by a high term [[frequency (statistics)|frequency]] (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to filter out common terms. Since the ratio inside the idf's log function is always greater than or equal to 1, the value of idf (and tf-idf) is greater than or equal to 0. As a term appears in more documents, the ratio inside the logarithm approaches 1, bringing the idf and tf-idf closer to 0.

==Justification of idf==
Idf was introduced, as &quot;term specificity&quot;, by [[Karen Spärck Jones]] in a 1972 paper. Although it has worked well as a [[heuristic]], its theoretical foundations have been troublesome for at least three decades afterward, with many researchers trying to find [[information theory|information theoretic]] justifications for it.&lt;ref name=&quot;understanding&quot;&gt;{{cite doi|10.1108/00220410410560582}}&lt;/ref&gt;

Spärck Jones's own explanation didn't propose much theory, aside from a connection to [[Zipf's law]].&lt;ref name=&quot;understanding&quot;/&gt; Attempts have been made to put idf on a [[probability theory|probabilistic]] footing,&lt;ref&gt;See also [http://nlp.stanford.edu/IR-book/html/htmledition/probability-estimates-in-practice-1.html#p:justificationofidf Probability estimates in practice] in ''Introduction to Information Retrieval''.&lt;/ref&gt; by estimating the probability that a given document {{mvar|d}} contains a term {{mvar|t}} as

&lt;math&gt;
P(t|d) = \frac{|\{d \in D: t \in d\}|}{N}
&lt;/math&gt;

so that we can define idf as

&lt;math&gt;
\begin{align}
\mathrm{idf} &amp; = -\log P(t|d) \\
             &amp; = \log \frac{1}{P(t|d)} \\
             &amp; = \log \frac{N}{|\{d \in D: t \in d\}|}
\end{align}
&lt;/math&gt;

This probabilistic interpretation in turn takes the same form as that of [[self-information]]. However, applying such information-theoretic notions to problems in information retrieval leads to problems when trying to define the appropriate [[event space]]s for the required [[probability distribution]]s: not only documents need to be taken into account, but also queries and terms.&lt;ref name=&quot;understanding&quot;/&gt;

==Example of tf–idf==
Suppose we have term frequency tables for a collection consisting of only two documents, as listed on the right, then calculation of tf–idf for the term &quot;this&quot; in document 1 is performed as follows.

{| class=&quot;wikitable&quot; style=&quot;float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;&quot;
|+ Document 2
! Term
! | Term Count
|-
| this || 1
|-
| is
| 1
|-
| another
| 2
|-
| example
| 3
|}

{| class=&quot;wikitable&quot; style=&quot;float: right; margin-left: 1.5em; margin-right: 0; margin-top: 0;&quot;
|+ Document 1
! Term
! Term Count
|-
| this || 1
|-
| is
| 1
|-
| a
| 2
|-
| sample
| 1
|}

Tf, in its basic form, is just the frequency that we look up in appropriate table. In this case, it's one.

Idf is a bit more involved:
:&lt;math&gt; \mathrm{idf}(\mathsf{this}, D) =  \log \frac{N}{|\{d \in D: t \in d\}|}&lt;/math&gt;

The numerator of the fraction is the number of documents, which is two. The number of documents in which &quot;this&quot; appears is also two, giving
:&lt;math&gt; \mathrm{idf}(\mathsf{this}, D) =  \log \frac{2}{2} = 0&lt;/math&gt;

So tf–idf is zero for this term, and with the basic definition this is true of any term that occurs in all documents.

A slightly more interesting example arises from the word &quot;example&quot;, which occurs three times but in only one document. For this document, tf–idf of &quot;example&quot; is:
:&lt;math&gt;\mathrm{tf}(\mathsf{example}, d_2) = 3&lt;/math&gt;
:&lt;math&gt;\mathrm{idf}(\mathsf{example}, D) = \log \frac{2}{1} \approx 0.3010&lt;/math&gt;
:&lt;math&gt;\mathrm{tfidf}(\mathsf{example}, d_2) = \mathrm{tf}(\mathsf{example}, d_2) \times \mathrm{idf}(\mathsf{example}, D) = 3 \times 0.3010 \approx 0.9030&lt;/math&gt;

(using the [[base 10 logarithm]]).

==See also==
{{Div col||25em}}
* [[Okapi BM25]]
* [[Noun phrase]]
* [[Word count]]
* [[Vector space model]]
* [[PageRank]]
* [[Kullback–Leibler divergence]]
* [[Mutual information]]
* [[Latent semantic analysis]]
* [[Latent semantic indexing]]
* [[Latent Dirichlet allocation]]
{{Div col end}}

==References==
{{Reflist}}
* {{Cite doi|10.1108/eb026526}}
* {{Cite book
 | last1 = Salton | first1 = G | authorlink1 = Gerard Salton
 | last2 = McGill | first2 = M. J.
 | year = 1986
 | title = Introduction to modern information retrieval
 | publisher = [[McGraw-Hill]]
 | isbn = 978-0070544840
}}
* {{Cite doi|10.1145/182.358466}}
* {{Cite doi|10.1016/0306-4573(88)90021-0}}
* {{Cite doi|10.1145/1361684.1361686}}

==External links and suggested reading==
* [[Gensim]] is a Python library for vector space modeling and includes tf–idf weighting.
* [http://bscit.berkeley.edu/cgi-bin/pl_dochome?query_src=&amp;format=html&amp;collection=Wilensky_papers&amp;id=3&amp;show_doc=yes Robust Hyperlinking]: An application of tf–idf for stable document addressability.
* [http://infinova.wordpress.com/2010/01/26/distance-between-documents/ A demo of using tf–idf with PHP and Euclidean distance for Classification]
* [http://www.codeproject.com/KB/IP/AnatomyOfASearchEngine1.aspx Anatomy of a search engine]
* [http://lucene.apache.org/core/3_6_1/api/all/org/apache/lucene/search/Similarity.html tf–idf and related definitions] as used in [[Lucene]]
* [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer TfidfTransformer] in [[scikit-learn]]
* [http://scgroup.hpclab.ceid.upatras.gr/scgroup/Projects/TMG/ Text to Matrix Generator (TMG)]  MATLAB toolbox that can be used for various tasks in text mining (TM) specifically  i) indexing, ii) retrieval, iii) dimensionality reduction, iv) clustering, v) classification. The indexing step offers the user the ability to apply local and global weighting methods, including tf–idf.
* [http://blog.christianperone.com/?p=1589 Pyevolve: A tutorial series explaining the tf-idf calculation].
* [http://trimc-nlp.blogspot.com/2013/04/tfidf-with-google-n-grams-and-pos-tags.html TF/IDF with Google n-Grams and POS Tags]

{{DEFAULTSORT:Tf-Idf}}
[[Category:Statistical natural language processing]]
[[Category:Ranking functions]]
[[Category:Vector space model]]</text>
      <sha1>o3aho5kyo5gbl98myszs86wj7qenv3k</sha1>
    </revision>
  </page>
  <page>
    <title>Generalized vector space model</title>
    <ns>0</ns>
    <id>25945829</id>
    <revision>
      <id>620460282</id>
      <parentid>601481378</parentid>
      <timestamp>2014-08-09T04:11:10Z</timestamp>
      <contributor>
        <username>Tmdietrich</username>
        <id>22129183</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3245">{{Confusing|date=January 2010}}
The '''Generalized vector space model''' is a generalization of the [[vector space model]] used in [[information retrieval]].  Many classifiers, especially those which are related to document or text classification, use the TFIDF basis of VSM.  However, this is where the similarity between the models ends - the generalized model uses the results of the TFIDF dictionary to generate similarity metrics based on distance or angle difference, rather than centroid based classification.  '''Wong et al.'''&lt;ref name=&quot;wong&quot;&gt;{{cite | title=Generalized vector spaces model in information retrieval | url=http://doi.acm.org/10.1145/253495.253506 | first=S. K. M. | last=Wong | coauthors=Wojciech Ziarko, Patrick C. N. Wong | publisher=[[Association for Computing Machinery|SIGIR ACM]] | date=1985-06-05}}&lt;/ref&gt; presented an analysis of the problems that the pairwise orthogonality assumption of the [[vector space model]] (VSM) creates. From here they extended the VSM to the generalized vector space model (GVSM).

==Definitions==

GVSM introduces a term to term correlations, which deprecate the pairwise orthogonality assumption. More specifically, the factor considered a new space, where each term vector ''t&lt;sub&gt;i&lt;/sub&gt;'' was expressed as a linear combination of ''2&lt;sup&gt;n&lt;/sup&gt;'' vectors ''m&lt;sub&gt;r&lt;/sub&gt;'' where ''r = 1...2&lt;sup&gt;n&lt;/sup&gt;''.

For a document ''d&lt;sub&gt;k&lt;/sub&gt;'' and a query ''q'' the similarity function now becomes:

:&lt;math&gt;sim(d_k,q) = \frac{\sum _{j=1}^n \sum _{i=1}^n w_{i,k}*w_{j,q}*t_i \cdot t_j }{\sqrt{\sum _{i=1}^n w_{i,k}^2}*\sqrt{\sum _{i=1}^n w_{i,q}^2}}&lt;/math&gt;

where ''t&lt;sub&gt;i&lt;/sub&gt;'' and ''t&lt;sub&gt;j&lt;/sub&gt;'' are now vectors of a ''2&lt;sup&gt;n&lt;/sup&gt;'' dimensional space.

Term correlation &lt;math&gt;t_i \cdot t_j&lt;/math&gt; can be implemented in several ways. For an example, Wong et al. uses the term occurrence frequency matrix obtained from automatic indexing as input to their algorithm. The term occurrence  and the output is the term correlation between any pair of index terms.

==Semantic information on GVSM==

There are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model:
# compute semantic correlations between terms
# compute frequency co-occurrence statistics from large corpora

Recently Tsatsaronis&lt;ref&gt;{{cite | title=A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness | url=http://www.aclweb.org/anthology/E/E09/E09-3009.pdf | last= Tsatsaronis | first=George | coauthors=Vicky Panagiotopoulou | publisher=[[Association for Computing Machinery|EACL ACM]] |date=2009-04-02}}&lt;/ref&gt; focused on the first approach.

They measure semantic relatedness (''SR'') using a thesaurus (''O'') like [[WordNet]]. It considers the path length, captured by compactness (''SCM''), and the path depth, captured by semantic path elaboration (''SPE'').
They estimate the &lt;math&gt;t_i \cdot t_j&lt;/math&gt; inner product by:

&lt;math&gt;t_i \cdot t_j = SR((t_i, t_j), (s_i, s_j), O)&lt;/math&gt;

where ''s&lt;sub&gt;i&lt;/sub&gt;'' and ''s&lt;sub&gt;j&lt;/sub&gt;'' are senses of terms ''t&lt;sub&gt;i&lt;/sub&gt;'' and ''t&lt;sub&gt;j&lt;/sub&gt;'' respectively, maximizing &lt;math&gt;SCM \cdot SPE&lt;/math&gt;.

== References ==
{{reflist}}

[[Category:Vector space model]]</text>
      <sha1>pur3gc7svc32ynbpp9vmsdeh4xwu7b2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Search algorithms</title>
    <ns>14</ns>
    <id>1406201</id>
    <revision>
      <id>627523616</id>
      <parentid>588940223</parentid>
      <timestamp>2014-09-29T09:59:41Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>removed [[Category:Artificial intelligence]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="117">{{Commons category|Search algorithms}}
{{Cat main|Search algorithms}}

[[Category:Algorithms]]
[[Category:Searching]]</text>
      <sha1>71qv1e5rwtnss23j4kpaq1jhyxfpdhi</sha1>
    </revision>
  </page>
  <page>
    <title>Agrep</title>
    <ns>0</ns>
    <id>308939</id>
    <revision>
      <id>641252709</id>
      <parentid>588680049</parentid>
      <timestamp>2015-01-06T15:24:55Z</timestamp>
      <contributor>
        <username>Naraht</username>
        <id>230475</id>
      </contributor>
      <comment>underscores</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3989">{{lowercase|title=agrep}}
{{Infobox software
| name                   = agrep
| logo                   = &lt;!-- Image name is enough --&gt;
| logo caption           = 
| logo_size              = 
| logo_alt               = 
| screenshot             = &lt;!-- Image name is enough --&gt;
| caption                = 
| screenshot_size        = 
| screenshot_alt         = 
| collapsible            = 
| developer              = {{Plainlist|
* [[Udi Manber]]
* Sun Wu
}}
| released               = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| status                 = 
| programming language   = 
| operating system       = {{Plainlist|
* [[Unix-like]]
* [[OS/2]]
* [[DOS]]
* [[Microsoft Windows|Windows]]
}}
| platform               = 
| size                   = 
| language               = 
| language count         = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| language footnote      = 
| genre                  = [[Pattern matching]]
| license                = 
| website                = &lt;!-- {{URL|example.org}} --&gt;
| standard               = 
}}

'''agrep''' (approximate [[grep]]) is a [[proprietary software|proprietary]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].

It selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu's [[bitap algorithm]] based on [[Levenshtein distance]]s.

agrep is also the [[search engine]] in the indexer program [[GLIMPSE]]. agrep is free for private and non-commercial use only, and belongs to the University of Arizona.

== Alternative implementations ==
A more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.&lt;ref&gt;{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}&lt;/ref&gt; Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].

FREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.&lt;ref&gt;{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}&lt;/ref&gt; However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.

==References==
{{Reflist}}

==External links==
* Wu-Manber agrep
**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add &lt;code&gt;-Wno-return-type&lt;/code&gt; to the &lt;code&gt;CFLAGs  = -O&lt;/code&gt; line in the Makefile)
**[http://www.tgries.de/agrep For DOS, Windows and OS/2 home page]
*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for &quot;agrep&quot; in Christoph's Personal Wiki]

*See also
**[http://laurikari.net/tre TRE regexp matching package]
**[http://www.bell-labs.com/project/wwexptools/cgrep/ cgrep a command line approximate string matching tool]
**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool
**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]

[[Category:Searching]]
[[Category:Unix text processing utilities]]</text>
      <sha1>izvdwmxx50v41ulgx2zg4pkjoyjd3mo</sha1>
    </revision>
  </page>
  <page>
    <title>Ptx (Unix)</title>
    <ns>0</ns>
    <id>1442890</id>
    <revision>
      <id>507033744</id>
      <parentid>334905076</parentid>
      <timestamp>2012-08-12T12:57:40Z</timestamp>
      <contributor>
        <username>Wanzhang</username>
        <id>16959257</id>
      </contributor>
      <comment>add some thing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="594">{{Unreferenced stub|auto=yes|date=December 2009}}
{{Lowercase|title=ptx}}
'''ptx''' is a [[Unix]] utility, named for the ''permuted index'' which can perform the function of the Keyword in Context ([[Key Word in Context|KWIC]]) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, thesauruses, or web sites to aid in locating entries of interest.

==See also==
* [[Concordancer]]

[[Category:Searching]]
[[Category:Unix text processing utilities]]


{{Unix-stub}}</text>
      <sha1>28i2zzdppmrz9261yx7jiufouta43xz</sha1>
    </revision>
  </page>
  <page>
    <title>Reverse DNS lookup</title>
    <ns>0</ns>
    <id>1286913</id>
    <revision>
      <id>643000889</id>
      <parentid>642991308</parentid>
      <timestamp>2015-01-18T03:56:19Z</timestamp>
      <contributor>
        <username>Kbrose</username>
        <id>3938795</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/50.141.119.37|50.141.119.37]] ([[User talk:50.141.119.37|talk]]) to last version by Zac67</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8628">'''{{Redirect|Reverse DNS}}

In [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' (rDNS) is the determination of a [[domain name]] that is associated with a given  [[IP address]] using the [[Domain Name System]] (DNS) of the [[Internet]].

Computer networks use the Domain Name System to determine the IP address associated with a domain name. This process is also known as ''forward'' DNS resolution.  ''Reverse'' DNS lookup is the inverse process, the resolution of an IP address to its designated domain name.

The reverse DNS database of the Internet is rooted in the ''Address and Routing Parameter Area'' (&lt;tt&gt;[[.arpa|arpa]]&lt;/tt&gt;) [[top-level domain]] of the Internet. [[IPv4]] uses the &lt;tt&gt;in-addr.arpa&lt;/tt&gt; domain and the &lt;tt&gt;ip6.arpa&lt;/tt&gt; domain is delegated for [[IPv6]]. The process of reverse resolving an IP address uses the ''pointer'' DNS record type ([[List of DNS record types#Resource records|PTR record]]).

Informational RFCs (RFC 1033, RFC 1912 Section 2.1) specify that ''&quot;Every Internet-reachable host should have a name&quot;'' and that such names match with a reverse pointer record, but it is not a requirement of standards governing operation of the DNS itself.

==IPv4 reverse resolution==
Reverse DNS lookups for [[IPv4]] addresses use a ''reverse IN-ADDR entry'' in the special domain &lt;tt&gt;in-addr.arpa&lt;/tt&gt;. In this domain, an IPv4 address is represented as a concatenated sequence of ''four decimal numbers'', separated by dots, to which is appended the second level domain suffix &lt;tt&gt;.in-addr.arpa&lt;/tt&gt;. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four 8-bit portions and converting each 8-bit portion into a decimal number.  These decimal numbers are then concatenated in the order: least significant 8-bit portion first (leftmost), most significant 8-bit portion last (rightmost). It is important to note that ''this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses'' in textual form.
For example, an address (A) record for &lt;tt&gt;mail.example.com&lt;/tt&gt; points to the IP address 192.0.2.5.
In pointer records of the reverse database, this IP address is stored as the domain name &lt;tt&gt;5.2.0.192.in-addr.arpa&lt;/tt&gt; pointing back to its designated host name &lt;tt&gt;mail.example.com&lt;/tt&gt;. 
This allows it to pass the [[Forward Confirmed reverse DNS]] process.

===Classless reverse DNS method===
Historically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A.  By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using ''canonical name'' ([[CNAME]]) DNS records.

==IPv6 reverse resolution==
Reverse DNS lookups for [[IPv6]] addresses use the special domain &lt;code&gt;ip6.arpa&lt;/code&gt;. An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address &lt;code&gt;2001:db8::567:89ab&lt;/code&gt; is &lt;code&gt;b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa&lt;/code&gt;.

==Multiple pointer records==
While most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended, unless there is a specific need.  For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically.  Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.&lt;ref&gt;[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]&lt;/ref&gt; In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause responses to be truncated if they exceed the DNS 512 byte UDP message limit.

==Records other than PTR records==
Record types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]] (RFC 4025), [[Secure Shell|SSH]] (RFC 4255) and [[Internet Key Exchange|IKE]] (RFC 4322).
[[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] (RFC 6763) uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.&lt;ref&gt;{{Citation | publisher = IETF | title = RFC 6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}&lt;/ref&gt;
Less standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.

==Uses==
The most common uses of the reverse DNS include:
* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the &quot;Received:&quot; trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.
* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, dynamically assigned addresses, or other inexpensive Internet services.  Owners of such IP addresses typically assign them generic rDNS names such as &quot;1-2-3-4-dynamic-ip.example.com.&quot;  Some corporate anti-spam services take the view that the vast majority, but by no means all, of e-mail that originates from these computers is spam with spam filters refusing e-mail with such rDNS names.&lt;ref&gt;[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]&lt;/ref&gt;&lt;ref&gt;[http://postmaster.aol.com/info/rdns.html reference page from AOL]&lt;/ref&gt; However data has shown that just as much if not more spam has originated from unpatched machines within corporate networks that are more likely to use out of date browsers than cheaper services such as DSL networks not to mention the difficulty of blocking spam from major providers like Yahoo and Hotmail. A recent shift has shown that spamming has switched to mainly coming from hosting companies making using rDNS even less useful.&lt;ref&gt;http://www.mailchannels.com/blog/2013/03/worlds-largest-spam-sources-are-all-hosting-companies/&lt;/ref&gt; All of this adds to the argument that the few services that choose to block email servers purely on the basis of rDNS are simply discriminating without merit and often miss out more pro-active and useful indiscriminate anti spam measures.&lt;ref&gt;http://ask.slashdot.org/story/11/10/13/1643202/ask-slashdot-is-reverse-dns-a-worthy-standard-for-fighting-spam&lt;/ref&gt;
* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, mainly because [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually can't pass verification for it when they use [[zombie computer]]s to forge domains.
* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address

==See also==
*[[Forward-confirmed reverse DNS]]

==References==
{{reflist}}

==External links==
*{{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}
* [http://dns.icann.org ICANN DNS Operations]
* RFC 2317 documents a way to do rDNS delegation for [[Classless Inter-Domain Routing|CIDR]] blocks
* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]
* RDNS policies: [http://postmaster.aol.com/Postmaster.Errors.php#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]

[[Category:Searching]]
[[Category:Domain name system]]

[[nl:Domain Name System#Omgekeerde lookups]]'''</text>
      <sha1>rfgusdc1ghn2p17inekrum7o8w0drwg</sha1>
    </revision>
  </page>
  <page>
    <title>Statistically Improbable Phrases</title>
    <ns>0</ns>
    <id>2724706</id>
    <revision>
      <id>604119323</id>
      <parentid>587775480</parentid>
      <timestamp>2014-04-14T06:34:50Z</timestamp>
      <contributor>
        <ip>213.70.217.168</ip>
      </contributor>
      <comment>/* Example */ added dot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1401">'''Statistically Improbable Phrases''', '''Statimprophrases''' or '''SIPs''' constitute a system developed by [[Amazon.com]] to compare all of the books they index in the Search Inside! program and find phrases in each that are the most unlikely to be found in any other book indexed.&lt;ref&gt;{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}&lt;/ref&gt; The system is used to find the most nearly unique portions of books for use as a summary or keyword.

== Example == 
The Statistically Improbable Phrases of Darwin's [[On the Origin of Species]] are: ''temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species'' and ''mongrel offspring''.&lt;ref&gt;[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005&lt;/ref&gt;

==See also==
*[[Googlewhack]] — a pair of words occurring on a single webpage, as indexed by Google
*[[tf-idf]] — a statistic used in information retrieval and text mining.

==References==
{{reflist}}

{{Amazon}}

[[Category:Amazon.com]]
[[Category:Searching]]
[[Category:Bookselling]]</text>
      <sha1>lpdvggwavfwd9npp6gvodhl4yjbnx4l</sha1>
    </revision>
  </page>
  <page>
    <title>Controlled vocabulary</title>
    <ns>0</ns>
    <id>1850719</id>
    <revision>
      <id>629558390</id>
      <parentid>625642655</parentid>
      <timestamp>2014-10-14T10:17:25Z</timestamp>
      <contributor>
        <ip>14.139.231.162</ip>
      </contributor>
      <comment>/* Indexing languages */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15104">{{refimprove|date=June 2012}}

'''Controlled vocabularies''' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]], [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designer of the vocabulary, in contrast to natural language vocabularies, where there is no restriction on the vocabulary.

== In library and information science ==

In [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.{{ref|warner}}{{ref|fast}} Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.

For example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms -- subject headings in this case -- have to be chosen to handle choices between variant spellings of the same concept (American versus British), choice among scientific and popular terms (Cockroaches versus ''Periplaneta americana''), and choices between synonyms (automobile versus cars), among other difficult issues.

Choices of authorized terms are based on the principles of ''user warrant'' (what terms users are likely to use), ''literary warrant'' (what terms are generally used in the literature and documents), and ''structural warrant'' (terms chosen by considering the structure, scope of the controlled vocabulary).

Controlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term &quot;pool&quot; has to be qualified to refer to either swimming pool, or the game pool to ensure that each authorized term or heading refers to only one concept.

There are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.

Historically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.

For example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term &quot;Broader term&quot; and &quot;Narrow term&quot;.

The [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.

Choosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.

Controlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].

== Indexing languages ==

There are three main types of indexing languages.

* Controlled indexing language - Only approved terms can be used by the indexer to describe the document
* [[Natural language]] indexing language - Any term from the document in question can be used to describe the document.
* Free indexing language  - Any term (not only from the document) can be used to describe the document.

When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each documen

In recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is ''indexed''). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.

Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word ''football'' for example. ''Football'' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|Association football]], which also happens to be called ''[[soccer]]'' in several countries. The [[English language]] [[football (word)|word football]] is also applied to [[Rugby football]] ([[Rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for ''football'' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.

Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).

In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don't need to worry about searching for other terms that might be synonyms of that term.

However, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.

This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.

Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example an article might mention football as a secondary focus, and the indexer might decide not to tag it with &quot;football&quot; because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.

On the other hand free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.

Controlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author's own words.

The use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.

Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.

==Applications==
Controlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.

In large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.

Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].

It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.{{ref|doctorow}} To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.{{ref|pilgrim}}

==See also==
*[[Controlled natural language]]
*[[IMS VDEX|IMS Vocabulary Definition Exchange]]
*[[Nomenclature]]
*[[Ontology (computer science)]]
*[[Terminology]]
*[[Thesaurus]]
*[[Universal Data Element Framework]]
*[[Vocabulary-based transformation]]

==References==
#{{note|warner}} Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].
#{{note|fast}} Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]
#{{note|doctorow}} Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].
#{{note|pilgrim}} Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].
#[http://www.imresources.fit.qut.edu.au/vocab/ Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes.
#[http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies] {{Dead link|date=February 2011}} Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.

==External links==
* [http://www.controlledvocabulary.com/ controlledvocabulary.com] — explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.
* [http://www.photo-keywords.com/ photo-keywords.com/] — useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.
* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]

{{Lexicography}}

[[Category:Searching]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]
[[Category:Controlled vocabularies]]
[[Category:Library science]]
[[Category:Information science]]</text>
      <sha1>3yudodvnhh6n7jzy8xtnn14cpla8wj8</sha1>
    </revision>
  </page>
  <page>
    <title>Multimedia search</title>
    <ns>0</ns>
    <id>5987236</id>
    <revision>
      <id>647631186</id>
      <parentid>612260704</parentid>
      <timestamp>2015-02-18T00:08:24Z</timestamp>
      <contributor>
        <username>KH-1</username>
        <id>21857263</id>
      </contributor>
      <comment>spammy</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3618">'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.
Multimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.
We can distinguish two methodologies in multimedia search:
*'''Metadata search''': the search is made on the layers of [[metadata]].
* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.


==Metadata search==

Search is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.

There are three processes which should be done in this method:
*'''[[Multimedia Information Retrieval#Feature Extraction Methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.
*'''[[ Multimedia Information Retrieval#Feature Extraction Methods |Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])
*'''[[ Multimedia Information Retrieval#Categorization Methods | Categorization of media descriptions ]]''' into classes.

==[[Query by Example]]==

In [[query by example]] the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often it’s used [[Search engine indexing |audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:
*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].
*Compare descriptors of the query and our database’s media.
*List the media sorted by maximum coincidence.

==Multimedia search engine==
There are two big search families, in function of the content:
* [[Visual search engine]]
*[[Audio search engine]]

===[[Visual search engine]]===
Inside this family we can distinguish two topics: [[image search]] and [[video search]]

*'''[[Image search]]''': Although usually it’s used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example [[QR codes]].
*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.

===[[Audio search engine]]===
There are different methods of audio searching:
*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].
*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album…) . There are some programs of [[music recognition]]. for example: [[Shazam (service)|Shazam]] or [[SoundHound]].

==See also==
*[[Search engine indexing]]
*[[Multimedia]]
*[[Multimedia Information Retrieval]]
*[[Streaming media]]
*[[Journal of Multimedia]]
*[[List of search engines#Multimedia|List of search engines]]
*[[Video search engine]]

==External links==

[[Category:Searching]]
[[Category:Multimedia]]</text>
      <sha1>3dmh3qt4tm2jw6padji6au9s8mxb899</sha1>
    </revision>
  </page>
  <page>
    <title>Hybrid search engine</title>
    <ns>0</ns>
    <id>2851233</id>
    <revision>
      <id>423118952</id>
      <parentid>410252594</parentid>
      <timestamp>2011-04-09T02:54:15Z</timestamp>
      <contributor>
        <username>MerlIwBot</username>
        <id>14161758</id>
      </contributor>
      <minor/>
      <comment>robot Removing: [[de:Hybridsuchmaschine]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="525">{{Notability|date=December 2009}}
A '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.

==References==
{{No footnotes|date=April 2010}}
*http://eprints.ecs.soton.ac.uk/17457/
*http://eprints.whiterose.ac.uk/3771/
*http://www.picollator.com

[[Category:Searching]]


{{web-stub}}</text>
      <sha1>mwgiblus4ven4sprrq2xx741uh3g5ak</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Internet search</title>
    <ns>14</ns>
    <id>8321034</id>
    <revision>
      <id>547928112</id>
      <parentid>490558243</parentid>
      <timestamp>2013-03-31T03:54:16Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8551728]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="235">{{Cat main|Internet search}}

[[Category:Web services]]
[[Category:Searching]]
[[Category:World Wide Web|Search]] &lt;!-- searching is a web function. Note that &quot;Internet search&quot; redirects to &quot;Web page search&quot; (or something like that)---&gt;</text>
      <sha1>3d3725gpz1gv28uisjv2toissmykyqf</sha1>
    </revision>
  </page>
  <page>
    <title>Contextual Query Language</title>
    <ns>0</ns>
    <id>9672320</id>
    <revision>
      <id>599561646</id>
      <parentid>599561644</parentid>
      <timestamp>2014-03-14T09:57:25Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contributions/115.254.32.162|115.254.32.162]] to version by 207.6.208.71. False positive? [[User:ClueBot NG/FalsePositives|Report it]]. Thanks, [[User:ClueBot NG|ClueBot NG]]. (1744997) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2662">'''Contextual Query Language''' (CQL), previously known as '''Common Query Language''',&lt;ref&gt;[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress&lt;/ref&gt; is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].

== Examples of query syntax ==

Simple queries:

&lt;blockquote&gt;&lt;tt&gt;dinosaur&lt;br/&gt;
&quot;complete dinosaur&quot;&lt;br/&gt;
title = &quot;complete dinosaur&quot;&lt;br/&gt;
title exact &quot;the complete dinosaur&quot;&lt;/tt&gt;&lt;/blockquote&gt;

Queries using [[Boolean logic]]:

&lt;blockquote&gt;&lt;tt&gt;dinosaur or bird&lt;br/&gt;
Palomar assignment and &quot;ice age&quot;&lt;br/&gt;
dinosaur not reptile&lt;br/&gt;
dinosaur and bird or dinobird&lt;br/&gt;
(bird or dinosaur) and (feathers or scales)&lt;br/&gt;
&quot;feathered dinosaur&quot; and (yixian or jehol)&lt;/tt&gt;&lt;/blockquote&gt;

Queries accessing [[index (publishing)|publication indexes]]:

&lt;blockquote&gt;&lt;tt&gt;publicationYear &lt; 1980&lt;br/&gt;
lengthOfFemur &gt; 2.4&lt;br/&gt;
bioMass &gt;= 100&lt;/tt&gt;&lt;/blockquote&gt;

Queries based on the proximity of words to each other in a document:

&lt;blockquote&gt;&lt;tt&gt;ribs prox/distance&lt;=5 chevrons&lt;br/&gt;
ribs prox/unit=sentence chevrons&lt;br/&gt;
ribs prox/distance&gt;0/unit=paragraph chevrons&lt;/tt&gt;&lt;/blockquote&gt;

Queries across multiple [[Dimension (data warehouse)|dimensions]]:

&lt;blockquote&gt;&lt;tt&gt;date within &quot;2002 2005&quot;&lt;br/&gt;
dateRange encloses 2003&lt;/tt&gt;&lt;/blockquote&gt;

Queries based on [[Relevance (information retrieval)|relevance]]:

&lt;blockquote&gt;&lt;tt&gt;subject any/relevant &quot;fish frog&quot;&lt;br/&gt;
subject any/rel.lr &quot;fish frog&quot;&lt;/tt&gt;&lt;/blockquote&gt;

The latter example specifies using a specific [[algorithm]] for [[logistic regression]].&lt;ref&gt;[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://www.loc.gov/standards/sru/cql/ CQL home page]
* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]
* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]

{{Query languages}}

{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}
{{LOC-stub}}

[[Category:Searching]]
[[Category:Library science]]
[[Category:Library of Congress]]
[[Category:Query languages]]
[[Category:Knowledge representation languages]]</text>
      <sha1>ah1sad9fvir9gx1rsljyu6id1cixbwh</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data search engines</title>
    <ns>14</ns>
    <id>7528659</id>
    <revision>
      <id>547444343</id>
      <parentid>491548704</parentid>
      <timestamp>2013-03-28T13:52:57Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363887]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="135">{{Cat main|Data search engine}}

[[Category:Metadata]]
[[Category:XML]]
[[Category:Database management systems]]
[[Category:Searching]]</text>
      <sha1>jvpi1mnkm1z0kza4axpu3ffcv634fxb</sha1>
    </revision>
  </page>
  <page>
    <title>Search-oriented architecture</title>
    <ns>0</ns>
    <id>7470226</id>
    <revision>
      <id>490357222</id>
      <parentid>459895790</parentid>
      <timestamp>2012-05-02T20:37:52Z</timestamp>
      <contributor>
        <username>Edward</username>
        <id>4261</id>
      </contributor>
      <minor/>
      <comment>link [[search engine technology]] using [[User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1891">{{unreferenced|date=October 2007}}
The use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.

In a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.

In a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.

The benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. 

== Contrast with ==
* [[Service-oriented architecture]] (SOA)
* [[Service-Oriented Modeling]]

== See also ==
* [[Hibernate search]]
 
[[Category:Software architecture]]
[[Category:Data search engines]]
[[Category:Searching]]</text>
      <sha1>m9uo916908bb31wio51zqm8rf5mggct</sha1>
    </revision>
  </page>
  <page>
    <title>IBM Omnifind</title>
    <ns>0</ns>
    <id>13762814</id>
    <revision>
      <id>632587992</id>
      <parentid>627064054</parentid>
      <timestamp>2014-11-05T18:38:00Z</timestamp>
      <contributor>
        <ip>173.62.108.181</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2348">'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].
It did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.&lt;ref&gt;[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]&lt;/ref&gt; IBM OmniFind as a standalone product was withdrawn in April 2011&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&amp;infotype=an&amp;appname=iSource&amp;supplier=897&amp;letternum=ENUS911-075 IBM US Announcement Letter]&lt;/ref&gt; and is now part of [[IBM Watson Content Analytics with Enterprise Search]].&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&amp;subtype=CA&amp;htmlfid=897/ENUS211-133 IBM US Announcement Letter]&lt;/ref&gt;

'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&amp;infotype=an&amp;appname=iSource&amp;supplier=897&amp;letternum=ENUS910-115 IBM US Announcement Letter]&lt;/ref&gt;

'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.

== See also ==
* [[Languageware]]
* [[UIMA]]
* [[Comparison of enterprise search software]]
* [[List of enterprise search vendors]]

==External links==
* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]
* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}
* [http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] {{Dead link|date=January 2012}}
* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]

==Notes==
{{reflist}}

[[Category:IBM software|OmniFind]]
[[Category:Searching]]</text>
      <sha1>atqoaxhmim5786xf4n0bkjczc05t70g</sha1>
    </revision>
  </page>
  <page>
    <title>OpenGrok</title>
    <ns>0</ns>
    <id>5768711</id>
    <revision>
      <id>639341245</id>
      <parentid>608555426</parentid>
      <timestamp>2014-12-23T14:43:38Z</timestamp>
      <contributor>
        <username>MureninC</username>
        <id>675035</id>
      </contributor>
      <comment>/* External links */ add bxr.su</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3070">{{multiple issues|
{{Advert|date=March 2012}}
{{Notability|Products|date=March 2012}}
}}

{{Infobox software
| name                   = OpenGrok
| logo                   = [[Image:OpenGrok Logo.png|150px|OpenGrok Logo]]
| screenshot             = 
| caption                =
| collapsible            = yes
| developer              = [[Sun Microsystems]]/[[Oracle Corporation]]
| latest release version = 0.12.1
| latest release date    = {{release date|2014|04|29}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Index (search engine)|Index]]er and [[cross-reference]]r with [[Revision control]]
| license                = [[CDDL]]
| website                = http://opengrok.github.com/OpenGrok/
}}

'''OpenGrok''' is a [[source code]] search and cross reference engine. It helps programmers to search, cross-reference and navigate source code trees.

It can understand various [[program (computing)|program]] [[file formats]] and [[version control]] histories like [[Monotone (software)|Monotone]], [[Source Code Control System|SCCS]], [[Revision Control System|RCS]], [[Concurrent Versions System|CVS]], [[Subversion (software)|Subversion]], [[Mercurial (software)|Mercurial]], [[Git (software)|Git]], [[IBM Rational ClearCase|Clearcase]], [[Perforce]] and [[Bazaar (software)|Bazaar]].&lt;ref&gt;https://github.com/OpenGrok/OpenGrok/wiki/Supported-Revision-Control-Systems&lt;/ref&gt;

The name comes from the term ''[[grok]]'', a [[jargon]] term used in computing to mean &quot;profoundly understand&quot;. The term ''[[grok]]'' originated in a science fiction novel by Robert A. Heinlein called ''[[Stranger in a Strange Land]]''.

OpenGrok is being developed mainly by [[Oracle Corporation]] (former [[Sun Microsystems]]) engineers with help from its community. OpenGrok is released under the terms of the [[Common Development and Distribution License]] (CDDL).

== Features ==

OpenGrok's features include:

* Full text Search
* Definition Search
* Identifier Search
* Path search
* History Search
* Shows matching lines
* Hierarchical Search
* query syntax like ''AND'', ''OR'', ''field'':
* Incremental update
* Syntax highlighting-Xref
* Quick navigation inside the file
* Interface for SCM
* Usable URLs
* Individual file download
* Changes at directory level
* Multi language support

== See also ==

* [[LXR Cross Referencer]]
* [[ViewVC]]
* [[FishEye (software)]]

== References ==

{{reflist}}

== External links ==
* [http://opengrok.github.com/OpenGrok/ OpenGrok project page]
* {{ohloh|opengrok}}
* [http://code.metager.de/source/ Metager]
* [http://BXR.SU/ Super User's BSD Cross Reference]

{{Sun Microsystems}}
{{Java (Sun)}}

[[Category:Cross-platform free software]]
[[Category:Free revision control software]]
[[Category:Source code]]
[[Category:Searching]]
[[Category:Java platform software]]
[[Category:Concurrent Versions System]]
[[Category:Subversion]]
[[Category:Code search engines]]


{{programming-software-stub}}</text>
      <sha1>6jit3unh18tazn01i4iwfhshm6bhsir</sha1>
    </revision>
  </page>
  <page>
    <title>Reverse telephone directory</title>
    <ns>0</ns>
    <id>5279920</id>
    <revision>
      <id>645767475</id>
      <parentid>645672192</parentid>
      <timestamp>2015-02-05T16:30:11Z</timestamp>
      <contributor>
        <username>Barek</username>
        <id>1746167</id>
      </contributor>
      <minor/>
      <comment>Removed protection from &quot;[[Reverse telephone directory]]&quot;: remove semi-protection - issue resolved via WT:WPSPAM and WP:SBL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6694">A '''reverse telephone directory''' (also known as a '''gray pages''' directory, criss-cross directory or '''reverse phone lookup''') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer's details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.

Reverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only.

Publicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.

==History==
Printed reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].&lt;ref&gt;{{cite news | url=http://news.google.com/newspapers?nid=1454&amp;dat=19720102&amp;id=87osAAAAIBAJ&amp;sjid=vgkEAAAAIBAJ&amp;pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}&lt;/ref&gt; In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.

==Australia==
In 2001, a legal case ''[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd'' was heard in the Australian Federal Court.&lt;ref&gt;{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}&lt;/ref&gt;&lt;ref name=austliiPP&gt;{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}&lt;/ref&gt; gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.

In February 2010 a Federal Court of Australia case ''[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd'' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.&lt;ref&gt;{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}&lt;/ref&gt;

As it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.

==United States==

In United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called &quot;Your Listing Not Published&quot; and the cost ranges between $0.80 and $1.50 for residential customers.

As [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the &quot;Wireless 411 Privacy Act&quot; 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.&lt;ref&gt;{{cite web | url=http://www.ncbi.nlm.nih.gov/pubmed/15652722 | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | publisher=Ncbi.nlm.nih.gov | work=2005 Feb | accessdate=9 February 2014 | author=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J.}}&lt;/ref&gt;

In recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.

==United Kingdom==
In the United Kingdom proper, reverse directory information is not publicly available.&lt;ref&gt;{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner's Office | accessdate=9 February 2014}}&lt;/ref&gt; However, in the [[Channel Islands]] it is provided in the printed telephone directories. 

Although the information is, of necessity, available to emergency services, for other agencies it is treated as 'communication data' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.

==References==
{{reflist}}
==External links==
&lt;!-- Do not delete these comments. --&gt;
&lt;!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --&gt; 
*[http://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]
*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]


[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:Searching]]</text>
      <sha1>ref2zx7h0az1vut464cyxbdox7h2fdu</sha1>
    </revision>
  </page>
  <page>
    <title>Desktop search</title>
    <ns>0</ns>
    <id>1274156</id>
    <revision>
      <id>645679238</id>
      <parentid>645679127</parentid>
      <timestamp>2015-02-05T00:19:22Z</timestamp>
      <contributor>
        <username>Cathryngoodman</username>
        <id>20382465</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15606">{{multiple issues|
{{Cleanup|date=October 2010}}
{{technical|date=October 2014}}
}}

[[File:Puggle-search.png|thumb|Puggle Desktop Search]]
[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]
'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.

One of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.

A variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.

Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users needs to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]] — the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.&lt;ref&gt;{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.&lt;/ref&gt;  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.

Companies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index — but not preview — items they should not even know exist.{{Citation needed|date = November 2009}}

Historically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.

== Technologies ==
Most desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,&lt;ref&gt;{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}&lt;/ref&gt; which performs searches over only filenames &amp;mdash; not the files' contents &amp;mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,&lt;ref&gt;{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}&lt;/ref&gt; which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products, such as Lookeen,&lt;ref&gt;{{cite web|title=Real-Time Indexing and Lookeen 8|url=http://www.lookeen.net/2884/News/real-time-ndexing-and-lookeen-8/|publisher=Lookeen|accessdate=26 October 2014}}&lt;/ref&gt; have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.

Desktop search tools typically collect three types of information about files:
* file and folder names
* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]
* file content (for supported types of documents only)

To search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.

Long-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.&lt;ref&gt;[http://www.niallkennedy.com/blog/archives/2006/10/video-search.html &quot;The current state of video search&quot;, by Niall Kennedy]&lt;/ref&gt;&lt;ref&gt;[http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html &quot;The current state of audio search&quot;, by Niall Kennedy]&lt;/ref&gt;

The sector attracted considerable attention from the struggle between Microsoft and Google.&lt;ref&gt;[http://news.bbc.co.uk/1/hi/technology/3952285.stm &quot;Search wars hit desktop computers&quot;. (Oct 2004) BBC News]&lt;/ref&gt; According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.&lt;ref&gt;[http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/ &quot;Microsoft agrees to change Vista Desktop Search Tool&quot; (Jun 2007)]&lt;/ref&gt;

As of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more. &lt;ref&gt;[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ &quot;Google Desktop Update&quot; (Sept 2011)]&lt;/ref&gt;  

X1 makes one of the leading desktop search products on the market. X1 Search 8 is a software alternative to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more. &lt;ref&gt;[http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/ &quot;X1 rises again with Desktop Search 8, Virtual Edition&quot; (May 2013)]&lt;/ref&gt;   

==Platforms &amp; their histories==
There are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS &amp; [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.

'''Windows'''

Today's Windows Search replaced WDS (Windows Desktop Search). WDS, in turn, replaced Indexing Service. A &quot;a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching&quot;&lt;ref&gt;https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx&lt;/ref&gt; Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.&lt;ref&gt;https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx&lt;/ref&gt; This made indexing large amounts of files require extremely powerful hardware and very long wait times.

In 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought &quot;Instant searching&quot; meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.&lt;ref&gt;http://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx&lt;/ref&gt; Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.

With the release of Windows Vista came Windows Search 3.1. Unlike it's predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the RAM and CPU requirements were greatly reduced. Cutting back indexing times immensely. This brings us to the Windows Search 4.0 which is currently running on all PCs with Windows 7 and up.

'''Mac OS'''

Mac OS was the first to fully implement Desktop Search, it allowed users to fully search all documents with in their Macintosh computer. This means file format types, meta-data on those file formats and the content within the files. Released in 1994 two years before Windows Search was released, AppleSearch already had content searching. The biggest issue that AppleSearch had large resource requirements &quot;AppleSearch requires at least a 68040 processor and 5MB of RAM.&quot;&lt;ref&gt;http://infomotions.com/musings/tricks/manuscript/1600-0001.html&lt;/ref&gt; A Macintosh computer that had these specs cost around $1400 in today's dollars that's around $2050.&lt;ref&gt;http://stats.areppim.com/calc/calc_usdlrxdeflator.php&lt;/ref&gt; On top of that, the software it self cost around $1400 for a single licenses.

In 1997, Sherlock was released alongside Mac OS 8.5. Sherlock, named after the famous fictional detective Sherlock Holmes, was integrated into Mac OS's file browser: Finder. Sherlock extended the desktop search to the world wide web. Allowing users to now search locally and externally. Adding the web to Sherlock was relatively easy as the plugins only needed to be written in a plain text file. Sherlock was included in every single Mac OS 8, 9 and 10 until 10.5.

Spotlight was released in 2005, on Mac OSX 10.4, is a Selection-based search which means the user invokes a query using only the mouse. It allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage. Spotlight also uses a built-in Oxford American Dictionary and calculator to offer quick access to definitions and small calculations.&lt;ref&gt;http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html&lt;/ref&gt; While Spotlight had a initially long start-up time (for first time set up). The entire hard disk was indexed, and as files are added to the hard disk, the index is constantly being updated in the background. This is done using minimal CPU &amp; RAM resources, making searching relatively easy and quick.

'''Linux'''

For Linux, we will primarily cover the Ubuntu distribution as it was and currently is still the most popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using Tracker&lt;ref&gt;http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/&lt;/ref&gt; desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX based systems. Tracker, was released in late 2007 was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting, and meta-data matching, but support for searching through emails and messages (instant messages) was added. Years later, in 2014 Recoll&lt;ref&gt;http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING&lt;/ref&gt; was added to Linux distributions, it works with other search programs such as Tracker and Beagle to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. The wonderful thing about Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example you could search for just file types or by content.&lt;ref&gt;http://archive09.linux.com/feature/114283&lt;/ref&gt;

==See also==
*[[List of search engines#Desktop search engines|List of desktop search engines]]

== References ==
&lt;!--* [http://ims.dei.unipd.it/members/agosti/teaching/2006-07/ir/ Maristella Agosti's website]--&gt;
{{reflist|2}}

== External links ==
* ''[http://www.slate.com/id/2111643/ Keeper Finders]'', by Paul Boutin, ''[[Slate (magazine)|Slate]]'', December 31, 2004 &amp;mdash; A comparison of Google, Ask Jeeves, HotBot, MSN and Copernic desktop search tools.
* [http://www.goebelgroup.com/desktopmatrix.htm GoebelGroup.com's desktop search tools comparison chart] - Date of last update: 15 January 2007.
* [http://labnol.blogspot.com/2004/10/detailed-comparison-of-desktop-search.html A detailed comparison of desktop search tools] - dated 2004.
* [http://www.wikinfo.org/index.php/Comparison_of_desktop_search_software Comparison of desktop search software] - Date of last update: March 2008
* [http://tbox.codeplex.com/ TBox] - DevTool, with ability to do fast search by text files

{{Navigationbox Desktopsearch}}

{{DEFAULTSORT:Desktop Search}}
[[Category:Desktop search engines| ]]
[[Category:Searching]]</text>
      <sha1>lj6qhqxqpvu3hzyyscnvcasvfpzq8sq</sha1>
    </revision>
  </page>
  <page>
    <title>Indexing Service</title>
    <ns>0</ns>
    <id>4047242</id>
    <revision>
      <id>591786166</id>
      <parentid>590070538</parentid>
      <timestamp>2014-01-21T22:57:19Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* References */Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated date parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6650">{{Use dmy dates|date=February 2011}}
{{Infobox Windows component
| name                = Indexing Service
| screenshot          = Indexing Service Query Form.PNG
| screenshot_size     = 300px
| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].
| type                = [[Desktop search]]
| service_name        = Indexing Service
| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.
| replaced_by         = [[Windows Search]]
| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]&lt;ref name=&quot;MIS-Intro&quot; /&gt;&lt;br/&gt;[[Windows 2000]]&lt;ref name=&quot;MIS-v3&quot; /&gt;&lt;br/&gt;[[Windows XP]]&lt;ref name=&quot;TnC-144&quot; /&gt;&lt;br/&gt;[[Windows Server 2003]]&lt;ref name=&quot;TnC-144&quot; /&gt;&lt;br/&gt;[[Windows Server 2008]]&lt;ref name=&quot;WIS-Install2008&quot; /&gt;
}}

'''Indexing Service''' (originally called '''Index server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by [[Windows Search]].

== History ==
Indexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]&lt;ref name=&quot;MIS-Intro&quot; /&gt; as well as [[Windows 2000]] and later.&lt;ref name=&quot;MIS-v3&quot; /&gt;&lt;ref name=&quot;TnC-144&quot; /&gt;&lt;ref name=&quot;WIS-What&quot; /&gt; The first incarnation of the indexing service was shipped in August 1996&lt;ref name=&quot;MIS-Intro&quot; /&gt; as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}

In [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.&lt;ref name=&quot;WIS-Install2008&quot; /&gt;

Indexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.&lt;ref&gt;{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}&lt;/ref&gt; It has been removed from [[Windows 8]].

== Search interfaces ==

Comprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.

Once the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.

Microsoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.&lt;ref&gt;{{cite web
 | url = http://support.microsoft.com/kb/319506
 | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)
 | work = Microsoft Support
 | publisher = 10 May 2002
 | accessdate = 1 February 2011
}}&lt;/ref&gt;

== References ==
{{Reflist|refs=
&lt;ref name = &quot;MIS-Intro&quot;&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx
  |title = Introduction to Microsoft Index Server
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date = 15 October 1997
  |accessdate = 1 February 2011
  |first1 = Krishna
  |last1 = Nareddy
  }}&lt;/ref&gt;
&lt;ref name = &quot;MIS-v3&quot;&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx
  |title = Indexing Service Version 3.0
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}&lt;/ref&gt;
&lt;ref name = &quot;WIS-What&quot;&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx
  |title = What is Indexing Service?
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}&lt;/ref&gt;
&lt;ref name=&quot;WIS-Install2008&quot;&gt;{{Cite web
  |url = http://support.microsoft.com/kb/954822
  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)
  |work = Microsoft Support
  |publisher = Microsoft Corporation
  |date = 3 May 2010
  |accessdate = 1 February 2011
  }}&lt;/ref&gt;
&lt;ref name=&quot;TnC-144&quot;&gt;{{Cite book
  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&amp;displaylang=en
  |format = Microsoft Word
  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP
  |edition = 2.0
  |publisher = Microsoft Corporation
  |page = 144
  |date=December 2005
  |first1 = Mike
  |last1  = Danseglio
  |first2 = Kurt
  |last2  = Dillard
  |first3 = José
  |last3  = Maldonado
  |first4 = Paul
  |last4  = Robichaux
  |editor1-first = Reid
  |editor1-last  = Bannecker
  |editor2-first = John
  |editor2-last  = Cobb
  |editor3-first = Jon
  |editor3-last  = Tobey
  |editor4-first = Steve
  |editor4-last  = Wacker
  }}&lt;/ref&gt;
}}

{{DEFAULTSORT:Indexing Service}}
[[Category:Windows communication and services]]
[[Category:Desktop search engines|Desktop search engines]]
[[Category:Searching]]
[[Category:Windows components]]</text>
      <sha1>p2t6joneyudz7j7qvhnc6jc2zpevrcj</sha1>
    </revision>
  </page>
  <page>
    <title>Unified Information Access</title>
    <ns>0</ns>
    <id>29478350</id>
    <revision>
      <id>641384815</id>
      <parentid>619461067</parentid>
      <timestamp>2015-01-07T09:57:19Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>Tagging using [[Project:AWB|AWB]] (10703)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3063">{{Multiple issues|
{{confusing|date=November 2010}}
{{cleanup|date=November 2010}}
}}

'''Unified Information Access Platforms''' are [[computing platforms]] that integrate large volumes of [[unstructured information|unstructured]], semi-structured, and structured information into a unified environment for processing, analysis and decision-making. These platforms are highly scalable, hybrid architectures that combine elements of database and search technologies in order to make information access dynamic and ad hoc, while offering the reporting and visualization features commonly found in business intelligence applications. While the vision for such integrated platforms has been around for years, only since 20XX have products been released into the market. Companies like [[Applied Relevance]], [[Attivio]], [[BA-Insight]], [[Cambridge Semantics]], [[Endeca Technologies Inc.|Endeca]], [[Exalead]], [[HP Autonomy]], [[PolySpot]], [[MarkLogic]], [[PerfectSearch]], [[Palantir Technologies|Palantir]], [[TopQuadrant]], [[Sinequa]] and [http://www.virtualworks.com VirtualWorks] have recognized the need for this approach.

Unified access applications:
*Create [[Hybrid computer|hybrid]] data structures that combine structured data and data operators with [[Text (literary theory)|text]] and semi-structured operations and analytics. They combine semantic understanding, fuzzy matching, sorting, joins, and various operations such as [[range searching]] within a single architecture, rather than federating a query to multiple sources in multiple forms.
*Leverage these hybrid structures to provide real-time access through ad hoc queries to multiple sources of information, including information across a spectrum of [[File format|format]]s (e.g. rich media) through a single [[Interface (computer science)|interface]].
*Handle sparse matrices of unpredictable content.
*Optimize interactions for consumption and decisions, [[Process (computing)|processing]] queries faster than traditional database and/or BI applications and implementing visual consumption metaphors.
*Scale to [[terabyte]]s.
*Provide reporting tools that are BI-like, or integrate easily with BI applications and reporting tools.

== References ==
* Worldwide Search and Discovery 2009 Vendor Shares and Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.
* Building the Intelligent Enterprise: The Case for Unified Access and Analytics
Susan Feldman, Jul 2009 - Doc # 219467   
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* Video: [http://www.attivio.com/poweringbusiness/videos/63-attivio/869-unified-information-access-in-4-minutes.html &quot;Unified Information Access in 4 Minutes&quot;]
* http://www.computerworld.com/s/article/9180280/Five_Advantages_of_Unified_Information_Access_UIA
* http://www.eweek.com/c/a/Enterprise-Applications/How-to-Use-Unified-Information-Access-to-Get-Most-Value-from-Your-Data/

[[Category:Searching]]</text>
      <sha1>ilwny9cpohszabsl3ekgq6ewcp8t839</sha1>
    </revision>
  </page>
  <page>
    <title>Concordance (publishing)</title>
    <ns>0</ns>
    <id>2937111</id>
    <revision>
      <id>644405195</id>
      <parentid>636823267</parentid>
      <timestamp>2015-01-27T14:10:18Z</timestamp>
      <contributor>
        <username>Anothernogginthenog</username>
        <id>23857423</id>
      </contributor>
      <minor/>
      <comment>Corrected punctuation.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8544">{{sister
|project=wiktionary
|text=See the [[Wiktionary:Wiktionary:Concordances|list of concordances]] in [[Wiktionary]], the free dictionary
}}

A '''concordance''' is an alphabetical list of the principal words used in a book or body of work, with their immediate [[context (language use)#Verbal context|context]]s.  Because of the time, difficulty, and expense involved in creating a concordance in the pre-[[computer]] era, only works of special importance, such as the [[Vedas]],&lt;ref&gt;{{cite book|first = Maurice | last = Bloomfield| authorlink = Maurice Bloomfield | title = A Vedic Concordance| year  = 1990| publisher = Motilal Banarsidass Publ| isbn = 81-208-0654-9}}&lt;/ref&gt; [[Bible]], [[Qur'an]] or the works of [[William Shakespeare|Shakespeare]] or classical Latin and Greek authors,&lt;ref&gt;{{cite journal | first = Roy | last = Wisby | authorlink = Roy Wisby | title = Concordance Making by Electronic Computer: Some Experiences with the Wiener Genesis| journal = The Modern Language Review | publisher = Modern Humanities Research Association | volume = 57 | issue = 2 | pages = 161–172 | date = April 1962 | doi=10.2307/3720960}}&lt;/ref&gt; had concordances prepared for them. 
[[File:Mordechai nathan hebrew latin concordance.jpg|right|thumb|225px|Mordecai Nathan's Hebrew-Latin Concordance of the Bible]]
A concordance is more than an index; additional material, such as commentary, definitions, and topical cross-indexing make producing them a labor-intensive process, even when assisted by computers.

Although an automatically generated [[subject indexing|index]] lacks the richness of a published concordance, the ability to combine the result of queries concerning multiple terms (such as searching for words near other words) has reduced interest in concordance publishing.  In addition, mathematical techniques such as [[Latent Semantic Indexing]] have been proposed as a means of automatically identifying linguistic information based on word context.

A '''bilingual concordance''' is a concordance based on [[aligned parallel text]].

A '''topical concordance''' is a list of subjects that a book (usually The Bible) covers, with the immediate context of the coverage of those subjects. Unlike a traditional concordance, the indexed word does not have to appear in the verse. The most well known topical concordance is [[Nave's Topical Bible]].

The first concordance, to the [[Vulgate]] Bible, was compiled by [[Hugh of St Cher]] (d.1262), who employed 500 monks to assist him. In 1448 Rabbi Mordecai Nathan completed a concordance to the Hebrew Bible. It took him ten years. 1599 saw a concordance to the Greek New Testament published by Henry Stephens and the Septuagint was done a couple of years later by Conrad Kircher in 1602. The first concordance to the English bible was published in 1550 by Mr Marbeck. According to Cruden it did not employ the verse numbers devised by Robert Stephens in 1545 but &quot;the pretty large concordance&quot; of Mr Cotton did. Then followed [[Cruden's Concordance]] and [[Strong's Concordance]].

==Use in linguistics==
Concordances are frequently used in [[linguistics]], when studying a text. For example:    
* comparing different usages of the same word
* analysing keywords
* analysing [[word frequencies]]
* finding and analysing phrases and [[idioms]]
* finding [[translation]]s of subsentential elements, e.g. [[terminology]], in [[Bitext#Bitexts and translation memories|bitexts and translation memories]]
* creating indexes and word lists (also useful for publishing)

Concordancing techniques are widely used in national corpora such as [[American National Corpus]], [[British National Corpus]], and [[Corpus of Contemporary American English]] available on-line.  Stand-alone applications that employ concordancing techniques are known as concordancers.&lt;ref&gt;[http://www.lexically.net/wordsmith/introduction.htm?gclid=COjFnvGKhakCFVJX4Qod-RqjjQ Introduction to WordSmith]&lt;/ref&gt; Some of them have integrated part-of-speech taggers and enable the user to create his/her own pos-annotated corpora to conduct various type of searches adopted in corpus linguistics.&lt;ref&gt;[http://yatsko.zohosites.com/linguistic-toobox-a-concordancer.html Linguistic Toolbox]&lt;/ref&gt;

==Inversion==

The reconstruction of the text of some of the [[Dead Sea Scrolls]] involved a concordance.

Access to some of the scrolls was governed by a &quot;secrecy rule&quot; that allowed only the original International Team or their designates to view the original materials. After the death of [[Roland de Vaux]] in 1971, his successors repeatedly refused to even allow the publication of photographs to other scholars. This restriction was circumvented by [[Martin Abegg]] in 1991, who used a computer to &quot;invert&quot; a concordance of the missing documents made in the 1950s which had come into the hands of scholars outside of the International Team, to obtain an approximate reconstruction of the original text of 17 of the documents.&lt;ref&gt;{{cite web |last= Hawrysch |first= George |title= Dr. George Hawrysch's speech on concordance book launch |work= The Ukrainian Weekly, No. 31, Vol. LXX |publisher= Ukrainian National Association |date= 2002-08-04 |url= http://www.ukrweekly.com/old/archive/2002/310217.shtml |accessdate= 2008-06-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web |last= Jillette |first= Penn |title= You May Already be a &quot;Computer Expert&quot; |url= http://pennandteller.com/sincity/penn-n-teller/pcc/deadsea.html |accessdate= 2008-06-14}}&lt;/ref&gt; This was soon followed by the release of the original text of the scrolls.

== See also ==
* [[Back-of-the-book index]]
* [[A Vedic Word Concordance]]
* [[Bible concordance]]
* [[Bitext]]
* [[Concordancer]]
* [[Cross-reference]]
* [[Index (publishing)|Index]]
* [[Key Word in Context|KWIC]]
* [[Text mining]]

== References ==
{{Reflist}}

== External links ==
* [http://www.opensourceshakespeare.org/concordance/ Shakespeare concordance] - A concordance of Shakespeare's complete works (from Open Source Shakespeare)
* [http://www.arts.ualberta.ca/~ukr/skovoroda/NEW/ Online Concordance to the Complete Works of Hryhorii Skovoroda] - A concordance to Hryhorii Skovoroda's complete works (University of Alberta, Edmonton, Canada)
* [http://infomotions.com/alex/ Alex Catalogue of Electronic Texts] - The Alex Catalogue is a collection of public domain electronic texts from American and English literature as well as Western philosophy. Each of the 14,000 items in the Catalogue are available as full-text but they are also complete with a concordance. Consequently, you are able to count the number of times a particular word is used in a text or list the most common (10, 25, 50, etc.) words.
* [http://victorian.lang.nagoya-u.ac.jp/concordance/ Hyper-Concordance] - The Hyper-Concordance is written in C++, a program that scans and displays lines based on a command entered by the user. The main advantage of the C++ program is that it not only identifies the concordance lines but the words occurring to the left and the right of the word or phrase searched. It also reports the total number of text lines, the total word count and the number of occurrences of the word or phrase searched. The full text of the book is displayed in a box at the bottom of the screen. Each line of the text is numbered, and the line number and the term(s) searched provide a link to the full text.
* [http://cherry.conncoll.edu/cohar/Programs.htm Concord] - Page includes link to Concord, an on-the-fly KWIC concordance generator.  Works with at least some non-Latin scripts (modern Greek, for instance).  Multiple choices for sorting results; multi-platform; Open Source.
* [http://buschmeier.org/bh/study/ccd/ ConcorDance] - A concordance interface to the WorldWideWeb, it uses Google's or Yahoo's search engine to find concordances and can be used directly from the browser.
* [http://ctext.org/tools/concordance Chinese Text Project Concordance Tool] - Concordance lookup and discussion of the continued importance of printed concordances in [[Sinology]] - [[Chinese Text Project]]
* [http://khc.sourceforge.net/en/ KH Coder] - A free software for KWIC concordance and collocation stats generation. Various statistical analysis functions are also available such as co-occurrence network, multidimensional scaling, hierarchical cluster analysis, and correspondence analysis of words.

{{DEFAULTSORT:Concordance (Publishing)}}
[[Category:Concordances (publishing)| ]]
[[Category:Indexing]]
[[Category:Searching]]
[[Category:Library science]]
[[Category:Information science]]
[[Category:Reference works]]</text>
      <sha1>lv0zcsqo0dchiuw13wa70lextmp8ql6</sha1>
    </revision>
  </page>
  <page>
    <title>Social search</title>
    <ns>0</ns>
    <id>6843345</id>
    <revision>
      <id>638831762</id>
      <parentid>638138160</parentid>
      <timestamp>2014-12-19T21:58:02Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>fixed [[Help:CS1 errors#bad_date|CS1 errors: dates]] to meet [[MOS:DATEFORMAT]] (also [[WP:AWB/GF|General fixes]]) using [[Project:AWB|AWB]] (10518)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14869">'''Social search''' or a '''social search engine''' is a type of [[web search]] that takes into account the [[Social Graph]] of the person initiating the search query. When applied to web searches the Social-Graph uses established algorithmic or machine-based approaches where relevance is determined by analyzing the text of each document or the link structure of the documents.&lt;ref&gt;[http://searchenginewatch.com/showPage.html?page=3623153 What's the Big Deal With Social Search?], SearchEngineWatch, Aug 15, 2006&lt;/ref&gt; Search results produced by '''social search engine''' give more visibility to content created or &quot;touched&quot; by users in the Social Graph.

Social search takes many forms, ranging from simple [[social bookmarking|shared bookmarks]] or [[Tag (metadata)|tagging]] of content with descriptive labels to more sophisticated approaches that combine human intelligence with computer [[algorithm]]s.&lt;ref&gt;[http://www2.computer.org/portal/web/csdl/doi/10.1109/MC.2009.87 Chi, Ed H. Information Seeking Can Be Social, Computer, vol. 42, no. 3, pp. 42-46, Mar. 2009, ] {{doi|10.1109/MC.2009.87}}&lt;/ref&gt;&lt;ref&gt;[http://blog.delver.com/index.php/2008/07/31/taxonomy-of-social-search-approaches/ A Taxonomy of Social Search Approaches], Delver company blog, Jul 31, 2008&lt;/ref&gt;
&lt;ref&gt;[http://www.springerlink.com/content/e12233858017h042/ Longo, Luca et al., Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time. Transactions on Computational Collective Intelligence II, Lecture Notes in Computer Science, Volume 6450. ISBN 978-3-642-17154-3. Springer Berlin Heidelberg, 2010, p. 46 ] {{doi|10.1007/978-3-642-17155-0_3}}&lt;/ref&gt;&lt;ref&gt;[http://www.springerlink.com/content/gg3p6177pw6h10j8/ Longo, Luca et al., Information Foraging Theory as a Form of Collective Intelligence for Social Search. Computational Collective Intelligence. Semantic  Web, Social Networks and  Multiagent Systems Lecture Notes in Computer Science, 2009, Volume 5796/2009, 63-74] {{doi|10.1007/978-3-642-04441-0_5}}&lt;/ref&gt;

The search experience takes into account varying sources of metadata, such as collaborative discovery of web pages, tags, social ranking, commenting on bookmarks, news, images, videos, knowledge sharing, podcasts and other web pages. Example forms of user input include social bookmarking or direct interaction with the search results such as promoting or demoting results the user feels are more or less relevant to their query.&lt;ref&gt;[http://venturebeat.com/2008/01/31/googles-marissa-mayer-social-search-is-the-future Google’s Marissa Mayer: Social search is the future], VentureBeat, Jan 31, 2008&lt;/ref&gt;

==History==

The term social search began to emerge between 2004 and 2005. The concept of social ranking can be considered to derive from Google's [[PageRank]] algorithm,{{citation needed|date=March 2009}} which assigns importance to web pages based on analysis of the link structure of the web, because PageRank is relying on the collective judgment of webmasters linking to other content on the web. Links, in essence, are positive votes by the webmaster community for their favorite sites.

In 2008, there were a few startup companies that focused on ranking search results according to one's [[social graph]] on [[social networks]].&lt;ref&gt;[http://online.wsj.com/public/article/SB121063460767286631.html New Sites Make It Easier To Spy on Your Friends], Wall Street Journal, May 13. 2008&lt;/ref&gt;&lt;ref&gt;[http://mashable.com/2007/08/27/social-search/ Social Search Guide: 40+ Social Search Engines], Mashable, Aug 27. 2007&lt;/ref&gt; Companies in the social search space include  Evam-SOCOTO Wajam, Slangwho, [[Sproose]], [[Mahalo.com|Mahalo]], [[Jumper 2.0]], [[Qitera]], [[Scour Inc.|Scour]], [[Wink Technologies|Wink]], [[Eurekster]], [[Baynote]], [[Delver (Social Search)|Delver]], and OneRiot. Former efforts include [[Wikia Search]]. In 2008, a story on ''[[TechCrunch]]'' showed [[Google]] potentially adding in a voting mechanism to search results similar to [[Digg]]'s methodology.&lt;ref&gt;[http://www.techcrunch.com/2008/07/16/is-this-the-future-of-search/ Is This The Future Of Search?], TechCrunch, July 16, 2008&lt;/ref&gt; This suggests growing interest in how social groups can influence and potentially enhance the ability of algorithms to find meaningful data for end users. There are also other services like Sentiment that turn search personal by searching within the users' social circles.

In October 2009, [[Google]] rolled out its &quot;Social Search&quot; feature; after a time in [[beta]], the feature was expanded to multiple languages in May 2011. Before the expansion however in 2010 [[Bing]] and [[Google]] were already taking into account re-tweets and Likes when providing search results.&lt;ref&gt;{{cite web|url = http://www.marchpr.com/blog/2013/04/seo-social-media-search/|title = Retweets and Likes influencing search results|date = 10 April 2013|accessdate = 1 December 2014| publisher = March Communications}}&lt;/ref&gt; However, after a search deal with Twitter ended without renewal, Google began to retool its Social Search. In January 2012, Google released &quot;Search plus Your World&quot;, a further development of Social Search. The feature, which is integrated into Google's regular search as an opt-out feature, pulls references to results from [[Google+]] profiles. The goal was to deliver better, more relevant and personalized search results with this integration. This integration however had some problems in which [[Google+]] still isn't wildly adopted or has much usage among many users.&lt;ref name=&quot;HubSpot&quot;&gt;{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = Facebook Announces New Social Search Feature|date = 15 January 2013|accessdate = 1 December 2014| publisher = HubSpot}}&lt;/ref&gt;

In January 2013, [[Facebook]] announced a new search engine called [[Graph Search]] still in the beta stages. The goal in mind was to accomplish what [[Google]] failed at, skipping the results that are popular to the internet, in favor of the results that are popular within your social circle. Unlike [[Google]], [[Facebook]]'s Graph search differed in two large areas, first, people use Facebook frequently. This allows [[Facebook]] to use all it's user generated content that is uploaded everyday to improve the [[Facebook]] search experience.&lt;ref name=&quot;HubSpot&quot;/&gt; Secondly, [[Facebook]] did not incorporate Google into Facebook search, instead Graph Search is powered by [[Bing]].This allows [[Bing]] results to show when Facebook's Graph Search can't find a match.&lt;ref&gt;{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search powered by Bing|date = 15 January 2013|accessdate = 1 December 2014| publisher = Forbes}}&lt;/ref&gt;

==Concerns==

When Google announced &quot;Search plus Your World&quot; the reaction was mixed among tech companies. The company was subsequently criticized by [[Twitter]] for the perceived potential impact of &quot;Search plus Your World&quot; upon web publishers, describing the feature's release to the public as a &quot;bad day for the web&quot;, while Google replied that Twitter refused to allow deep search crawling by Google of Twitter's content.&lt;ref&gt;{{cite web|url = http://www.cnbc.com/id/100381337#.|title = Twitter unhappy about Google's social search changes|date = 11 January 2012|accessdate = 11 January 2012|publisher = BBC News}}&lt;/ref&gt; The criticism from [[Twitter]] wasn't without merits however, by [[Google]] integrating [[Google+]], they were essentially forcing people to switch from a social network on to theirs in order to improve search results. One famous example occurred when [[Google]] showed a link to Mark Zuckerberg's dormant [[Google+]] account rather than the active [[Facebook]] profile.&lt;ref name=&quot;Google pushing Google&quot;&gt;{{cite web|url = http://searchengineland.com/googles-knowledge-graph-finally-shows-social-networks-named-google-209171.|title = Google pushing Google+|date = 18 November 2014|accessdate = 1 December 2012|publisher = Third Door Media}}&lt;/ref&gt; Further more this affected businesses in which if they do not have time to leverage all other social media sites, they knew they should use [[Google+]] to maximize their efforts since the data shows it impacts rankings more than [[Twitter]] and [[Facebook]].&lt;ref&gt;{{cite web|url = http://www.quicksprout.com/2014/01/31/how-social-signals-impact-search-engine-rankings/#.|title = Google+ impacts ranking more|date = 31 January 2014|accessdate = 1 December 2014|publisher = Quick Sprout}}&lt;/ref&gt; in November 2014 these accusations started to die down because Google's Knowledge Graph started to finally show links to [[Facebook]], [[Twitter]], and other social media sites.&lt;ref name=&quot;Google pushing Google&quot;/&gt;

[[Google]] was not the only one that garnished concerns over social search. After the introduction of [[Graph Search]] by [[Facebook]] many pointed out how [[Graph Search]] showed private information that isn't in web search.&lt;ref&gt;{{cite web|url = http://www.forbes.com/sites/tomiogeron/2013/01/15/live-facebook-announces-graph-search/|title = Graph Search results|date = 1 January 2013|accessdate = 1 December 2014|publisher = Forbes}}&lt;/ref&gt; Information that was once obscure is now easier to dig up, which is why Facebook urges users to monitor post and pictures users are tagged in and filter and filter any content that users would not want to make public.&lt;ref&gt;{{cite web|url = http://www.forbes.com/sites/larrymagid/2013/01/15/facebooks-new-social-search-what-it-is-and-how-it-affects-your-privacy/|title = Graph Search Privacy Concerns|date = 15 January 2013|accessdate = 1 December 2014|publisher = Forbes}}&lt;/ref&gt;

This in large points towards the biggest concern toward social search which is that social media networks don't have a vested interest in working with search engines. [[LinkedIn]] for example has taken steps to improve its own individual search functions in order to stray users from external search engines. Even [[Microsoft]] started working with [[Twitter]] in order to integrate some tweets into [[Bing]]'s search results in November 2013. Yet [[Twitter]] has its own search engine which points out how much value their data has and why they'd like to keep it in house.&lt;ref&gt;{{cite web|url = http://venturebeat.com/2014/06/30/microsoft-and-twitter-make-bing-a-better-social-search-engine/|title = Bing's twitter integration|date = 30 June 2014|accessdate = 1 December 2014|publisher = Venture Beat}}&lt;/ref&gt; In the end though social search will never be truly comprehensive of the subjects that matter to people unless users opt to be completely public with their information.&lt;ref&gt;{{cite web|url = https://blog.hubspot.com/blog/tabid/6307/bid/34058/Facebook-Announces-New-Social-Search-Feature-Called-Graph-Search.aspx|title = User data will never be competently public|date = 15 January 2013|accessdate = 1 December 2014|publisher = HubSpot}}&lt;/ref&gt;

==Social discovery==
Social discovery is the use of social preferences and personal information to predict what content will be desirable to the user.&lt;ref name=&quot;Bailyn2012&quot;&gt;{{cite book|last=Bailyn|first=Evan|title=Outsmarting Social Media: Profiting in the Age of Friendship Marketing|url=http://books.google.com/books?id=M97RiODwKHEC&amp;pg=PT51|accessdate=20 January 2014|date=2012-04-12|publisher=Que Publishing|isbn=9780132861403|pages=51–}}&lt;/ref&gt; Technology is used to discover new people and sometimes new experiences shopping, meeting friends or even traveling.&lt;ref&gt;{{cite web|last=Burke|first=Amy|url=http://mashable.com/2013/07/08/social-discovery-apps/|publisher=Mashable|title=Are Social Discovery Apps Too Creepy?}}&lt;/ref&gt;  The discovery of new people is often in real-time, enabled by [[mobile apps]]. However, social discovery is not limited to meeting people in real-time, it also leads to sales and revenue for companies via social media.&lt;ref&gt;{{cite web|last=Cubie|first=Gregor|url=http://www.thedrum.com/news/2013/10/02/social-discovery-sites-influence-retail-expanding-rakutens-playcom-numbers-find|publisher=The Drum|title=Social Discovery sites' influence on retail expanding}}&lt;/ref&gt;  An example of retail would be the addition of social sharing with music, through the iTunes music store. There is a social component to discovering new music &lt;ref&gt;{{cite web|last=Constine|first=Josh|url=http://techcrunch.com/2013/09/10/bitcovery/|publisher=TechCrunch|title=Bitcovery Brings A Desperately Needed Social Discovery Layer To The iTunes Store}}&lt;/ref&gt; Social discovery is at the basis of [[Facebook]]'s profitability, generating ad revenue by targeting the ads to users using the social connections to enhance the commercial appeal.&lt;ref name=&quot;Bailyn2012&quot;/&gt;

==Developments==

[[Google]] may be falling behind in terms of social search, but in reality they see the potential and importance of this technology with [[Web 3.0]] and [[web semantics]]. The importance of social media lies within how Semantic search works. Semantic search understands much more, including where you are, the time of day, your past history, and many other factors including social connections, and social signals. The first step in order to achieve this will be to teach algorithms to understand the relationship between things.&lt;ref&gt;{{cite web|url = http://www.socialmediatoday.com/content/google-semantic-search|title = Google Semantic Search|date = 28 February 2014|accessdate = 1 December 2014|publisher = Social Media Today}}&lt;/ref&gt;

However this is not possible unless social media sites decide to work with search engines, which is difficult since everyone would like to be the main toll bridge to the internet. As we continue on, and more articles are referred by social media sites, the main concern becomes what good is a search engine without the data of users.

One development that seeks to redefine search is the combination of [[distributed search]] with social search. The goal is a basic search service whose operation is controlled and maintained by the community itself. This would largely work like Peer to Peer networks in which users provide the data they seems appropriate. Since the data used by search engines belongs to the user they should have absolute control over it. The infrastructure required for a search engine is already available in the from of thousands of idle desktops and extensive residential broadband access.&lt;ref&gt;{{cite web|url = http://www2009.eprints.org/242/|title = Towards Distributed Social Search Engines|accessdate = 1 December 2014|publisher = EPrints}}&lt;/ref&gt;

== See also ==
* [[Collaborative filtering]]
* [[Enterprise bookmarking]]
* [[Human search engine]]
* [[Relevance feedback]]
* [[Social software]]

== References ==
{{reflist}}
{{Internet search}}

[[Category:Searching]]
[[Category:Social search| ]]
[[Category:Social software|Search]]</text>
      <sha1>cqdjwe09k01zc1aprce4na4fnxp4f7x</sha1>
    </revision>
  </page>
  <page>
    <title>Web indexing</title>
    <ns>0</ns>
    <id>33914</id>
    <revision>
      <id>640360908</id>
      <parentid>640360850</parentid>
      <timestamp>2014-12-31T09:23:20Z</timestamp>
      <contributor>
        <username>Dai Pritchard</username>
        <id>21757787</id>
      </contributor>
      <comment>/* Further reading */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3942">{{no footnotes|date=December 2014}}
'''Web indexing''' (or '''Internet indexing''') refers to various methods for indexing the contents of a [[website]] or of the [[Internet]] as a whole. Individual websites or [[intranet]]s may use a [[back-of-the-book index]], while [[search engines]] usually use keywords and [[Metadata (computing)|metadata]] to provide a more useful vocabulary for Internet or onsite searching. With the increase in the number of [[periodical]]s that have articles online, web indexing is also becoming important for periodical websites.

Back-of-the-book-style web indexes may be called &quot;web site A-Z indexes&quot;. The implication with &quot;A-Z&quot; is that there is an alphabetical browse view or interface. This interface differs from that of a browse through layers of hierarchical categories (also known as a [[Taxonomy (general)|taxonomy]]) which are not necessarily alphabetical, but are also found on some web sites. Although an A-Z index could be used to index multiple sites, rather than the multiple pages of a single site, this is unusual.

Metadata web indexing involves assigning keywords or phrases to web pages or web sites within a [[metadata tag]] (or &quot;meta-tag&quot;) field, so that the web page or web site can be retrieved with a search engine that is customized to search the keywords field. This may or may not involve using keywords restricted to a controlled vocabulary list. This method is commonly used by [[search engine indexing]].

==See also==
* [[Information architecture]]
* [[Search engine indexing]]
* [[Search engine optimization]]
* [[Site map]]
* [[Web navigation]]
* [[Web search engine]]

==References==
{{reflist}}

==External links==
&lt;!--========================({{No More Links}})============================
    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA  |
    | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |
    |                                                                     |
    |           Excessive or inappropriate links WILL BE DELETED.         |
    | See [[Wikipedia:External links]] &amp; [[Wikipedia:Spam]] for details.  |
    |                                                                     |
    | If there are already plentiful links, please propose additions or   |
    | replacements on this article's discussion page, or submit your link |
    | to the relevant category at the Open Directory Project (dmoz.org)   |
    | and link back to that category using the {{dmoz}} template.         |
    =======================({{No More Links}})=============================--&gt;
*TheAlphaWeb [http://www.eprodoffice.com/shhh/abcdefghijklmnopqrstuvwxyz.htm ''An example of an Internet A-Z'']
*Glenda Browne and Jonathan Jermey, [http://www.webindexing.biz/ ''Website indexing: enhancing access to information within websites, 2nd Edition''], ISBN 1-875145-56-7
*James Lamb, [http://www.jalamb.com/publications.html ''Website Indexes: visitors to content in two clicks, or website indexing with XRefHT32 freeware''], ISBN 978-1-4116-7937-5
*[http://www.infotoday.com/books/books/BeyondBookIndex.shtml ''Beyond Book Indexing: How to Get Started in Web Indexing, Embedded Indexing, and Other Computer-Based Media''], edited by Marilyn Rowland and Diane Brenner, American Society of Indexers, Info Today, Inc, NJ, 2000, ISBN 1-57387-081-1
* {{Cite web
  |url=http://www.boxesandarrows.com/view/improving_usability_with_a_website_index
  |title=Improving Usability with a Website Index
  |archiveurl=http://www.webcitation.org/5vJwZDVkj
  |archivedate=2010-12-28
  |accessdate=2010-12-28
  |first=Fred
  |last=Leise
  |date=2002-07-15
}}
* [http://www.web-indexing.org/article-brown.htm Why Create an Index?]
* [http://www.theeasybee.com/directory/web-content-extraction Open Social Web 3.0 Directory] – Compare and review web indexing programs
{{Internet search}}
[[Category:Searching]]
[[Category:Indexing]]


{{internet-stub}}</text>
      <sha1>i32g8nkkbvtt5wz1jeg1dhjy3zb8w5b</sha1>
    </revision>
  </page>
  <page>
    <title>Search-based software engineering</title>
    <ns>0</ns>
    <id>11220518</id>
    <revision>
      <id>644445265</id>
      <parentid>642219041</parentid>
      <timestamp>2015-01-27T19:36:52Z</timestamp>
      <contributor>
        <username>Anothernogginthenog</username>
        <id>23857423</id>
      </contributor>
      <minor/>
      <comment>altered 'asks' to 'tasks' - presumably a typo. If 'asks' was intended, not good English.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18085">{{copy edit|date=October 2013}}

{{Use dmy dates|date=November 2011}}

'''Search-based software engineering''' ('''SBSE''') is an approach to apply [[metaheuristic]] search techniques like [[genetic algorithms]], [[simulated annealing]] and [[tabu search]] to [[software engineering]] problems. It is inspired by the observation that many activities in [[software engineering]] can be formulated as [[Optimization (mathematics)|optimization]] problems. Due to the [[computational complexity]] of these problems, exact [[Optimization (mathematics)|optimization]] techniques of [[operations research]] like [[linear programming]] or [[dynamic programming]] are mostly impractical for large scale [[software engineering]] problems. Because of this, researchers and practitioners have used [[metaheuristic]] search techniques to find near optimal or good-enough solutions.

Broadly speaking SBSE problems can be divided into two types. The first are black-box optimization problems, for example, assigning people to tasks (a typical [[combinatorial optimization]] problem). 
With this sort of problem domain, the underlying problem could have come from the software industry, but equally it could have originated from any domain where people are assigned tasks. 
The second type are white-box problems where operations on source code need to be considered.&lt;ref&gt;
{{Cite conference
| doi = 10.1109/SCAM.2010.28
| conference = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)
| pages = 7–19
| last = Harman
| first = Mark
| title = Why Source Code Analysis and Manipulation Will Always be Important
| booktitle = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)
| year = 2010
}}&lt;/ref&gt;

__TOC__

==Definition==

The basic idea of SBSE is to take a software engineering problem and convert it into a computational search problem which can be tackled with a [[metaheuristic]]. 
This essentially involves a number of stages. Firstly defining a search space (the set of possible solutions to the problem). 
This space is typically too large to be explored exhaustively and therefore a  [[metaheuristic]] is employed to sample this space. 
Secondly, a metric &lt;ref&gt;
{{Cite conference
| doi = 10.1109/METRIC.2004.1357891
| conference = 10th International Symposium on Software Metrics, 2004
| pages = 58–69
| last = Harman
| first = Mark
|author2=John A. Clark
 | title = Metrics are fitness functions too
| booktitle = Proceedings of the 10th International Symposium on Software Metrics, 2004 
| year = 2004
}}&lt;/ref&gt; (also called a fitness function, cost function, objective function or quality measure) is used to measure the quality of a potential solution. Many software engineering problems can be reformulated as a computational search problem.&lt;ref&gt;{{Cite journal
| doi = 10.1049/ip-sen:20030559
| issn = 1462-5970
| volume = 150
| issue = 3
| pages = 161–175
| last = Clark
| first = John A.
| coauthors = Dolado, José Javier; Harman, Mark; Hierons, Robert M.; Jones, Bryan F.; Lumkin, M.; Mitchell, Brian S.; Mancoridis, Spiros; Rees, K.; Roper, Marc; Shepperd, Martin J.
| title = Reformulating software engineering as a search problem
| journal = IEE Proceedings - Software 
| year = 2003
}}&lt;/ref&gt;

The term &quot;[[search-based application]]&quot;, in contrast, refers to using [[search engine technology]], rather than search techniques, in another industrial application.

==Brief history==

One of the earliest attempts in applying [[Optimization (mathematics)|optimization]] to a [[software engineering]] problem was reported by [[Webb Miller]] and David Spooner in 1976 in the area of software testing.&lt;ref&gt;
{{Cite journal
| doi = 10.1109/TSE.1976.233818
| issn = 0098-5589
| volume = SE-2
| issue = 3
| pages = 223–226
| last = Miller
| first = Webb
| last2 = Spooner
| first2 = David L. 
| title = Automatic Generation of Floating-Point Test Data
| journal = IEEE Transactions on Software Engineering
| year = 1976
}}&lt;/ref&gt; 
In 1992, Xanthakis and his colleagues applied a search technique to a [[software engineering]] problem for the first time.&lt;ref&gt;S. Xanthakis, C. Ellis, C. Skourlas, A. Le Gall, S. Katsikas and K. Karapoulios, &quot;Application of genetic algorithms to software testing,&quot; in ''Proceedings of the 5th International Conference on Software Engineering and its Applications'', Toulouse, France, 1992, pp.&amp;nbsp;625–636&lt;/ref&gt; 
The term SBSE was first used in 2001 by [[Mark Harman (computer scientist)|Harman]] and Jones.&lt;ref&gt;
{{Cite journal
| doi = 10.1016/S0950-5849(01)00189-6
| issn = 0950-5849
| volume = 43
| issue = 14
| pages = 833–839
| last = Harman
| first = Mark
| last2 = Jones
| first2 = Bryan F.
| title = Search-based software engineering
| journal = Information and Software Technology
| accessdate = 2013-10-31
| date = 2001-12-15
| url = http://www.sciencedirect.com/science/article/pii/S0950584901001896
}}&lt;/ref&gt; Since then, the research community has grown to include more than 800 authors in 2013, from approximately 270 institutions in 40 countries.{{Citation needed|date=October 2013}}

==Application areas==

Search-based software engineering is applicable to almost all phases of the [[software life cycle|software development process]]. [[Software testing]] has been one of the major applications of search techniques in [[software engineering]].&lt;ref&gt;
{{Cite journal
| doi = 10.1002/stvr.294
| issn = 1099-1689
| volume = 14
| issue = 2
| pages = 105–156
| last = McMinn
| first = Phil
| title = Search-based software test data generation: a survey
| journal = Software Testing, Verification and Reliability
| accessdate = 2013-10-31
| year = 2004
| url = http://onlinelibrary.wiley.com/doi/10.1002/stvr.294/abstract
}}&lt;/ref&gt; Search techniques have also been applied to other [[software engineering]] activities, for instance, [[requirements analysis]],&lt;ref&gt;
{{Cite journal
| doi = 10.1016/j.infsof.2003.07.002
| issn = 0950-5849
| volume = 46
| issue = 4
| pages = 243–253
| last = Greer
| first = Des
| last2 = Ruhe
| first2 = Guenther
| title = Software release planning: an evolutionary and iterative approach
| journal = Information and Software Technology
| accessdate = 2013-09-06
| date = 2004-03-15
| url = http://www.sciencedirect.com/science/article/pii/S095058490300140X
}}&lt;/ref&gt;
&lt;ref&gt;{{Cite conference
| doi = 10.1109/SBES.2009.23
| conference = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09
| pages = 207–215
| last = Colares
| first = Felipe
| last2 = Souza
| first2 = Jerffeson
| last3 = Carmo
| first3 = Raphael
| last4 = Pádua
| first4 = Clarindo
| last5 = Mateus
| first5 = Geraldo R.
| title = A New Approach to the Software Release Planning
| booktitle = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09
| year = 2009
}}&lt;/ref&gt; [[software design]],&lt;ref&gt;
{{Cite journal
| doi = 10.1016/S0950-5849(01)00195-1
| issn = 0950-5849
| volume = 43
| issue = 14
| pages = 891–904
| last = Clark
| first = John A.
| last2 = Jacob
| first2 = Jeremy L. 
| title = Protocols are programs too: the meta-heuristic search for security protocols
| journal = Information and Software Technology
| accessdate = 2013-10-31
| date = 2001-12-15
| url = http://www.sciencedirect.com/science/article/pii/S0950584901001951
}}&lt;/ref&gt; [[software development]],&lt;ref&gt;
{{Cite journal
| doi = 10.1016/j.ins.2006.12.020
| issn = 0020-0255
| volume = 177
| issue = 11
| pages = 2380–2401
| last = Alba
| first = Enrique
| last2 = Chicano
| first2 = J. Francisco 
| title = Software project management with GAs
| journal = Information Sciences
| accessdate = 2013-10-31
| date = 2007-06-01
| url = http://www.sciencedirect.com/science/article/pii/S0020025507000175
}}&lt;/ref&gt; and [[software maintenance]].&lt;ref&gt;
{{Cite conference
| doi = 10.1109/ICSM.2005.79
| conference = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05
| pages = 240–249
| last = Antoniol
| first = Giuliano
| last2 = Di Penta
| first2 = Massimiliano 
| last3 = Harman
| first3 = Mark
| title = Search-based techniques applied to optimization of project planning for a massive maintenance project
| booktitle = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05
| year = 2005
}}&lt;/ref&gt;

===Requirements engineering===

[[Requirements engineering]] is the process by which the needs of a software's users and environment are determined and managed. Search-based methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches users' requests and different constraints such as limited resources and interdependencies between requirements. This problem is often tackled as a [[MCDM|multiple-criteria decision-making]] problem and, generally speaking, involves presenting the decision maker with a range of good compromises between cost and user satisfaction.&lt;ref&gt;
{{Cite thesis
| type = Ph.D.
| publisher = University of London
| last = Zhang
| first = Yuanyuan
| title = Multi-Objective Search-based Requirements Selection and Optimisation
| location = Strand, London, UK
| date = February 2010
| url = http://eprints.ucl.ac.uk/170695/
}}&lt;/ref&gt;
&lt;ref&gt;
Y.&amp;nbsp;Zhang and M.&amp;nbsp;Harman and S.&amp;nbsp;L.&amp;nbsp;Lim, &quot;[http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/images/Research_Student_Information/RN_11_12.pdf Search Based Optimization of Requirements Interaction Management],&quot; Department of Computer Science, University College London, Research Note RN/11/12, 2011.
&lt;/ref&gt;

===Debugging and maintenance===

Identifying a [[software bug]] (or a [[code smell]]) and then [[debugging]] (or [[refactoring]]) the software is largely a manual and labor-intensive endeavor, though the process is supported by a number of tools. One objective of SBSE is to automatically identify bugs (for example via [[mutation testing]]), then automatically fix them.

[[Genetic programming]], a biologically-inspired technique which involves evolving programs through the use of crossover and mutation, has been used to search for repairs to programs by altering a few lines of source code. The [http://dijkstra.cs.virginia.edu/genprog/ GenProg Evolutionary Program Repair] software was shown to be able to repair 55 out of 105 bugs for approximately $8 each.&lt;ref&gt;{{Cite conference
| doi = 10.1109/ICSE.2012.6227211
| conference = 2012 34th International Conference on Software Engineering (ICSE)
| pages = 3–13
| last = Le Goues
| first = Claire
| last2 = Dewey-Vogt
| first2 = Michael
| last3 = Forrest
| first3 = Stephanie
| last4 = Weimer
| first4 = Westley 
| title = A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each
| booktitle = 2012 34th International Conference on Software Engineering (ICSE)
| year = 2012
}}&lt;/ref&gt;

[[Coevolution]] has also been used as an approach. It follows a predator and prey metaphor where a population of programs and a population of [[Unit Testing|unit tests]] evolve together and influence each other.&lt;ref&gt;{{Cite conference
| doi = 10.1109/CEC.2008.4630793
| conference = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)
| pages = 162–168
| last = Arcuri
| first = Andrea
| last2 = Yao
| first2 = Xin 
| title = A novel co-evolutionary approach to automatic software bug fixing
| booktitle = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)
| year = 2008
}}&lt;/ref&gt;

===Testing===

Search-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. [[Regression testing]] has also received some attention.

===Optimizing software===
The use of SBSE in [[program optimization]], or modifying a piece of software to make it more efficient in terms of speed and resource use, has been the object of developing research interest and success. Genetic programming has been used to improve programs. In one instance, a 50,000 line program was genetically improved, resulting in a program 70 times faster on average.&lt;ref&gt;
{{Cite journal
| last = Langdon
| first = William B.
| last2 = Harman
| first2 = Mark 
| title = Optimising Existing Software with Genetic Programming
| journal = IEEE Transactions on Evolutionary Computation
| url = http://www0.cs.ucl.ac.uk/staff/w.langdon/ftp/papers/Langdon_2013_ieeeTEC.pdf
}}&lt;/ref&gt;

===Project management===
A number of decisions which are normally made by a project manager can be done automatically, for example, project scheduling.&lt;ref&gt;
{{Cite conference
| publisher = ACM
| doi = 10.1145/2330163.2330332
| isbn = 978-1-4503-1177-9
| pages = 1221–1228
| last = Minku
| first = Leandro L.
| last2 = Sudholt
| first2 = Dirk
| last3 = Yao
| first3 = Xin 
| title = Evolutionary algorithms for the project scheduling problem: runtime analysis and improved design
| booktitle = Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference
| location = New York, NY, USA
| series = GECCO '12
| accessdate = 2013-10-31
| year = 2012
| url = http://doi.acm.org/10.1145/2330163.2330332
}}&lt;/ref&gt;

==Tools==

There are a number of tools available for SBSE approaches. These include tools like [[OpenPAT]].&lt;ref&gt; 
{{cite conference
|ref        = harv
|last       = Mayo
|first      = M.
|coauthors  = Spacey, S.
|title      = Predicting Regression Test Failures Using Genetic Algorithm-Selected Dynamic Performance Analysis Metrics
|url        = http://rd.springer.com/chapter/10.1007/978-3-642-39742-4_13
|format     = PDF
|journal    = Proceedings of the 5th International Symposium on Search-Based Software Engineering (SSBSE)
|volume     = 8084
|pages      = 158–171
|year       = 2013
}}&lt;/ref&gt;
and Evosuite &lt;ref&gt;(http://www.evosuite.org/)&lt;/ref&gt;
and a code coverage measurement for Python
&lt;ref&gt;
https://pypi.python.org/pypi/coverage
&lt;/ref&gt;

==Methods and techniques==

There are a number of methods and techniques available. 
A non-exhaustive list of these tools includes:

•[[profiling (computer programming)|Profiling]]
&lt;ref&gt;http://java-source.net/open-source/profilers&lt;/ref&gt; via [[instrumentation]] in order to monitor certain parts of a program as it is executed.

•Obtaining an [[abstract syntax tree]] associated with the program, which can be automatically examined to gain insights into the structure of a program.

•Applications of [[program slicing]] relevant to SBSE include [[software maintenance]], [[Optimization (computer science)|optimization]], [[Program analysis (computer science)|program analysis]].

•[[Code coverage]] allows measuring how much of the code is executed with a given 
set of input data.

•[[Static program analysis]]

==Industry acceptance==

As a relatively new area of research, SBSE does not yet benefit from broad industry acceptance. One issue is that software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are quite different from the ones humans would produce.&lt;ref&gt;
{{cite web
 |url        = http://shape-of-code.coding-guidelines.com/2013/10/18/programming-using-genetic-algorithms-isnt-that-what-humans-already-do/
 |title      = Programming using genetic algorithms: isn’t that what humans already do ;-)
 |last       = Jones
 |first      = Derek
 |date       = 18 October 2013
 |website    = The Shape of Code
 |accessdate = 31 October 2013
}}
&lt;/ref&gt;
In the context of SBSE use in fixing or improving programs, developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a system's requirements and testing environment. Considering that fully automated programming has yet to be achieved, a desirable property of such modifications would be that they need to be easily understood by humans to favor program maintainability.&lt;ref&gt;
{{Cite journal
| doi = 10.1007/s11219-013-9208-0
| issn = 1573-1367
| volume = 21
| issue = 3
| pages = 421–443
| last = Le Goues
| first = Claire
| last2 = Forrest 
| first2 = Stephanie 
| last3 = Weimer
| first3 = Westley
| title = Current challenges in automatic software repair
| journal = Software Quality Journal
| accessdate = 2013-10-31
| date = 2013-09-01
| url = http://link.springer.com/article/10.1007/s11219-013-9208-0
}}
&lt;/ref&gt;

Another concern is that SBSE might make the software engineer redundant. Researchers have argued that, on the contrary, the motivation for SBSE is to enhance the relationship between the engineer and the program.&lt;ref&gt;
{{Cite conference
| publisher = IEEE Press
| conference = First International Workshop on Combining Modelling with Search-Based Software Engineering,First International Workshop on Combining Modelling with Search-Based Software Engineering
| pages = 49–50
| last = Simons
| first = Christopher L.
| title = Whither (away) software engineers in SBSE?
| location = San Francisco, USA
| accessdate = 2013-10-31
| date = May 2013
| url = http://eprints.uwe.ac.uk/19938/
}}&lt;/ref&gt;

==See also==
{{Portal|Software Testing}}
*[[Program analysis (computer science)]]
*[[Dynamic program analysis]]

==References==
{{reflist|colwidth=30em}}

==External links==
*[http://crestweb.cs.ucl.ac.uk/resources/sbse_repository/ Repository of publications on SBSE]
*[http://neo.lcc.uma.es/mase/ Metaheuristics and Software Engineering]
*[http://sir.unl.edu/portal/index.php  Software-artifact Infrastructure Repository]
*[http://2013.icse-conferences.org/ International Conference on Software Engineering]
*[http://www.sigevo.org/wiki/tiki-index.php Genetic and Evolutionary Computation (GECCO)]
*[http://scholar.google.co.uk/citations?view_op=search_authors&amp;hl=en&amp;mauthors=label:sbse Google Scholar page on Search-based software engineering]

[[Category:2001 introductions]]
[[Category:Software engineering]]
[[Category:Software testing]]
[[Category:Searching]]
[[Category:Search algorithms]]
[[Category:Optimization algorithms and methods]]
[[Category:Genetic algorithms]]</text>
      <sha1>6kz9t5sdtolaor65643edz6gpshnmim</sha1>
    </revision>
  </page>
  <page>
    <title>Find</title>
    <ns>0</ns>
    <id>1486231</id>
    <revision>
      <id>645417482</id>
      <parentid>645263234</parentid>
      <timestamp>2015-02-03T06:23:06Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fix for #03.  Missing Reflist.  Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (10813)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17237">{{other uses}}
{{unreferenced|date=September 2013}}
{{lowercase|title=find}} 
In [[Unix-like]] and some other [[operating system]]s, &lt;code&gt;'''find'''&lt;/code&gt; is a [[command-line utility]] that [[Search engine (computing)|searches]] through one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[file name]] or a time range to match against the modification time or access time of the file. By default, &lt;code&gt;find&lt;/code&gt; returns a list of all files below the current [[working directory]].

The related &lt;code&gt;[[locate (Unix)|locate]]&lt;/code&gt; programs use a database of indexed files obtained through &lt;code&gt;find&lt;/code&gt; (updated at regular intervals, typically by &lt;code&gt;[[cron]]&lt;/code&gt; job) to provide a faster method of searching the entire filesystem for files by name.

==History==
&lt;code&gt;find&lt;/code&gt; appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer's Workbench]] project.&lt;ref name=&quot;reader&quot;&gt;{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 1971–1986 |series=CSTR |number=139 |institution=Bell Labs}}&lt;/ref&gt;

== Find syntax ==
{{expand section|date=August 2008}}

&lt;code&gt;find [-H] [-L] [-P] path... [expression]&lt;/code&gt;

The three options control how the &lt;code&gt;find&lt;/code&gt; command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the &lt;code&gt;find&lt;/code&gt; command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of &lt;code&gt;find&lt;/code&gt;.

At least one path must precede the expression. &lt;code&gt;find&lt;/code&gt; is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].

Expression elements are whitespace-separated and evaluated from left to right.  They can contain logical elements such as AND (&amp;#x2011;and or &amp;#x2011;a) and OR (&amp;#x2011;or &amp;#x2011;o) as well as more complex predicates.

The [[GNU Find Utilities|GNU]] &lt;code&gt;find&lt;/code&gt; has a large number of additional features not specified by POSIX.

== POSIX protection from infinite output ==

Real-world filesystems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]].  The [[POSIX|POSIX standard]] requires that
{{Quotation|
The &lt;code&gt;find&lt;/code&gt; utility shall detect infinite loops; that is, entering a previously visited
directory that is an ancestor of the last file encountered. When it detects an infinite
loop, &lt;code&gt;find&lt;/code&gt; shall write a diagnostic message to standard error and shall either recover
its position in the hierarchy or terminate.
}}

==Operators ==
Operators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:

*'''( expr )''' Force precedence. 
*'''! expr''' True if expr is false.
*'''expr1 expr2''' And (implied); expr2 is not evaluated if expr1 is false. 
*'''expr1 -a expr2''' Same as expr1 expr2.  
*'''expr1 -o expr2''' Or; expr2 is not evaluated if expr1 is true.

 find . -name 'fileA_*' -o -name 'fileB_*'

This command searches files whose name has a prefix of &quot;fileA_&quot; or &quot;fileB_&quot; in the current directory.

 find . -name 'foo.cpp' '!' -path '.svn'

This command searches for files with the name &quot;foo.cpp&quot; in all subdirectories of the current directory (current directory itself included) other than &quot;.svn&quot;.   We quote the ! so that it's not interpreted by the shell as the history substitution character.

==Type filter explanation==

'''-type''' ''option used to specify search for only file, link or directory.''
Various type filters are supported by find. They are activated using the

 find -type c

configuration switch where c may be any of:
* '''b '''[[Device file|block (buffered) special]]
* '''c '''[[Device file|character (unbuffered special)]]
* '''d [[Directory (computing)|directory]]'''
* '''p '''[[Named pipe|named pipe (FIFO)]]
* '''f [[regular file]]'''
* '''l '''[[symbolic link]]; this is never true if the -L option or the -follow option is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension).
* '''s '''[[Unix domain socket|socket]]
* '''D '''[[Doors (computing)|door (Solaris)]]

The configuration switches listed in bold are most commonly used.

==Examples==

===From current directory===
 find . -name 'my*'

This searches in the current directory (represented by the dot character) and below it, for files and directories with names starting with ''my''. The quotes avoid the [[shell (computing)|shell]] expansion — without them the shell would replace ''my*'' with the list of files whose names begin with ''my'' in the current directory. In newer versions of the program, the directory may be omitted, and it will imply the current directory.

===Files only===
 find . -name 'my*' -type f
This limits the results of the above search to only regular files, therefore excluding directories, special files, pipes, symbolic links, etc. ''my*'' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of  files in the current directory starting with ''my''......

===Commands===
The previous examples created listings of results because, by default, &lt;code&gt;find&lt;/code&gt; executes the '-print' action.   (Note that early versions of the &lt;code&gt;find&lt;/code&gt; command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)

 find . -name 'my*' -type f -ls
This prints extended file information.

===Search all directories===
 find / -name myfile -type f -print
This searches every file on the computer for a file with the name ''myfile'' and prints it to the screen. It is generally not a good idea to look for data files this way.  This can take a considerable amount of time, so it is best to specify the directory more precisely.  Some operating systems may mount dynamic filesystems that are not congenial to &lt;code&gt;find&lt;/code&gt;.   More complex filenames including characters special to the shell may need to be enclosed in single quotes.

===Search all but one directory subtree===
 find / -path excluded_path -prune -o -type f -name myfile -print
This searches every folder on the computer except the subtree ''excluded_path'' (full path including the leading /), for a file with the name ''myfile''.  It will not detect directories, devices, links, doors, or other &quot;special&quot; filetypes.

===Specify a directory===
 find /home/weedly -name 'myfile' -type f -print
This searches for files named ''myfile'' in the ''/home/weedly'' directory, the home directory for userid ''weedly''.  You should always specify the directory to the deepest level you can remember.  The quotes are optional in this example because &quot;myfile&quot; contains no characters special to the shell.

===Search several directories===
 find local /tmp -name mydir -type d -print
This searches for directories named ''mydir'' in the ''local'' subdirectory of the current working directory and the ''/tmp'' directory.

===Ignore errors===
If you're doing this as a user other than root, you might want to ignore permission denied (and any other) errors.  Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null.  The following example shows how to do this in the bash shell: 
 find / -name 'myfile' -type f -print 2&gt;/dev/null

If you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well.  You can use sh to run the &lt;code&gt;find&lt;/code&gt; command to get around this:
 sh -c find / -name 'myfile' -type f -print 2&gt;/dev/null

An alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.
 find . -name 'myfile' |&amp; grep -v 'Permission denied'

===Find any one of differently named files===
 find . \( -name '*jsp' -o -name '*java' \) -type f -ls

The &lt;code&gt;-ls&lt;/code&gt; option prints extended information, and the example finds any file whose name ends with either 'jsp' or 'java'. Note that the parentheses are required. Also note that the operator &quot;or&quot; can be abbreviated as &quot;o&quot;. The &quot;and&quot; operator is assumed where no operator is given.  In many shells the parentheses must be escaped with a backslash, &quot;\(&quot; and &quot;\)&quot;, to prevent them from being interpreted as special shell characters. The &lt;code&gt;-ls&lt;/code&gt; option and the &lt;code&gt;-or&lt;/code&gt; operator are not available on all versions of &lt;code&gt;find&lt;/code&gt;.

===Execute an action===
 find /var/ftp/mp3 -name '*.mp3' -type f -exec chmod 644 {} \;
This command changes the [[File system permissions|permissions]] of all files with a name ending in ''.mp3'' in the directory ''/var/ftp/mp3''. The  action is carried out by specifying the option &lt;code&gt;-exec [[chmod]] 644 {} \;&lt;/code&gt; in the command. For every file whose name ends in &lt;code&gt;.mp3&lt;/code&gt;, the command &lt;code&gt;chmod 644 {}&lt;/code&gt; is executed replacing &lt;code&gt;{}&lt;/code&gt; with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission &lt;code&gt;644&lt;/code&gt;, usually shown as &lt;code&gt;rw-r--r--&lt;/code&gt;, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the &lt;code&gt;{}&lt;/code&gt; must be quoted.  The trailing &quot;;&quot; is customarily quoted with a leading &quot;\&quot;, but could just as effectively be enclosed in single quotes.

Note that the command itself should *not* be quoted; otherwise you get error messages like

 find: echo &quot;mv ./3bfn rel071204&quot;: No such file or directory

which means that &lt;code&gt;find&lt;/code&gt; is trying to run a file called 'echo &quot;mv ./3bfn rel071204&quot;' and failing.

If you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.

 find . -exec COMMAND {} +

This will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.

===Delete files and directories===
'''Caveats''': the -delete action is a GNU extension, and using it turns on -depth.   So, if you are testing a find command with -print instead of -delete in order to figure out what will happen before going for it, you need to use -depth -print.

Delete empty files and directories and print the names (note that -empty is a vendor unique extension from GNU find that may not be available in all find implementations)
 find /foo -empty -delete -print

Delete empty files
 find /foo -type f -empty -delete

Delete empty directories
 find /foo -type d -empty -delete

Delete files and directories (if empty) named &lt;code&gt;bad&lt;/code&gt; 
 find /foo -name bad -empty -delete

'''Warning''': &lt;code&gt;-delete&lt;/code&gt; should be used with other operators such as
&lt;code&gt;-empty&lt;/code&gt; or &lt;code&gt;-name&lt;/code&gt;.

 find /foo -delete  # this deletes '''all''' in /foo

===Search for a string===
This command will search for a string in all files from the /tmp directory and below:
&lt;source lang=&quot;bash&quot;&gt;
 $ find /tmp -type f -exec grep 'search string' '{}' /dev/null \+
&lt;/source&gt;
The &lt;tt&gt;[[/dev/null]]&lt;/tt&gt; argument is used to show the name of the file before the text that is found. Without it, only the text found is printed.  An equivalent mechanism is to use the &quot;-H&quot; or &quot;--with-filename&quot; option to grep:
&lt;source lang=&quot;bash&quot;&gt;
 $ find /tmp -type f -exec grep -H 'search string' '{}' '+' 
&lt;/source&gt;
GNU grep can be used on its own to perform this task:

 $ grep -r 'search string' /tmp

Example of search for &quot;LOG&quot; in jsmith's home directory
&lt;source lang=&quot;bash&quot; highlight=&quot;1&quot;&gt;
 $ find ~jsmith -exec grep LOG '{}' /dev/null \; -print
 /home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME
 /home/jsmith/scripts/errpt.sh:cat $LOG
 /home/jsmith/scripts/title:USER=$LOGNAME
&lt;/source&gt;
Example of search for the string &quot;ERROR&quot; in all XML files in the current directory and all sub-directories
&lt;source lang=&quot;bash&quot;&gt;

 $ find . -name &quot;*.xml&quot; -exec grep &quot;ERROR&quot; /dev/null '{}' \+ 
&lt;/source&gt;
The double quotes (&quot; &quot;) surrounding the search string and single quotes (&lt;nowiki&gt;' '&lt;/nowiki&gt;) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string.  Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since '''double quotes do not prevent all special interpretation'''. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes.  Example:
&lt;source lang=&quot;bash&quot;&gt;

 $ find . -name &quot;file-containing-can't&quot; -exec grep &quot;can't&quot; '{}' \; -print
&lt;/source&gt;

===Search for all files owned by a user===
 find . -user &lt;userid&gt;

===Search in case insensitive mode===
Note that -iname is not in the standard and may not be supported by all implementations.

 find . -iname ''''MyFile'''*'

If the &lt;code&gt;-iname&lt;/code&gt; switch is not supported on your system then workaround techniques may be possible such as:

 find . -name '[m'''M''']['''y'''Y][f'''F''']['''i'''I]['''l'''L]['''e'''E]*'

This uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):

 echo &quot;''''MyFile'''*'&quot; |perl -pe 's/([a-zA-Z])/[\L\1\U\1]/g;s/(.*)/find . -name \1/'|sh

===Search files by size===
Example of searching files with size between 100 kilobytes and 500 kilobytes.
 find . -size +100k -a -size -500k
Example of searching empty files.
 find . -size 0k
Example of searching non-empty files.
 find . ! -size 0k

===Search files by name and size ===
 '''find''' /usr/src {{abbr|!|the negation of the expression that follows}} {{abbr|\(|the start of a complex expression.}} -name '*,v' {{abbr|-o|a logical or of a complex expression. In this case the complex expression is all files like '*,v' or '.*,v'}} -name '.*,v' {{abbr|\)|the end of a complex expression.}} '{}' \; -print

This command will search in the /usr/src directory and all sub directories. All files that are of the form '*,v' and '.*,v' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.

&lt;source lang=&quot;bash&quot; enclose=&quot;div&quot;&gt;
for file in `find /opt \( -name error_log -o -name 'access_log' -o -name 'ssl_engine_log' -o -name 'rewrite_log' -o
 -name 'catalina.out' \) -size +300000k -a -size -5000000k`; do 
    cat /dev/null &gt; $file
done
&lt;/source&gt;
The units should be one of [bckw], 'b' means 512-byte blocks, 'c' means byte, 'k' means kilobytes and 'w' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.

==Related utilities==
* &lt;code&gt;[[locate (Unix)|locate]]&lt;/code&gt; is a Unix search tool that searches through a prebuilt database of files instead of directory trees of a file system. This is faster than &lt;code&gt;find&lt;/code&gt; but less accurate because the database may not be up-to-date.
* &lt;code&gt;[[grep]]&lt;/code&gt; is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].
* &lt;code&gt;[[tree (Unix)|tree]]&lt;/code&gt; is a command-line utility that recursively lists files found in a directory tree, indenting the file names according to their position in the file hierarchy.
* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools &lt;code&gt;find&lt;/code&gt; and [[xargs]].
* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of &lt;code&gt;find&lt;/code&gt;.
* &lt;code&gt;[[dir (command)|dir]]&lt;/code&gt; has the /s option that recursively searches for files or folders.

==See also==
*[[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]
*[[List of Unix programs]]
*[[List of DOS commands]]
*[[List of duplicate file finders]]
*[[Filter (higher-order function)]]
*[[find (command)]], a DOS and Windows command that is very different from UNIX &lt;code&gt;find&lt;/code&gt;

==References==
{{Reflist}}

==External links==
*{{man|cu|find|SUS|find files}}
*[http://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]

{{Unix commands}}

[[Category:Searching]]
[[Category:Standard Unix programs]]
[[Category:Unix SUS2008 utilities]]</text>
      <sha1>esby6ug0z0u8fry86qrvfaztie29tar</sha1>
    </revision>
  </page>
  <page>
    <title>Bayesian search theory</title>
    <ns>0</ns>
    <id>1510587</id>
    <revision>
      <id>645711401</id>
      <parentid>598670324</parentid>
      <timestamp>2015-02-05T05:57:58Z</timestamp>
      <contributor>
        <ip>50.240.211.170</ip>
      </contributor>
      <comment>adding comma</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7957">'''Bayesian search theory''' is the application of [[Bayesian statistics]] to the search for lost objects. It has been used several times to find lost sea vessels, for example the [[USS Scorpion (SSN-589)|USS ''Scorpion'']]. It also played a key role in the recovery of the flight recorders in the [[Air France Flight 447]] disaster of 2009. 

==Procedure==

The usual procedure is as follows:

# Formulate as many reasonable hypotheses as possible about what may have happened to the object. 
# For each hypothesis, construct a [[probability density function]] for the location of the object.
# Construct a function giving the probability of actually finding an object in location&amp;nbsp;X when searching there if it really is in location&amp;nbsp;X. In an ocean search, this is usually a function of water depth — in shallow water chances of finding an object are good if the search is in the right place. In deep water chances are reduced.
# Combine the above information coherently to produce an overall probability density map. (Usually this simply means multiplying the two functions together.) This gives the probability of finding the object by looking in location&amp;nbsp;X, for all possible locations&amp;nbsp;X. (This can be visualized as a [[contour map]] of probability.)
# Construct a search path which starts at the point of highest probability and 'scans' over high probability areas, then intermediate probabilities, and finally low probability areas.
# Revise all the probabilities continuously during the search. For example, if the hypotheses for location&amp;nbsp;X imply the likely disintegration of the object and the search at location&amp;nbsp;X has yielded no fragments, then the probability that the object is somewhere around there is greatly reduced (though not usually to zero) while the probabilities of its being at other locations is correspondingly increased. The revision process is done by applying [[Bayes' theorem]].

In other words, first search where it most probably will be found, then search where finding it is less probable, then search where the probability is even less (but still possible due to limitations on fuel, range, water currents, etc.), until insufficient hope of locating the object at acceptable cost remains.

The advantages of the Bayesian method are that all information available is used coherently (i.e., in a &quot;leak-proof&quot; manner) and the method automatically produces estimates of the cost for a given success probability. That is, even before the start of searching, one can say, hypothetically, &quot;there is a 65% chance of finding it in a 5-day search. That probability will rise to 90% after a 10-day search and 97% after 15&amp;nbsp;days&quot; or a similar statement. Thus the economic viability of the search can be estimated before committing resources to a search.

Apart from the [[USS Scorpion (SSN-589)|USS ''Scorpion'']], other vessels located by Bayesian search theory include the [[MV Derbyshire|MV&amp;nbsp;''Derbyshire'']], the largest British vessel ever lost at sea, and the [[SS Central America|SS&amp;nbsp;''Central America'']]. It also proved successful in the search for a lost [[hydrogen bomb]] following the [[1966 Palomares B-52 crash]] in Spain, and the recovery in the Atlantic Ocean of the crashed [[Air France Flight 447]].

Bayesian search theory is incorporated into the CASP (Computer Assisted Search Program) mission planning software used by the [[United States Coast Guard]] for [[search and rescue]]. This program was later adapted for inland search by adding terrain and ground cover factors for use by the [[United States Air Force]] and [[Civil Air Patrol]].

==Mathematics==

Suppose a grid square has a probability ''p'' of containing the wreck and that the probability of successfully detecting the wreck if it is there is ''q''. If the square is searched and no wreck is found, then, by Bayes' theorem, the revised probability of the wreck being in the square is given by

: &lt;math&gt;  p' = \frac{p(1-q)}{(1-p)+p(1-q)} = p \frac{1-q}{1-pq} &lt; p.&lt;/math&gt;
For each other grid square, if its prior probability is ''r'', its posterior probability is given by

: &lt;math&gt; r' = r \frac{1}{1- pq} &gt; r. &lt;/math&gt;


==Optimal Distribution of Search Effort==

The classical book on this subject, based on probabilistic information, by [[Lawrence D. Stone]], won the 1975 [[Frederick W. Lanchester Prize]] by the [[Operations Research Society of America]].



&lt;!-- Material on USS Scorpion, moved from Bayesian inference

In May 1968, the [[U.S. Navy]]'s [[nuclear submarine]] [[USS Scorpion (SSN-589)|USS ''Scorpion'' (SSN-589)]] failed to arrive as expected at her home port of [[Norfolk, Virginia]]. The command officers of the U.S. Navy were nearly convinced that the vessel had been lost off the [[East Coast of the United States|Eastern Seaboard]],  but an extensive search there failed to discover the remains of the ''Scorpion''.

Then, a Navy deep-water expert, [[John Craven USN|John P. Craven]], suggested that the USS ''Scorpion'' had sunk elsewhere. Craven organised a search southwest of the [[Azores]] based on a controversial approximate triangulation by [[hydrophone]]s. He was allocated only a single ship, the [[USNS Mizar (AGOR-11)|''Mizar'']], and he took advice from a firm of consultant mathematicians in order to maximise his resources. A Bayesian search methodology was adopted. Experienced submarine commanders were interviewed to construct hypotheses about what could have caused the loss of the ''Scorpion''.

The sea area was divided up into grid squares and a probability assigned to each square, under each of the hypotheses, to give a number of probability grids, one for each hypothesis. These were then added together to produce an overall probability grid. The probability attached to each square was then the probability that the wreck was in that square. A second grid was constructed with probabilities that represented the probability of successfully finding the wreck if that square were to be searched and the wreck were to be actually there. This was a known function of water depth. The result of combining this grid with the previous grid is a grid which gives the probability of finding the wreck in each grid square of the sea if it were to be searched.


==Optimal Distribution of Search Effort==

The classical book on this subject by [[Lawrence D. Stone]] won the 1975 [[Lancaster Prize]] by the American Operations Research Society.

--&gt;

==See also==
* [[Bayesian inference]]
* [[Search games]]

== References ==
* [[Stone, Lawrence D.]], ''The Theory of Optimal Search'', published by the [[Operations Research Society of America]], 1975
* [[Stone, Lawrence D.]], In search of Air France Flight 447. Institute of Operations Research and the Management Sciences, 2011
* Iida, Koji., '' Studies on the Optimal Search Plan'', Vol.&amp;nbsp;70, Lecture Notes in Statistics, Springer-Verlag, 1992.
* De Groot, Morris H., ''Optimal Statistical Decisions'', Wiley Classics Library, 2004.
* Richardson, Henry R; and Stone, Lawrence D. Operations Analysis during the underwater search for ''Scorpion''. ''Naval Research Logistics Quarterly'', June&amp;nbsp;1971, Vol.&amp;nbsp;18, Number&amp;nbsp;2. Office of Naval Research.
* Stone, Lawrence D. Search for the SS ''Central America'': Mathematical Treasure Hunting. Technical Report, Metron Inc. Reston, Virginia.
* [[Bernard Koopman|Koopman, B.O.]] ''Search and Screening'', Operations Research Evaluation Group Report 56, Center for Naval Analyses, Alexandria, Virginia. 1946.
* Richardson, Henry R; and Discenza, J.H. The United States Coast Guard computer-assisted search planning system (CASP). ''Naval Research Logistics Quarterly''. Vol.&amp;nbsp;27 number&amp;nbsp;4. pp.&amp;nbsp;659–680. 1980.
* [[Ross, Sheldon M.]], ''An Introduction to Stochastic Dynamic Programming'', Academic Press. 1983.

[[Category:Bayesian statistics|Search theory]]
[[Category:Searching]]
[[Category:Search algorithms]]
[[Category:Operations research]]</text>
      <sha1>pub6c1wu546va6636dqdw8ik12c7ggs</sha1>
    </revision>
  </page>
  <page>
    <title>Variable neighborhood search</title>
    <ns>0</ns>
    <id>35517591</id>
    <revision>
      <id>646509376</id>
      <parentid>641489698</parentid>
      <timestamp>2015-02-10T15:37:09Z</timestamp>
      <contributor>
        <username>Frietjes</username>
        <id>13791031</id>
      </contributor>
      <comment>Clean up [[:Category:Pages using duplicate arguments in template calls|duplicate template arguments]] using [[User:Frietjes/findargdups|findargdups]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23936">'''Variable neighborhood search''' (VNS),&lt;ref&gt;{{cite journal |pages=367–407 |last1 = Hansen  |first1 = P.|last2 = Mladenovic|first2 = N.|last3 = Perez|first3 = J.A.M.|title=Variable neighbourhood search: methods and applications
|volume=175 |journal= Annals of Operations Research |year=2010 |doi=10.1007/s10479-009-0657-6}}&lt;/ref&gt; proposed by [[Mladenović, Hansen]], 1997,&lt;ref name=&quot;.....&quot;&gt;{{cite journal
 | author = Nenad Mladenovi´c, Pierre Hansen
 | year = 1997
 | title = Variable neighborhood search
 | journal = Computers and Operations Research
 | volume = 24
 | issue= 11
 | pages = 1097–1100
 | doi=10.1016/s0305-0548(97)00031-2
 }}
&lt;/ref&gt; is a [[metaheuristic]] method for solving a set of [[combinatorial optimization (mathematics)|combinatorial optimization]] and global optimization problems.
It explores distant neighborhoods of the current incumbent solution, and moves from there to a new one if and only if an improvement was made. The local search method is applied repeatedly to get from solutions in the neighborhood to local optima.
VNS was designed for approximating solutions of discrete and continuous optimization problems and according to these, it is aimed for solving [[linear programming|linear program]] problems, [[linear programming|integer program]] problems, mixed integer program problems, [[nonlinear programming|nonlinear program]] problems, etc.

== Introduction ==
VNS systematically changes the neighborhood in two phases: firstly, descent to find a [[local optimum]] and finally, a perturbation phase to get out of the corresponding valley.

Applications are rapidly increasing in number and pertain to many fields: [[location theory]], [[cluster analysis]], [[scheduling]], [[Vehicle routing problem|vehicle routing]], [[Network planning and design|network design]], lot-sizing, [[artificial intelligence]], engineering, pooling problems, biology, [[Phylogenetics|phylogeny]], [[wikt:reliability|reliability]], geometry, telecommunication design, etc.

There are several books important for understanding VNS, such as: ''Handbook of Metaheuristics'', 2010,&lt;ref&gt;{{cite journal |last1=Gendreau|  first1=M.|last2= Potvin|first2=J-Y.|title=Handbook of Metaheuristics|publisher =Springer|year=2010 }}&lt;/ref&gt; Handbook of Metaheuristics, 2003&lt;ref&gt;{{cite journal|last1=Glover|  first1=F.|last2= Kochenberger|first2=G.A.|title=Handbook of Metaheuristics|publisher = Kluwer Academic Publishers |year=2003}}&lt;/ref&gt; and Search methodologies, 2005.&lt;ref&gt;{{cite journal |last1=Burke|first1=EK.|last2= Kendall | first2=G.| title=Search methodologies. Introductory tutorials in optimization and decision support techniques |journal = Springer|year=2005}}&lt;/ref&gt;
Earlier work that motivated this approach can be found in
# Davidson, W.C.,&lt;ref&gt;{{cite journal |last1=Davidson  |first1=W.C.|title=Variable metric algorithm for minimization  |journal= Argonne National Laboratory Report ANL-5990 |year=1959 }}&lt;/ref&gt;
# Fletcher, R., Powell, M.J.D.,&lt;ref&gt;{{cite journal |pages=163–168 |last1=Fletcher |first1=R. |last2=Powell |first2=M.J.D. |title=Rapidly convergent descent method for minimization|volume=6 |journal=Comput.J. |year=1963 |doi=10.1093/comjnl/6.2.163}}&lt;/ref&gt;
# Mladenovi´c, N.&lt;ref&gt;{{cite journal |pages= 112 |last1=Mladenovi´c |first1=N. |title=A variable neighborhood algorithm—a new metaheuristic for combinatorial optimization | journal=Abstracts of papers presented at Optimization Days, Montr´eal |year=1995 }}
&lt;/ref&gt; and 4. Brimberg, J., Mladenovi´c, N.&lt;ref&gt;{{cite journal |pages=1–12 |last1=Brimberg |first1=J. |last2 = Mladenovi´c |first2=N. |title=A variable neighborhood algorithm for solving the continuous location-allocation problem |volume=10 |journal=Stud. Locat. Anal. |year=1996}}&lt;/ref&gt; Recent surveys on VNS  methodology as well as numerous applications can be found in 4OR, 2008.&lt;ref&gt;{{cite journal |pages=319–360 |last1=Hansen |first1=P. |last2 = Mladenovi´c |first2=N. |last3= Perez| first3=J.A.M|title=Variable neighbourhood search: methods and applications|volume=6 |journal=4OR |year=2008 |doi=10.1007/s10288-008-0089-1}}&lt;/ref&gt; and Annals of OR, 2010.

== Basic description ==
Define one deterministic [[optimization problem]] with

&lt;math&gt; \min {\{f (x)|x \in X, X \subseteq S\}} &lt;/math&gt;, (1)

where ''S'', ''X'', ''x'', and ''f''  are the solution space, the feasible set, a feasible solution, and a real-valued [[mathematical optimization|objective function]], respectively. If ''S'' is a finite but large set, a combinatorial optimization problem is defined. If &lt;math&gt;{S = R^{n}}&lt;/math&gt;, there is continuous optimization model.

A solution &lt;math&gt;{x^* \in X}&lt;/math&gt; is optimal if

&lt;math&gt; {f (x^{*}) \leq f (x), \qquad \forall{x}\, \in X} &lt;/math&gt;.

Exact algorithm for problem (1) is to be found an optimal solution ''x*'', with the validation of its optimal structure, or if it is unrealizable, in procedure have to be shown that there is no  achievable solution, i.e., &lt;math&gt;X =\varnothing&lt;/math&gt;, or the solution is unbounded. CPU time has to be finite and short. For continuous optimization, it is reasonable to allow for some degree of tolerance, i.e., to stop when a feasible solution &lt;math&gt;x^{*}&lt;/math&gt; has been found such that

&lt;math&gt; {f (x^{*}) \leq f (x) + \epsilon, \qquad \forall{x}\, \in X} &lt;/math&gt; or
&lt;math&gt; {(f (x^{*})- f (x))/ f (x^{*})  &lt;  \epsilon  , \qquad \forall{x}\, \in X} &lt;/math&gt;

Some heuristics speedily accept an approximate solution, or optimal solution but one with no validation of its optimality.
Some of them have an incorrect certificate, i.e., the solution &lt;math&gt;x_h&lt;/math&gt; obtained satisfies

&lt;math&gt; {(f (x_{h})- f (x))/ f (x_{h})  \leq  \epsilon  , \qquad \forall{x}\, \in X} &lt;/math&gt;
for some &lt;math&gt;\epsilon&lt;/math&gt;, though this is rarely small.

Heuristics are faced with the problem of local optima as a result of avoiding boundless computing time.
A local optimum &lt;math&gt;x_L&lt;/math&gt; of problem is such that

&lt;math&gt; {f (x_{L}) \leq f (x), \qquad \forall{x}\, \in N(x_{L}) \cap X} &lt;/math&gt;

where &lt;math&gt; N(x_{L})&lt;/math&gt;  denotes a neighborhood of &lt;math&gt; x_{L} &lt;/math&gt;

== Description ==
According to (Mladenovic, 1995), VNS is a metaheuristic which systematically performs the procedure of neighborhood change, both in descent to local minima and in escape from the valleys which contain them.

VNS is built upon the following perceptions:

# A local minimum with respect to one neighbourhood structure is not necessarily a local minimum for another neighbourhood structure.
# A global minimum is a local minimum with respect to all possible neighborhood structures.
# For many problems, local minima with respect to one or several neighborhoods are relatively close to each other.

Unlike many other metaheuristics, the basic schemes of VNS and its extensions are simple and require few, and sometimes no parameters. Therefore, in addition to providing very good solutions, often in simpler ways than other methods, VNS gives insight into the reasons for such a performance, which, in turn, can lead to more efficient and sophisticated implementations.

There are several papers where it could be studied among recently mentioned, such as (Hansen and Mladenovi´c 1999, 2001a, 2003, 2005; Moreno-Pérez et al.;&lt;ref&gt;{{cite journal||last1=Moreno-Pérez|first1=JA.|last2=Hansen|first2=P. |last3=Mladenovic|first3=N.| title = Parallel variable neighborhood search|journal=Alba E (ed) Parallel metaheuristics: a new class of algorithms|year=2005}}&lt;/ref&gt;)

==[[Local search (optimization)|Local search]]==

A local search heuristic is performed through choosing an initial solution x, discovering a direction of descent from x, within a neighbourhood N(x), and proceeding to the minimum of f(x) within N(x) in the same direction. If there is no direction of descent, the heuristic stops; otherwise, it is iterated. Usually the highest direction of descent, also related to as best improvement, is used. This set of rules is summarized in Algorithm 1, where we assume that an initial solution x is given. The output consists of a local minimum, also denoted by x, and its value. Observe that a neighbourhood structure N(x) is defined for all x ∈ X. At each step, the neighbourhood N(x) of x is explored completely. As this may be timeconsuming, an alternative is to use the first descent heuristic. Vectors &lt;math&gt;x^i \in N(x)&lt;/math&gt; are then enumerated systematically and a move is made as soon as a direction for the descent is found. This is summarized in Algorithm 2.

Algorithm 1 Best improvement (highest descent) heuristic

Function BestImprovement(x)

  1: repeat
  2:     x' ← x
  3:     x←argmin_{f (y)}, y∈N(x)
  4: until ( f (x) ≥ f (x'))
  5: return x

Algorithm 2 First improvement (first descent) heuristic

Function FirstImprovement(x)

  1: repeat
  2:    x' ← x; i←0
  3:    repeat
  4:       i←i+1
  5:       x←argmin{ f (x), f (x^i)}, x^i  ∈ N(x)
  6:    until ( f (x) &lt; f (x^i) or i = |N(x)|)
  7: until ( f (x) ≥ f (x'))
  8: return x

Let one denote &lt;math&gt; \mathcal{ N}_k(k=1, . . . ,k_{max}) &lt;/math&gt;, a finite set of pre-selected neighborhood structures, and with &lt;math&gt;\mathcal{N}_k(x)&lt;/math&gt; the set of solutions in the ''kth'' neighborhood of ''x''.

One will also use the notation &lt;math&gt;\mathcal{N'}_k(x), k = 1, . . . , k'_{max} &lt;/math&gt; when describing local descent. Neighborhoods &lt;math&gt;\mathcal{N}_k(x)&lt;/math&gt; or &lt;math&gt;\mathcal{N'}_k(x)&lt;/math&gt; may be induced from one or more [[metric (mathematics)|metric]] (or quasi-metric) functions introduced into a solution space ''S''.
An optimal solution &lt;math&gt;x_{opt}&lt;/math&gt; (or [[maxima and minima|global minimum]]) is a feasible solution where a minimum of problem ( is reached. We call ''x' ∈ X'' a local minimum of problem with respect to &lt;math&gt;\mathcal{N}_k(x) &lt;/math&gt;, if there is no solution &lt;math&gt; x \in \mathcal{N'}_k(x) \subseteq X &lt;/math&gt; such that &lt;math&gt;f (x) &lt; f (x')&lt;/math&gt;.

In order to solve problem by using several neighbourhoods, facts 1–3 can be used in three different ways: (i) deterministic; (ii) [[stochastic]]; (iii) both deterministic and stochastic. We first give in Algorithm 3 the steps of the neighbourhood change function which will be used later. Function NeighbourhoodChange() compares the new value f(x') with the incumbent value f(x) obtained in the neighbourhood k (line 1). If an improvement is obtained, k is returned to its initial value and the new incumbent updated (line 2). Otherwise, the next neighbourhood is considered (line 3).

Algorithm 3&amp;nbsp;– Neighborhood change

Function NeighborhoodChange (x, x', k)

&lt;code&gt;
 1: if f (x') &lt; f(x) then
 2:    x ← x' // Make a move
 3:    k ← 1 // Initial neighborhood
 4: else
 5:    k ← k+1 // Next neighborhood

&lt;/code&gt;

When VNS does not render good solution, there are several steps which could be helped in process, such as comparing first and best improvement strategies in local search, reducing neighborhood, intensifying shaking, adopting VND, adopting FSS, and experimenting with parameter settings.

The Basic VNS (BVNS) method (Mladenovic and Hansen 1997) combines deterministic and stochastic changes of neighbourhood. Its steps are given in Algorithm 4. Often successive neighbourhoods &lt;math&gt; \mathcal{N}_k&lt;/math&gt; will be nested. Observe that point x' is generated at random in Step 4 in order to avoid cycling, which might occur if a deterministic rule were applied. In Step 5, the first improvement local search (Algorithm 2) is usually
adopted. However, it can be replaced with best improvement (Algorithm 1).

Algorithm 4: Basic VNS

Function VNS (x, kmax, tmax );

&lt;code&gt;

 1: repeat
 2:    k ← 1;
 3:    repeat
 4:       x' ←Shake(x, k) /* Shaking */;
 5:       x'' ← FirstImprovement(x' ) /* Local search */;
 6:       NeighbourhoodChange(x, x', k) /* Change neighbourhood */;
 7:    until k = k_max ;
 8:    t ←CpuTime()
 9: until t &gt; t_max ;

&lt;/code&gt;

The basic VNS is a first improvement [[method of steepest descent|descent method]] with randomization. Without much additional effort, it can be transformed into a descent-ascent method: in NeighbourhoodChange() function, replace also x by x&quot; with some probability, even if the solution is worse than the incumbent. It can also be changed into a best improvement method: make a move to the best neighbourhood k* among all k_max of them.
Another variant of the basic VNS can be to find a solution x' in the “Shaking” step as the best among b (a parameter) randomly generated solutions from the ''k''th neighbourhood. There are two possible variants of this extension: (1) to perform only one local search from the best among b points; (2) to perform all b local searches and then choose the best. In paper (Fleszar and Hindi&lt;ref&gt;{{cite journal|last1=Fleszar|first1=K|last2=Hindi|first2=KS|title=Solving the resource-constrained project scheduling problem by a variable neighborhood search|journal=Eur J Oper Res|year=2004|volume=155|issue=2|pages=402–413|doi=10.1016/s0377-2217(02)00884-6}}&lt;/ref&gt;) could be found algorithm.

== Extensions ==
* VND&lt;ref&gt;{{cite journal|last1=Brimberg|first1=J.|last2=Hansen|first2=P.|last3=Mladenovic|first3=N.|last4=Taillard |first4=E. |title=Improvements and comparison of heuristics for solving the multisource Weber problem|journal=Oper. Res.|year=2000|volume=48 |pages=444–460 |doi=10.1287/opre.48.3.444.12431}}&lt;/ref&gt;
:The variable neighborhood descent (VND) method is obtained if a change of neighborhoods is performed in a deterministic way. In the descriptions :of its algorithms, we assume that an initial solution x is given. Most local search heuristics in their descent phase use very few :neighbourhoods. The final solution should be a local minimum with respect to all &lt;math&gt;k_{max}&lt;/math&gt; neighbourhoods; hence the chances to reach :a global one are larger when using VND than with a single neighbourhood structure.
* RVNS&lt;ref&gt;{{cite journal|last1=Mladenovic|first1=N.|last2=Petrovic|first2=J.|last3=Kovacevic-Vujcic|first3=V.|last4=Cangalovic |first4=M. |title=Solving spread spectrum radar polyphase code design problem by tabu search and variable neighborhood search|journal=Eur. J. Oper. Res.|year=2003b|volume=151 |pages=389–399 |doi=10.1016/s0377-2217(02)00833-0}}&lt;/ref&gt;

:The reduced VNS (RVNS) method is obtained if random points are selected from &lt;math&gt;\mathcal{N}_k(x)&lt;/math&gt; and no descent is made. Rather, the :values of these new points are compared with that of the incumbent and an update takes place in case of improvement. It is assumed that a :stopping condition has been chosen like the maximum [[CPU time]] allowed &lt;math&gt;t_{max}&lt;/math&gt; or the maximum number of iterations :between two improvements.
:To simplify the description of the algorithms it is used &lt;math&gt;t_{max}&lt;/math&gt; below. Therefore, RVNS uses two parameters: &lt;math&gt;t_{max}&lt;/math&gt; :and &lt;math&gt;k_{max}&lt;/math&gt;. RVNS is useful in very large instances, for which local search is costly. It has been observed that the best value for :the parameter k_max is often 2. In addition, the maximum number of iterations between two improvements is usually used as a stopping condition. :RVNS is akin to a [[Monte-Carlo method]], but is more systematic.
* Skewed VNS
:The skewed VNS (SVNS) method (Hansen et al.)&lt;ref&gt;{{cite journal|last1=Hansen|first1=P.|last2=Jaumard|first2=B|last3=Mladenovi´c|first3=N|last4=Parreira |first4=A |title=Variable neighborhood search :for weighted maximum satisfiability problem|journal=Les Cahiers du GERAD G–2000–62, HEC Montréal, Canada|year=2000}}&lt;/ref&gt; addresses the :problem of exploring valleys far from the incumbent solution. Indeed, once the best solution in a large region has been found, it is necessary to :go some way to obtain an improved one. Solutions drawn at random in distant neighbourhoods may differ substantially from the incumbent and VNS :can then degenerate, to some extent, into the Multistart heuristic (in which descents are made iteratively from solutions generated at random, a :heuristic which is known not to be very efficient). Consequently, some compensation for distance from the incumbent must be made.
* Variable Neighbourhood Decomposition Search
:The variable neighbourhood decomposition search (VNDS) method (Hansen et al.)&lt;ref&gt;{{cite journal|last1=Hansen|first1=P|last2=Mladenovi´c|first2=N|last3=Pérez-Brito|first3=D |title=Variable neighborhood decomposition :search|journal=J Heuristics|year=2001|volume=7|issue=4|pages=335–350}}&lt;/ref&gt; extends the basic VNS into a two-level VNS scheme based upon :decomposition of the problem. For ease of presentation, but without loss of generality, it is assumed that the solution x represents the set of :some elements.
* Parallel VNS
:Several ways of parallelizing VNS have recently been proposed for solving the p-Median problem. In García-López et al.:&lt;ref&gt;{{cite journal|last1=García-López|first1=F|last2=Melián-Batista|first2=B|last3= Moreno-Pérez|first3= JA|last4= |first4=JM :|title=The parallel :variable neighborhood search for the p-median problem|journal=J Heuristics|year=2002|volume=8|issue=3|pages=375–388}}&lt;/ref&gt;&amp;nbsp; three of them :are tested: (i) parallelize local search; (ii) augment the number of solutions drawn from the current neighbourhood and make a :local search in :parallel from each of them and (iii) do the same as (ii) but update the information about the best solution found. Three Parallel :VNS strategies :are also suggested for solving the [[Travelling purchaser problem]] in Ochi et al.&lt;ref&gt;{{cite journal|last1=Ochi|first1=LS|last2=Silva|first2=MB|last3= Drummond|first3= L|title=Metaheuristics based on GRASP and VNS for solving traveling purchaser :problem|journal=MIC’2001, Porto|year=2001|pages=489–494}}&lt;/ref&gt;
* Primal-dual VNS
:For most modern heuristics, the difference in value between the optimal solution and the obtained one is completely unknown. Guaranteed :performance of the primal heuristic may be determined if a [[upper and lower bounds|lower bound]] on the objective function value is known. To :this end, the standard approach is to relax the integrality condition on the primal variables, based on a mathematical programming formulation of :the problem.
:However, when the dimension of the problem is large, even the relaxed problem may be impossible to solve exactly by standard :commercial solvers. :Therefore, it seems a good idea to solve dual relaxed problems heuristically as well. It was obtained guaranteed bounds on :the primal heuristics :performance.  In Primal-dual VNS (PD-VNS) (Hansen et al.)&lt;ref&gt;{{cite journal|last1=Hansen|first1=P|last2=Brimberg|first2=J|last3=Uroševi´c|first3=D|last4=Mladenovi´c|first4=N|title=Primal-dual variable neighborhood search for the simple plant location problem|journal=INFORMS J Comput|year=2007a|volume=19|issue=4|pages=552–564|doi=10.1287/ijoc.1060.0196}}&lt;/ref&gt; one :possible general way to attain both the guaranteed bounds and the exact solution is proposed.
* Variable Neighborhood Branching.)&lt;ref&gt;{{cite journal|last1=Hansen|first1=P.|last2=Mladenovic|first2=N.|last3=Urosevic|first3=D.|title=Variable neighborhood search and local branching|journal=Computers and Operations Research|year=2006|volume=33|pages=3034–3045|doi=10.1016/j.cor.2005.02.033}}&lt;/ref&gt;
:The mixed integer linear programming (MILP) problem consists of maximizing or minimizing a linear function, subject to equality or inequality :constraints, and integrality restrictions on some of the variables.
* Variable Neighborhood Formulation Space Search .)&lt;ref&gt;{{cite journal|last1=Mladenovic|first1=N.|last2=Plastria|first2=F.|author2-link=Frank Plastria|last3=Urosevic|first3=D.|title=Reformulation descent applied to circle packing problems|journal=Computers and Operations Research|year=2006|volume=32|pages=2419–2434|doi=10.1016/j.cor.2004.03.010}}&lt;/ref&gt;
:FSS is method which is very useful because, one problem could be defined in addition formulations and moving through formulations is legitimate. :It is proved that local search works within formulations, implying a final solution when started from some initial solution in first formulation. :Local search systematically alternates between different formulations which was investigated for [[Circle packing in a circle|circle packing]] :problem (CPP) where [[stationary point]] for a [[nonlinear programming]] formulation of CPP in [[Cartesian coordinate system|Cartesian coordinates]] is not strictly a stationary point in [[Polar coordinate system|polar coordinates]].

== Development ==
In order to make a simple version of VNS, here is the list of steps which should be made. Most of it is very similar with steps in other metaheuristics.
# It is necessary to be involved in problem, give some examples and try to solve them
# Study books, surveys and scientific papers
# Try to test some benchmarks
# Choose appropriate data structure for representing in memory
# Find initial solution
# Calculate objective function
# Design a procedure for Shaking
# Choose an local search heuristic with some moves as drop, add, swap, interchange, etc.
# Compare VNS with other methods from the literature

== Applications ==
Applications of VNS, or of varieties of VNS are very abundant and numerous. Some fields where it could be found collections of scientific papers:
* Industrial applications
* Design problems in communication
* Location problems
* [[Data mining]]
* [[Graph theory|Graph problems]]
* [[Knapsack problem|Knapsack]] and packing problems
* Mixed integer problems
* Time tabling
* [[Scheduling]]
* [[Vehicle routing problem]]s
* [[Arc routing]] and waste collection
* Fleet sheet problems
* Extended vehicle routing problems
* Problems in biosciences and chemistry
* Continuous optimization
* Other optimization problems
* Discovery science

== Conclusion ==
VNS implies several features which are presented in Hansen and Mladenovic&lt;ref&gt;{{cite journal|last1=Hansen|first1=P|last2=Mladenovi´c|first2=N|title=Variable neighborhood search|journal=Glover F, Kochenberger G (eds) Handbook
of Metaheuristics|year=2003|issue=Kluwer, Dordrecht|pages=145–184}}&lt;/ref&gt; and some are presented here:

(i) Simplicity: VNS is simple a simple and clear which is universally applicable;

(ii) Precision: VNS is formulated in precise mathematical definitions;

(iii) Coherence: all actions of the heuristics for solving problems follow from the VNS principles;

(iv) Effectiveness: VNS supplies optimal or near-optimal solutions for all or at least most realistic instances;

(v) Efficiency: VNS takes a moderate computing time to generate optimal or near-optimal solutions;

(vi) Robustness: the functioning of the VNS is coherent over a variety of instances;

(vii) User friendliness: VNS has no parameters, so it is easy for understanding, expressing and using;

(viii) Innovation: VNS is generating new types of application.

(ix) Generality: VNS is inducing to good results for a wide variety of
problems;

(x) Interactivity: VNS allows the user to incorporate his knowledge to improve the resolution process;

(xi) Multiplicity: VNS is able to produce a certain near-optimal solutions from which the user can choose;

Interest in VNS is growing quickly, evidenced by the increasing number of papers published each year on this topic (10 years ago, only a few; 5 years ago, about a dozen; and about 50 in 2007).
Moreover, the 18th EURO mini-conference held in Tenerife in November 2005 was entirely devoted to VNS. It led to special issues of [[Institute of Mathematics and its Applications|IMA Journal of Management Mathematics]] in 2007, European Journal of Operational Research (http://www.journals.elsevier.com/european-journal-of-operational-research/), and Journal of Heuristics (http://www.springer.com/mathematics/applications/journal/10732/) in 2008.

== References ==
{{Reflist}}

== External links ==
* [http://toledo.mi.sanu.ac.rs/~grujicic/vnsconference EURO Mini Conference XXVIII on Variable Neighbourhood Search]

[[Category:Searching]]</text>
      <sha1>gt37pzc56kop2e1nrjs3iq91z0gygxq</sha1>
    </revision>
  </page>
  <page>
    <title>Search/Retrieve via URL</title>
    <ns>0</ns>
    <id>8465350</id>
    <revision>
      <id>546615566</id>
      <parentid>541504717</parentid>
      <timestamp>2013-03-23T21:36:59Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q337367]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="528">'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.

==See also==
* [[Search/Retrieve Web Service]]

==External links==
* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]

{{Internet search}}

{{DEFAULTSORT:Search Retrieve via URL}}
[[Category:Data search engines]]
[[Category:Searching]]
[[Category:Uniform resource locator]]

{{web-stub}}</text>
      <sha1>9lq02lhpxl153gt0whhn30ipw9jj9g1</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Search engine software</title>
    <ns>14</ns>
    <id>6521632</id>
    <revision>
      <id>586756126</id>
      <parentid>585206882</parentid>
      <timestamp>2013-12-19T07:30:03Z</timestamp>
      <contributor>
        <username>Codename Lisa</username>
        <id>16847332</id>
      </contributor>
      <comment>removed [[Category:System software]]; added [[Category:Utility software by type]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="151">[[Category:Searching]]
[[Category:Data search engines]]
[[Category:Utility software by type]]
[[Category:Marketing software]]
[[Category:Web software]]</text>
      <sha1>83w5e8x9a5kf51h7rfg41yzt35du9m5</sha1>
    </revision>
  </page>
  <page>
    <title>Locate (Unix)</title>
    <ns>0</ns>
    <id>3522125</id>
    <revision>
      <id>644214630</id>
      <parentid>643788961</parentid>
      <timestamp>2015-01-26T06:22:53Z</timestamp>
      <contributor>
        <username>ویرایشگر-1</username>
        <id>19433605</id>
      </contributor>
      <minor/>
      <comment>fix &quot;Daemon&quot; link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1882">{{lowercase}}
'''&lt;code&gt;locate&lt;/code&gt;''', a [[Unix]] utility first created in 1983,&lt;ref&gt;
Ref: [[Usenix]] ''';login:''', Vol 8, No 1, February/March, 1983, p. 8.
&lt;/ref&gt;
serves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by '''&lt;code&gt;updatedb&lt;/code&gt;''' or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than &lt;code&gt;[[find]]&lt;/code&gt;, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements (particularly on very large filesystems).

The GNU version forms a part of [[GNU Findutils]].

Some versions can also index network filesystems.

==mlocate==
mlocate is a locate/updatedb implementation.

[https://fedorahosted.org/mlocate/ mlocate site]

==References==
&lt;references/&gt;

==External links==
* {{man|1|locate|FreeBSD}}
* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]

Variants:
* {{wayback|url=http://slocate.trakker.ca/|title=slocate (Secure Locate)|date=20090204031919}}
** {{man|1|slocate|die.net}}
* [http://carolina.mff.cuni.cz/~trmac/blog/mlocate/ &lt;code&gt;mlocate&lt;/code&gt;] - faster updates
** {{man|1|locate|die.net|mlocate}}
* [http://rlocate.sourceforge.net/ rlocate] - always up-to-date
* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate
* [http://www.locate32.net/ Locate32 for Windows] Windows analog of GNU locate with GUI, released under GNU license

{{unix commands}}

[[Category:GNU Project software]]
[[Category:Unix file system-related software]]
[[Category:Searching]]

{{Unix-stub}}</text>
      <sha1>psky0bljzjmqqma5k3ianwm62eg0wp5</sha1>
    </revision>
  </page>
  <page>
    <title>Lookeen</title>
    <ns>0</ns>
    <id>28760509</id>
    <revision>
      <id>627064382</id>
      <parentid>627064133</parentid>
      <timestamp>2014-09-25T19:13:26Z</timestamp>
      <contributor>
        <username>Peteroe</username>
        <id>13043719</id>
      </contributor>
      <minor/>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5915">{{Infobox Software
| name = Lookeen
| screenshot =
| caption =
| developer = [[Axonic Informationssysteme GmbH]]
| latest_release_version = 8.3.1.5156
| latest_release_date = May 21, 2013
| latest_preview_version =
| latest_preview_date =
| operating_system = [[Microsoft Windows]]
| genre = [[Search Tool|Email search]]
| company_type   = Private (venture-backed)
| foundation     = 2006
| location       = [[Karlsruhe]], [[Germany]]
| key_people     = [[Martin Welker]], CEO&lt;br&gt;[[Peter Oehler]], COO
| industry       = Email Applications
| website = [http://www.lookeen.com www.lookeen.com]
}}
'''Lookeen''' is a business search [[Plug-in (computing)|add-on]] for [[Microsoft Outlook]], produced under shareware license. The program uses [[Apache Software Foundation|Apache]]'s search engine [[Lucene]] and helps searching for [[Computer file|files]], [[emails]], [[contacts]], [[Email attachment|attachements]] as well as [[desktop environment|desktop]] elements on [[personal computers]] as well as in large [[Terminal Server]] or [[Citrix]] environments.&lt;ref&gt;[http://email.about.com/od/outlookaddons/gr/lookeen.htm ''Lookeen 2010'']. Editor Review on about.com. Retrieved on August 22, 2014.&lt;/ref&gt;
==Using==
Lookeen is an add-on for Microsoft Outlook. The [[shareware]] program is developed according to the Microsoft company recommendation on the add-ons design. After installation the program automatically integrates into Microsoft Outlook workspace. After the indexing process, Lookeen easily allows to search whole [[Personal Storage Table|Outlook archives]] and the [[My Documents]] folder.&lt;ref&gt;[http://www.pcworld.com/article/233114/lookeen.html ''Lookeen'']. Editor Review on pcworld.com. Retrieved on August 22, 2014.&lt;/ref&gt;
In contrast to the Microsoft Outlook [[native (computing)|native]] search engine, Lookeen indexes the complete [[folder (computing)|folder]] structure. Whereas the native Outlook search only allows searches within the presently used and active folder, Lookeen searches in complete Outlook archives for needed information. 

===Supported mailbox storages===
Lookeen supports the following types of mail accounts: [[POP3]], [[IMAP]], [[HTTP]] and [[Microsoft Exchange Server]]. Both uncached and [[cache (computing)|cached]] exchange server modes are supported.
===Supported filetypes===
The following filetypes can be indexed and searched for with Lookeen (in alphabetical order]: [[.bmp]], [[.doc]], [[.docx]], [[.gif]], [[.htm]], [[.html]], [[.jpeg]], [[.jpg]], [[.msg]], [[.pdf]], [[.php]], [[.png]], [[.pps]], [[.ppsx]], [[.ppt]], [[.pptx]], [[.rtf]], [[.txt]], [[.tif]], [[.tiff]], [[.xls]], [[.xlsm]], [[.xlsx]], [[.xml]]. 
===Central indexing===
Lookeen 8 supports central indexing of shared resources (e.g. network files, public exchange folders). This shared index is created once and integrated by the clients via its URL. Goal is to reduce network- and server-traffic and reduce the index storage cost for local indexes.&lt;ref&gt;[http://www.techmynd.com/outlook-search-tool-lookeen-licenses-giveaway/ ''Excellent Outlook Search Tool – Lookeen'']. Editor Review on Techmynd.com. Retrieved on August 22, 2014.&lt;/ref&gt;
===Enterprise Roll-Out Support===
Lookeen 8 supports [[Group Policies]] for advanced software distributions in companies. Many options (e.g. index location, settings location, included sources, index intervals, license keys, etc.) can be defined by the administrator. That enables enterprises to use Lookeen in large [[Terminal Server]] or [[Citrix]] environments.&lt;ref&gt;[http://www.itwire.com/featured-news/54892-lookeen-8-accelerates-outlook-e-mail-search ''Lookeen 8 accelerates Outlook E-Mail-Search'']. Official Press Release on ITwire.com. Retrieved on August 22, 2014.&lt;/ref&gt;

==History==
Structure and Design of the first version strongly resembled the e-mail search software Lookout as developed by the [[Silicon Valley]] [[Startup company|Startup]] [[Lookout Software LCC]]. In 2004, Microsoft bought Lookout for allegedly 6 Million US-Dollars in order to integrate the search technology into its [[Windows Desktop Search]].&lt;ref&gt;[http://www.microsoft.com/presspass/press/2004/jul04/07-16lookoutpr.mspx ''MSN Announces Investment in Search Technology'']. Press Release on Microsoft.com. Retrieved on August 12, 2014.&lt;/ref&gt; Lookout continued being available as [[Freeware]], but was not compatible anymore with Microsoft Outlook with the Release of [[Microsoft Windows Vista]] in January 2007. 
In 2007, the German IT company [[Axonic Informationssysteme GmbH]] started working on a follow-up software for Lookout and finally released Lookeen in January 2008 as a professional solution for file and e-mail searches.&lt;ref&gt;[http://unternehmen.wikia.com/wiki/Axonic ''Company history of the creators of Lookeen'']. Official company registry entry on unternehmens.wikia.com. Retrieved on August 22, 2014.&lt;/ref&gt; Within eight months, Lookeen was then sold in more than 40 countries.

==Lookeen Server Enterprise Search==
In Juli 2011, a corresponding [[enterprise search]] version has been released. The [[Lookeen Server]] supports global indexing functions taking privacy and data security concerns into account by totally centralizing control options.&lt;ref&gt;[http://www.lookeen-server.com/en/product/overview ''Overview: Lookeen Server'']. From lookeen-server.com. Retrieved on August 20, 2014.&lt;/ref&gt; 

==See also==
* [[Comparison of enterprise search software]]
* [[List of enterprise search vendors]]
* [[List of Search Engines]]

==References==
&lt;references /&gt;
== External links ==
* [http://www.crunchbase.com/company/lookeen CrunchBase: Lookeen Profile]
* [http://www.lookeen.net Lookeen homepage]
* [http://www.lookeen-server.com Lookeen Server homepage]
* [http://www.axonic.net Creators of Lookeen]

[[Category:Shareware]]
[[Category:Software]]
[[Category:Microsoft Office-related software]]
[[Category:Desktop search engines]]
[[Category:Searching]]</text>
      <sha1>q43sz6cjkv6mem64j456x3qq7ljw1qo</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Concordances (publishing)</title>
    <ns>14</ns>
    <id>43967942</id>
    <revision>
      <id>641288927</id>
      <parentid>627367063</parentid>
      <timestamp>2015-01-06T20:00:53Z</timestamp>
      <contributor>
        <username>Damiens.rf</username>
        <id>6857411</id>
      </contributor>
      <comment>new cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="189">{{Cat main|Concordance (publishing)}}
[[Category:Biblical studies]]
[[Category:Indexing]]
[[Category:Linguistics]]
[[Category:Reference works]]
[[Category:Searching]]
[[Category:Hypertext]]</text>
      <sha1>stzb7gbb9b0e8l5hswv4a0mzrbo0iuo</sha1>
    </revision>
  </page>
  <page>
    <title>Daffodil (software)</title>
    <ns>0</ns>
    <id>3851301</id>
    <revision>
      <id>631868191</id>
      <parentid>631868144</parentid>
      <timestamp>2014-10-31T09:46:42Z</timestamp>
      <contributor>
        <username>Babitaarora</username>
        <id>18777500</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/122.252.233.106|122.252.233.106]] ([[User talk:122.252.233.106|talk]]): nonconstructive edits ([[WP:HG|HG]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="985">{{Orphan|date=February 2009}}
The '''Daffodil''' system is a virtual [[digital library]] system for strategic support of users during the information search process. It implements mainly high-level search functions, so-called stratagems, which provide functionality beyond today's digital libraries.  The Daffodil system was developed as a research project starting as a collaboration between the University of Dortmund (Germany) and the IZ Bonn (Germany), funded by the [[Deutsche Forschungsgemeinschaft]] (DFG) (2000–2004). 

Currently the Daffodil framework is extended to become an experimental evaluation platform for digital library evaluation at the [[University of Duisburg-Essen]].

== External links ==
* [http://www.dlib.org/dlib/june04/kriewel/06kriewel.html A description of functions and services]
* [http://www.is.informatik.uni-duisburg.de/projects/daffodil/index.html Project description]

[[Category:Library science]]
[[Category:Searching]]


{{Compu-library-stub}}</text>
      <sha1>hx5hayxwfctlh3bvvcfzm3qj6tdspik</sha1>
    </revision>
  </page>
  <page>
    <title>Search by sound</title>
    <ns>0</ns>
    <id>44579120</id>
    <revision>
      <id>641974239</id>
      <parentid>640524917</parentid>
      <timestamp>2015-01-11T06:05:36Z</timestamp>
      <contributor>
        <username>Niceguyedc</username>
        <id>5288432</id>
      </contributor>
      <minor/>
      <comment>[[:en:WP:CLEANER|WPCleaner]] v1.34 - Repaired 1 link to disambiguation page - [[WP:DPL|(You can help)]] - [[Tunes]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4434">Searching by sound for now has limited uses. There are a handful of applications, specifically for mobile devises that utilizes searching by sound. [[Shazam (service)]], [[Soundhound]], Midomi, and others has seen considerable success by using a simple algorithm to match an acoustic fingerprint to a song in a library. These applications takes a sample clip of a song, or a user generated melody and checks a music library to see where the clip matches with the song. From there, song information will be pulled up and displayed to the user. 

These kind of applications is mainly used for finding a song that the user does not already know. 

Searching by sound is not limited so just identifying [[songs]], but also for identifying [[melodies]], [[Music|tunes]] or [[advertisements]], [[sound library management]] and [[video files]].

==Acoustic Fingerprinting==
The way these apps search by sound is through generating an acoustic fingerprint; a digital summary of the sound. A microphone is used to pick up an audio sample, which is then broken down into a simple numeric signature, a code unique to each track. Using the same method of fingerprinting sounds, when Shazam picks up a sound clip, it will generate a signature for that clip. Then it’s simple pattern matching from there using an extensive audio music database. 

The practice of using [[acoustic fingerprints]] is not limited to just music however, but other areas of the entertainment business as well. Shazam also can identify television shows with the same technique of acoustic fingerprinting. Of course, this method of breaking down a sound sample into a unique signature is useless unless there is an extensive database of music with keys to match with the samples. Shazam has over 11 million songs in its database. &lt;ref&gt; http://www.slate.com/articles/technology/technology/2009/10/that_tune_named.html &lt;/ref&gt;

Other services such as Midomi and Soundhound allow users to add to that library of music in order to expand the chances to match a sound sample with its corresponding sound. 

==Spectogram==
Generating a signature from the song is essential for searching by sound, and can be tricky. However, the way certain applications such as Shazam found a way around this issue by creating a spectrogram. 

Any piece of music can be translated to a time frequency graph called a spectrogram. For each song in its database, each song is basically a graph that plots the three dimensions of music, frequency vs amplitude (intensity) vs time. The algorithm then picks out the points which peaks in the graph, labeled as “higher energy content”. In practice, this seems to work out to about three points per song. &lt;ref&gt; http://www.soyoucode.com/2011/how-does-shazam-recognize-song &lt;/ref&gt;

This is how a song can be identified with just two or three notes. This greatly reduces the impact that [[background noise]] has on searching by sound. The key values taken away from this would be frequency in hertz and time in seconds. Shazam builds their fingerprint catalog out as a hash table, where the key is the frequency. They do not just mark a single point in the spectrogram, rather they mark a pair of points: the “peak intensity” plus a second “anchor point”. &lt;ref&gt; Li-Chun Wang, Avery. &quot;An Industrial-Strength Audio Search Algorithm.&quot; Columbia University. Web. 1 Dec. 2014. &lt;http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf&gt;. &lt;/ref&gt; So their key is not just a single frequency, it is a hash of the frequencies of both points.  This leads to less hash collisions which in turn speeds up catalog searching by several orders of magnitude by allowing them to take greater advantage of the table’s constant (O(1)) look-up time. &lt;ref&gt; &quot;How Shazam Works.&quot; Free Wont. Web. 1 Dec. 2014. &lt;http://laplacian.wordpress.com/2009/01/10/how-shazam-works/&gt;. &lt;/ref&gt;

This method of acoustic fingerprinting allows applications such as Shazam to have the ability to differentiate between two closely related covers, as well as not having to account for popularity of a certain song. 

==Query by Humming==
Midomi and Soundhound both utilize Query by Humming, or QbH. This is a branch off of acoustic fingerprints, but is still a musical retrieval system. After receiving a user generated hummed melody, which is the input query, and returns a ranked list of songs that is closest to the user query. 

==References==
{{reflist}}




[[Category:Searching]]</text>
      <sha1>4z5heehoffuu34qxi6059shpkkek62y</sha1>
    </revision>
  </page>
  <page>
    <title>Hamming distance</title>
    <ns>0</ns>
    <id>41227</id>
    <revision>
      <id>641250506</id>
      <parentid>639652386</parentid>
      <timestamp>2015-01-06T15:06:16Z</timestamp>
      <contributor>
        <username>Olof nord</username>
        <id>11614128</id>
      </contributor>
      <minor/>
      <comment>made &quot;rook&quot; a wikilink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7646">{| align=&quot;right&quot;
|-
| [[Image:Hamming distance 3 bit binary.svg|thumb|140px|3-bit binary [[cube]] for finding Hamming distance]]
| [[Image:Hamming distance 3 bit binary example.svg|thumb|140px|Two example distances: 100→011 has distance 3 (red path); 010→111 has distance 2 (blue path)]]
|-
|colspan=2 | [[Image:Hamming distance 4 bit binary.svg|thumb|280px|4-bit binary [[tesseract]] for finding Hamming distance]]
|-
|colspan=2 | [[Image:Hamming distance 4 bit binary example.svg|thumb|280px|Two example distances: 0100→1001 has distance 3 (red path); 0110→1110 has distance 1 (blue path)]]
|}

In [[information theory]], the '''Hamming distance''' between two [[String (computer science)|string]]s of equal length is the number of positions at which the corresponding symbols are different. In another way, it measures the minimum number of ''substitutions'' required to change one string into the other, or the minimum number of ''errors'' that could have transformed one string into the other.

==Examples==
The Hamming distance between:
* &quot;'''&lt;/span&gt;ka&lt;span style=&quot;color:#0082ff&quot;&gt;rol&lt;/span&gt;in&lt;/span&gt;'''&quot; and &quot;'''&lt;/span&gt;ka&lt;span style=&quot;color:red;&quot;&gt;thr&lt;/span&gt;in&lt;/span&gt;'''&quot; is 3.
* &quot;'''&lt;/span&gt;k&lt;span style=&quot;color:#0082ff&quot;&gt;a&lt;/span&gt;r&lt;span style=&quot;color:#0082ff&quot;&gt;ol&lt;/span&gt;in&lt;/span&gt;'''&quot; and &quot;'''&lt;/span&gt;k&lt;span style=&quot;color:red;&quot;&gt;e&lt;/span&gt;r&lt;span style=&quot;color:red;&quot;&gt;st&lt;/span&gt;in&lt;/span&gt;'''&quot; is 3.
* '''10&lt;span style=&quot;color:#0082ff&quot;&gt;1&lt;/span&gt;1&lt;span style=&quot;color:#0082ff&quot;&gt;1&lt;/span&gt;01''' and '''10&lt;span style=&quot;color:red;&quot;&gt;0&lt;/span&gt;1&lt;span style=&quot;color:red;&quot;&gt;0&lt;/span&gt;01''' is 2.
* '''2&lt;span style=&quot;color:#0082ff&quot;&gt;17&lt;/span&gt;3&lt;span style=&quot;color:#0082ff&quot;&gt;8&lt;/span&gt;96''' and '''2&lt;span style=&quot;color:red;&quot;&gt;23&lt;/span&gt;3&lt;span style=&quot;color:red;&quot;&gt;7&lt;/span&gt;96''' is 3.

==Special properties==
For a fixed length ''n'', the Hamming distance is a [[Metric (mathematics)|metric]] on the vector space of the words of length n, as it fulfills the conditions of non-negativity, identity of indiscernibles and symmetry, and it can be shown by [[complete induction]] that it satisfies the [[triangle inequality]] as well. The Hamming distance between two words ''a'' and ''b'' can also be seen as the [[Hamming weight]] of ''a''&amp;minus;''b'' for an appropriate choice of the &amp;minus; operator.

For '''binary strings''' ''a'' and ''b'' the Hamming distance is equal to the number of ones ([[Hamming weight|population count]]) in ''a'' [[Exclusive or|XOR]] ''b''. The metric space of length-''n'' binary strings, with the Hamming distance, is known as the ''Hamming cube''; it is equivalent as a metric space to the set of distances between vertices in a [[hypercube graph]]. One can also view a binary string of length ''n'' as a vector in &lt;math&gt;R^n&lt;/math&gt; by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an ''n''-dimensional [[hypercube]], and the Hamming distance of the strings is equivalent to the [[Manhattan distance]] between the vertices.

==History and applications==

The Hamming distance is named after [[Richard Hamming]], who introduced it in his fundamental paper on [[Hamming code]]s ''Error detecting and error correcting codes'' in 1950.&lt;ref&gt;{{harvtxt|Hamming|1950}}.&lt;/ref&gt; It is used in [[telecommunication]] to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the '''signal distance'''. Hamming weight analysis of bits is used in several disciplines including [[information theory]], [[coding theory]], and [[cryptography]]. However, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the [[Levenshtein distance]] is more appropriate.
For ''q''-ary strings over an [[alphabet]] of size ''q''&amp;nbsp;≥&amp;nbsp;2 the Hamming distance is applied in case of orthogonal [[modulation]], while the [[Lee distance]] is used for phase modulation. If ''q''&amp;nbsp;=&amp;nbsp;2 or ''q''&amp;nbsp;=&amp;nbsp;3 both distances coincide.

The Hamming distance is also used in [[systematics]] as a measure of genetic distance.&lt;ref name=&quot;pmid18351799&quot;&gt;{{harvtxt|Pilcher|Wong|Pillai|2008}}.&lt;/ref&gt;

On a grid such as a chessboard, the Hamming distance is the minimum number of moves it would take a [[Rook_(chess)|rook]] to move from one cell to the other.

== Algorithm example ==
The [[Python (programming language)|Python]] function &lt;code&gt;hamming_distance()&lt;/code&gt; computes the Hamming distance between
two strings (or other [[Iterator|iterable]] objects) of equal length, by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs, and then summing the sequence with False and True values being interpreted as zero and one.
{{-}}

&lt;syntaxhighlight lang=&quot;python&quot;&gt;
def hamming_distance(s1, s2):
    &quot;&quot;&quot;Return the Hamming distance between equal-length sequences&quot;&quot;&quot;
    if len(s1) != len(s2):
        raise ValueError(&quot;Undefined for sequences of unequal length&quot;)
    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))
&lt;/syntaxhighlight&gt;

The following [[C (programming language)|C]] function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the [[bitwise operation|bitwise]] [[exclusive or]] of the two inputs, and then finds the [[Hamming weight]] of the result (the number of nonzero bits) using an algorithm of {{harvtxt|Wegner|1960}} that repeatedly finds and clears the lowest-order nonzero bit.

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
int hamming_distance(unsigned x, unsigned y)
{
    int       dist;
    unsigned  val;

    dist = 0;
    val = x ^ y;    // XOR

    // Count the number of bits set
    while (val != 0)
    {
        // A bit is set, so increment the count and clear the bit
        dist++;
        val &amp;= val - 1;
    }

    // Return the number of differing bits
    return dist;
}
&lt;/syntaxhighlight&gt;

==See also==
{{Portal|Mathematics}}
* [[Closest string]]
* [[Damerau–Levenshtein distance]]
* [[Euclidean distance]]
* [[Mahalanobis distance]]
* [[Jaccard index]]
* [[String metric]]
* [[Sørensen similarity index]]
* [[Word ladder]]

==Notes==
{{Reflist}}

==References==
*{{FS1037C}}
*{{citation
 | last = Hamming | first = Richard W. | author-link = Richard W. Hamming
 | mr = 0035935
 | issue = 2
 | journal = [[Bell System Technical Journal]]
 | pages = 147–160
 | title = Error detecting and error correcting codes
 | url = http://wayback.archive.org/web/20060525060427/http://www.caip.rutgers.edu/~bushnell/dsdwebsite/hamming.pdf
 | volume = 29
 | year = 1950
 | doi=10.1002/j.1538-7305.1950.tb00463.x}}.
*{{citation
 | last1 = Pilcher | first1 = C. D.
 | last2 = Wong | first2 = J. K.
 | last3 = Pillai | first3 = S. K.
 | date = March 2008
 | doi = 10.1371/journal.pmed.0050069
 | issue = 3
 | journal = PLoS Med.
 | page = e69
 | pmid = 18351799
 | title = Inferring HIV transmission dynamics from phylogenetic sequence relationships
 | volume = 5
 | pmc = 2267810}}.
*{{citation
 | last = Wegner | first = Peter | author-link = Peter Wegner
 | doi = 10.1145/367236.367286
 | issue = 5
 | journal = [[Communications of the ACM]]
 | page = 322
 | title = A technique for counting ones in a binary computer
 | volume = 3
 | year = 1960}}.

[[Category:String similarity measures]]
[[Category:Coding theory]]
[[Category:Articles with example Python code]]
[[Category:Articles with example C++ code]]
[[Category:Metric geometry]]
[[Category:Cubes]]</text>
      <sha1>hlzhudxzyp3fe3wmh0ad00rv2tn679d</sha1>
    </revision>
  </page>
  <page>
    <title>Inversion (discrete mathematics)</title>
    <ns>0</ns>
    <id>21473801</id>
    <revision>
      <id>615613386</id>
      <parentid>611737486</parentid>
      <timestamp>2014-07-04T19:53:38Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>[[Petra Mutzel]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8120">{{contradict-other-multiple|Permutation|Lehmer code|Factorial number system|date=March 2013}}
[[File:Inversion set and vector of a permutation.svg|thumb|right|380px|The permutation (4,1,5,2,6,3) has the inversion vector (0,1,0,2,0,3) and the inversion set {(1,2),(1,4),(3,4),(1,6),(3,6),(5,6)}. The inversion vector [[w:Factorial number system|converted]] to decimal is 373.]]
[[File:Inversion set 16; wp(13,11, 7,15).svg|thumb|250px|Inversion set of the permutation&lt;br&gt;(0,15, 14,1, 13,2, 3,12,&lt;br&gt;11,4, 5,10, 6,9, 8,7)&lt;br&gt;showing the pattern of the&lt;br&gt;[[Thue–Morse sequence]]]]
In [[computer science]] and [[discrete mathematics]], an '''inversion''' is a pair of places of a sequence where the elements on these places are out of their natural [[total order|order]].

== Definitions ==

Formally, let &lt;math&gt;(A(1), \ldots, A(n))&lt;/math&gt; be a sequence of ''n'' distinct numbers.  If &lt;math&gt;i &lt; j&lt;/math&gt; and &lt;math&gt;A(i) &gt; A(j)&lt;/math&gt;, then the pair &lt;math&gt;(i, j)&lt;/math&gt; is called an inversion of &lt;math&gt;A&lt;/math&gt;.{{sfn|Cormen|Leiserson|Rivest|Stein|2001|pp=39}}{{sfn|Vitter|Flajolet|1990|pp=459}}

The '''inversion number''' of a sequence is one common measure of its sortedness.{{sfn|Barth|Mutzel|2004|pp=183}}{{sfn|Vitter|Flajolet|1990|pp=459}}  Formally, the inversion number is defined to be the number of inversions, that is, 
:&lt;math&gt;\text{inv}(A) = \# \{(A(i),A(j)) \mid i &lt; j \text{ and } A(i) &gt; A(j)\}&lt;/math&gt;.{{sfn|Barth|Mutzel|2004|pp=183}}  
Other measures of (pre-)sortedness include the minimum number of elements that can be deleted from the sequence to yield a fully sorted sequence, the number and lengths of sorted &quot;runs&quot; within the sequence, and the smallest number of exchanges needed to sort the sequence.{{sfn|Mahmoud|2000|pp=284}} Standard [[comparison sort]]ing algorithms can be adapted to compute the inversion number in time {{math|O(''n'' log ''n'')}}.

The '''inversion vector''' ''V(i)'' of the sequence is defined for ''i'' = 2, ..., ''n'' as &lt;math&gt;V[i] = \left\vert\{k \mid k &lt; i \text{ and } A(k) &gt; A(i)\}\right\vert&lt;/math&gt;.  In other words each element is the number of elements preceding the element in the original sequence that are greater than it.  Note that the inversion vector of a sequence has one less element than the sequence, because of course the number of preceding elements that are greater than the first is always zero.  Each permutation of a sequence has a unique inversion vector and it is possible to construct any given permutation of a (fully sorted) sequence from that sequence and the permutation's inversion vector.{{sfn|Pemmaraju|Skiena|2003|pp=69}}

==Weak order of permutations==
The set of permutations on ''n'' items can be given the structure of a [[partial order]], called the '''weak order of permutations''', which forms a [[lattice (order)|lattice]].

To define this order, consider the items being permuted to be the integers from 1 to ''n'', and let Inv(''u'') denote the set of inversions of a permutation ''u'' for the natural ordering on these items. That is, Inv(''u'') is the set of ordered pairs (''i'', ''j'') such that 1 ≤ ''i'' &lt; ''j'' ≤ ''n'' and ''u''(''i'') &gt; ''u''(''j''). Then, in the weak order, we define ''u'' ≤ ''v'' whenever Inv(''u'') ⊆ Inv(''v'').

The edges of the [[Hasse diagram]] of the weak order are given by permutations ''u'' and ''v'' such that ''u &lt; v'' and such that ''v'' is obtained from ''u'' by interchanging two consecutive values of ''u''. These edges form a [[Cayley graph]] for the [[symmetric group|group of permutations]] that is isomorphic to the [[skeleton (topology)|skeleton]] of a [[permutohedron]].

The identity permutation is the minimum element of the weak order, and the permutation formed by reversing the identity is the maximum element.

== See also ==
{{wikiversity|Inversion (discrete mathematics)}}
{{commons|Category:Inversion (discrete mathematics)|Inversion (discrete mathematics)}}
* [[Factorial number system]] (a factorial number is a reflected inversion vector)
* [[Permutation group#Transpositions, simple transpositions, inversions and sorting|Transpositions, simple transpositions, inversions and sorting]]
* [[Damerau–Levenshtein distance]]
* [[Parity of a permutation]]

'''Sequences in the [[On-Line Encyclopedia of Integer Sequences|OEIS]]:'''
* [https://oeis.org/wiki/Index_to_OEIS:_Section_Fa#factorial Index entries for sequences related to factorial numbers]
* Reflected inversion vectors: {{OEIS link|A007623}} and {{OEIS link|A108731}}
* Sum of inversion vectors, cardinality of inversion sets: {{OEIS link|A034968}}
* Inversion sets of finite permutations interpreted as binary numbers: {{OEIS link|A211362}} &amp;nbsp; (related permutation: {{OEIS link|A211363}})
* Finite permutations that have only 0s and 1s in their inversion vectors: {{OEIS link|A059590}} &amp;nbsp; (their inversion sets: {{OEIS link|A211364}})
* Numbers of permutations of n elements with k inversions; Mahonian numbers: {{OEIS link|A008302}} &amp;nbsp; (their row maxima; Kendall-Mann numbers: {{OEIS link|A000140}})
* Number of connected labeled graphs with n edges and n nodes: {{OEIS link|A057500}}
* Arrays of permutations with similar inversion sets and inversion vectors: {{OEIS link|A211365}}, {{OEIS link|A211366}}, {{OEIS link|A211367}}, {{OEIS link|A211368}}, {{OEIS link|A211369}}, {{OEIS link|A100630}}, {{OEIS link|A211370}}, {{OEIS link|A051683}}

== References ==
{{reflist|4|refs=}}

=== Source bibliography ===
{{refbegin|1}}
* {{cite journal|ref=harv|first1=Wilhelm|last1=Barth|first2=Petra|last2=Mutzel|author2-link=Petra Mutzel|title=Simple and Efficient Bilayer Cross Counting|journal=[[Journal of Graph Algorithms and Applications]]|volume=8|issue=2|pages=179&amp;ndash;194|year=2004|doi=10.7155/jgaa.00088}}
* {{cite book|ref=harv
 | first1=Thomas H.|last1=Cormen|authorlink1=Thomas H. Cormen
 | last2=Leiserson|first2=Charles E.|authorlink2=Charles E. Leiserson
 | last3=Rivest|first3=Ronald L.|authorlink3=Ron Rivest
 | last4=Stein|first4=Clifford|authorlink4=Clifford Stein
 | title = [[Introduction to Algorithms]]
 | publisher = MIT Press and McGraw-Hill
 | year = 2001
 | isbn = 0-262-53196-8
 | edition = 2nd
 }}
* {{cite book|ref=harv|title=Sorting: a distribution theory|chapter=Sorting Nonrandom Data|volume=54|series=Wiley-Interscience series in discrete mathematics and optimization|first=Hosam Mahmoud|last=Mahmoud|publisher=Wiley-IEEE|year=2000|isbn=978-0-471-32710-3}}
* {{cite book|ref=harv|title=Computational discrete mathematics: combinatorics and graph theory with Mathematica|chapter=Permutations and combinations|first1=Sriram V.|last1=Pemmaraju|first2=Steven S.|last2=Skiena|publisher=Cambridge University Press|year=2003|isbn=978-0-521-80686-2}}
* {{cite book|ref=harv|title=Algorithms and Complexity|volume=1|editor1-first=Jan|editor1-last=van Leeuwen|editor1-link=Jan van Leeuwen|edition=2nd|publisher=Elsevier|year=1990|isbn=978-0-444-88071-0|chapter=Average-Case Analysis of Algorithms and Data Structures|first1=J.S.|last1=Vitter|first2=Ph.|last2=Flajolet}}
{{refend}}

=== Further reading ===
* {{cite journal|ref=harv|journal=Journal of Integer Sequences|volume=4|year=2001|title=Permutations with Inversions|first=Barbara H.|last=Margolius}}

=== Presortedness measures ===
* {{cite journal|ref=harv|journal=Lecture Notes in Computer Science|year=1984|volume=172|pages=324&amp;ndash;336|doi=10.1007/3-540-13345-3_29|title=Measures of presortedness and optimal sorting algorithms|first=Heikki|last=Mannila|authorlink=Heikki Mannila}}
* {{cite journal|ref=harv|first1=Vladimir|last1=Estivill-Castro|first2=Derick|last2=Wood|title=A new measure of presortedness|journal=Information and Computation|volume=83|issue=1|pages=111&amp;ndash;119|year=1989|doi=10.1016/0890-5401(89)90050-3}}
* {{cite journal|ref=harv|first=Steven S.|last=Skiena|year=1988|title=Encroaching lists as a measure of presortedness|journal=BIT|volume=28|issue=4|pages=755&amp;ndash;784|doi=10.1007/bf01954897}}

[[Category:Permutations]]
[[Category:Order theory]]
[[Category:String similarity measures]]
[[Category:Sorting algorithms]]
[[Category:Combinatorics]]
[[Category:Discrete mathematics]]</text>
      <sha1>ew0cpm1gy4lz4no6bx2skz7luyhqsmw</sha1>
    </revision>
  </page>
  <page>
    <title>Lee distance</title>
    <ns>0</ns>
    <id>17766039</id>
    <revision>
      <id>548314480</id>
      <parentid>548314430</parentid>
      <timestamp>2013-04-02T13:17:42Z</timestamp>
      <contributor>
        <ip>94.153.230.50</ip>
      </contributor>
      <comment>for q=3 it doesn't</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1647">{{No footnotes|date=July 2011}}
In [[coding theory]], the '''Lee distance''' is a [[distance]] between two [[String (computer science)|string]]s &lt;math&gt;x_1 x_2 \dots x_n&lt;/math&gt; and &lt;math&gt;y_1 y_2 \dots y_n&lt;/math&gt; of equal length ''n'' over the ''q''-ary [[alphabet]] {0,&amp;nbsp;1,&amp;nbsp;…,&amp;nbsp;''q''&amp;nbsp;&amp;minus;&amp;nbsp;1} of size ''q''&amp;nbsp;≥&amp;nbsp;2.
It is a [[Metric (mathematics)|metric]], defined as

: &lt;math&gt;\sum_{i=1}^n \min(|x_i-y_i|,q-|x_i-y_i|).&lt;/math&gt;

If ''q''&amp;nbsp;=&amp;nbsp;2 the Lee distance coincides with the [[Hamming distance]].

The [[metric space]] induced by the Lee distance is a discrete analog of the [[Elliptic geometry|elliptic space]].

==Example==
If ''q''&amp;nbsp;=&amp;nbsp;6, then the Lee distance between 3140 and 2543 is 1&amp;nbsp;+&amp;nbsp;2&amp;nbsp;+&amp;nbsp;0&amp;nbsp;+&amp;nbsp;3&amp;nbsp;=&amp;nbsp;6.

==History and application==
The Lee distance is named after [[C. Y. Lee (mathematician)|C. Y. Lee]]. It is applied for phase [[modulation]] while the Hamming distance is used in case of orthogonal modulation.

==References==
* {{Citation|first=C. Y.|last=Lee|title=Some properties of nonbinary [[error-correcting codes]]|journal=[[IEEE Transactions on Information Theory|IRE Transactions on Information Theory]]|volume=4|year=1958|pages=77–82|issue=2|doi=10.1109/TIT.1958.1057446}}.
* {{Citation|first=E. R.|last=Berlekamp|authorlink=Elwyn Berlekamp|title=Algebraic Coding Theory|publisher=McGraw-Hill|year=1968}}.
* {{Citation|last1=Deza|first1=E.|first2=M.|last2=Deza|author2-link=Michel Deza|title=Dictionary of Distances|year=2006|publisher=Elsevier|isbn=0-444-52087-2}}.

[[Category:Coding theory]]
[[Category:String similarity measures]]</text>
      <sha1>20x37quaqb7j8k8l135ok84nfkgfkg8</sha1>
    </revision>
  </page>
  <page>
    <title>Tversky index</title>
    <ns>0</ns>
    <id>29753359</id>
    <revision>
      <id>570239140</id>
      <parentid>564355552</parentid>
      <timestamp>2013-08-26T09:05:41Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>Tversky index has nothing to do with strings or IR per se (see the original paper)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2710">The '''Tversky index''', named after [[Amos Tversky]],&lt;ref&gt;{{cite journal |last=Tversky |first=Amos |title=Features of Similarity |journal=Psychological Reviews |volume=84 |number=4 |year=1977 |pages=327–352 |url=http://www.cogsci.ucsd.edu/~coulson/203/tversky-features.pdf}}&lt;/ref&gt; is an asymmetric [[similarity measure]] on [[set theory|sets]] that compares a variant to a prototype. The Tversky index can be seen as a generalization of [[Dice's coefficient]] and [[Tanimoto coefficient]].

For sets ''X'' and ''Y'' the Tversky index is a number between 0 and 1 given by 

&lt;math&gt;S(X, Y) = \frac{| X \cap Y |}{| X \cap Y | + \alpha | X - Y | + \beta | Y - X |} &lt;/math&gt;,

Here, &lt;math&gt;X - Y&lt;/math&gt; denotes the  [[Complement (set theory)| relative complement ]] of Y in X.

Further, &lt;math&gt;\alpha, \beta \ge 0 &lt;/math&gt; are parameters of the Tversky index.  Setting &lt;math&gt;\alpha = \beta = 1 &lt;/math&gt; produces the Tanimoto coefficient; setting &lt;math&gt;\alpha = \beta = 0.5 &lt;/math&gt; produces Dice's coefficient. 

If we consider ''X'' to be the prototype and ''Y'' to be the variant, then &lt;math&gt;\alpha&lt;/math&gt; corresponds to the weight of the prototype and &lt;math&gt;\beta&lt;/math&gt; corresponds to the weight of the variant. Tversky measures with &lt;math&gt;\alpha + \beta = 1&lt;/math&gt; are of special interest.&lt;ref&gt;http://www.daylight.com/dayhtml/doc/theory/theory.finger.html&lt;/ref&gt;

Because of the inherent asymmetry, the Tversky index does not meet the criteria for a similarity metric. However, if symmetry is needed a variant of the original formulation has been proposed using '''max''' and '''min''' functions &lt;ref&gt;Jimenez, S., Becerra, C., Gelbukh, A. [http://aclweb.org/anthology/S/S13/S13-1028.pdf SOFTCARDINALITY-CORE: Improving Text Overlap with Distributional Measures for Semantic Textual Similarity]. Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, p.194-201, June 7–8, 2013, Atlanta, Georgia, USA.&lt;/ref&gt;
.

&lt;math&gt;S(X,Y)=\frac{| X \cap Y |}{| X \cap Y |+\beta\left(\alpha a+(1-\alpha)b\right)}&lt;/math&gt;,

&lt;math&gt;a=\min\left(|X-Y|,|Y-X|\right) &lt;/math&gt;,

&lt;math&gt;b=\max\left(|X-Y|,|Y-X|\right) &lt;/math&gt;,

This formulation also re-arranges parameters &lt;math&gt;\alpha &lt;/math&gt; and &lt;math&gt;\beta &lt;/math&gt;. Thus, &lt;math&gt; \alpha &lt;/math&gt; controls the balance between &lt;math&gt; |X - Y| &lt;/math&gt; and &lt;math&gt; |Y - X| &lt;/math&gt; in the denominator. Similarly, &lt;math&gt;\beta&lt;/math&gt; controls the effect of the symmetric difference &lt;math&gt; |X\,\triangle\,Y\,| &lt;/math&gt; versus &lt;math&gt; | X \cap Y | &lt;/math&gt; in the denominator.

==Notes==
{{reflist}}

[[Category:Index numbers]]
[[Category:String similarity measures]]
[[Category:Measure theory]]</text>
      <sha1>nqcqhofi5nk0ywbv5u42lw9dfu582it</sha1>
    </revision>
  </page>
  <page>
    <title>String metric</title>
    <ns>0</ns>
    <id>9809306</id>
    <revision>
      <id>645766344</id>
      <parentid>632540134</parentid>
      <timestamp>2015-02-05T16:20:26Z</timestamp>
      <contributor>
        <ip>166.49.210.62</ip>
      </contributor>
      <comment>/* List of string metrics */  added link to article for dice similarity</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5051">{{redirect|String distance|the distance between strings and the fingerboard in musical instruments|Action (music)}}

In [[mathematics]] and [[computer science]], a '''string metric''' (also known as a '''string similarity metric''' or '''string distance function''') is a [[metric (mathematics)|metric]] that measures [[distance]] (&quot;inverse similarity&quot;) between two [[string (computer science)|text strings]] for [[approximate string  matching]] or comparison and in [[approximate string  matching|fuzzy string searching]]. Necessary requirement for a string ''metric'' (e.g. in contrast to [[string matching]]) is fulfillment of the [[triangle inequality]]. For example the strings &quot;Sam&quot; and &quot;Samuel&quot; can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.

The most widely known string metric is a rudimentary one called the [[Levenshtein distance|Levenshtein Distance]] (also known as Edit Distance).  It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Simplistic string metrics such as [[Levenshtein distance]] have expanded to include phonetic, [[token (parser)|token]], grammatical and character-based methods of statistical comparisons.

A widespread example of a string metric is [[DNA]] [[sequence analysis]] and RNA analysis, which are performed by optimized string metrics to identify matching sequences.

String metrics are used heavily in [[information integration]] and are currently used in areas including [[Data analysis techniques for fraud detection|fraud detection]], [[fingerprint analysis]], [[plagiarism detection]], [[ontology merging]], [[DNA analysis]], RNA analysis, [[image analysis]], evidence-based machine learning, [[database]] [[data deduplication]], [[data mining]], Web interfaces, e.g. [[Ajax (programming)|Ajax]]-style suggestions as you type, [[data integration]], and semantic [[knowledge integration]].

==List of string metrics==

&lt;!-- This can be a separate article, someday. --&gt;
* [[Sørensen–Dice coefficient]]
* [[Hamming distance]]
* [[Levenshtein distance]] and [[Damerau–Levenshtein distance]]
* [[Block distance]] or [[L1 distance]] or [[City block distance]]
* [[Simple matching coefficient]] (SMC)
* [[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]
* [[Most frequent k characters]]
* [[Tversky index]]
* [[Overlap coefficient]]
* [[Variational distance]]
* [[Hellinger distance]] or [[Bhattacharyya distance]]
* [[Information radius]] ([[Jensen–Shannon divergence]])
* [[Skew divergence]]
* [[Confusion probability]]
* [[Kendall_tau_distance|Tau metric]], an approximation of the [[Kullback–Leibler divergence]]
* [[Fellegi and Sunters metric]] (SFS)
* [[Maximal matches]]
* [[Lee distance]]

==Selected string measures examples==

{| class=&quot;wikitable&quot;
|-
! Name
! Example
|-
|[[Hamming distance]]
| &quot;'''&lt;/span&gt;ka&lt;span style=&quot;color:#0082ff&quot;&gt;rol&lt;/span&gt;in&lt;/span&gt;'''&quot; and &quot;'''&lt;/span&gt;ka&lt;span style=&quot;color:red;&quot;&gt;thr&lt;/span&gt;in&lt;/span&gt;'''&quot; is 3.
|-
|[[Levenshtein distance]] and [[Damerau–Levenshtein distance]]
| 
# '''k'''itten → '''s'''itten (substitution of &quot;s&quot; for &quot;k&quot;)
# sitt'''e'''n → sitt'''i'''n (substitution of &quot;i&quot; for &quot;e&quot;)
# sittin → sittin'''g''' (insertion of &quot;g&quot; at the end).
&lt;!--|-
|[[Simple matching coefficient]] (SMC)
|--&gt;
&lt;!--|-
|-
|[[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]
|--&gt;
|-
|[[Most frequent k characters]]
|MostFreqKeySimilarity('&lt;span style=&quot;color:red;&quot;&gt;r&lt;/span&gt;&lt;span style=&quot;color:#0082ff&quot;&gt;e&lt;/span&gt;s&lt;span style=&quot;color:#0082ff&quot;&gt;e&lt;/span&gt;a&lt;span style=&quot;color:red;&quot;&gt;r&lt;/span&gt;ch', 's&lt;span style=&quot;color:#0082ff&quot;&gt;ee&lt;/span&gt;king', 2) = 2
&lt;!--|-
|[[Tversky index]]
|--&gt;
&lt;!--|-
|[[Overlap coefficient]]
|--&gt;
&lt;!--|-
|[[Variational distance]]
|--&gt;
&lt;!--|-
|[[Hellinger distance]] or [[Bhattacharyya distance]]
|--&gt;
&lt;!--|-
|[[Information radius]] ([[Jensen–Shannon divergence]])
|--&gt;
&lt;!--|-
|[[Skew divergence]]
|--&gt;
&lt;!--|-
|[[Confusion probability]]
|--&gt;
&lt;!--|-
|[[Tau metric]], an approximation of the [[Kullback–Leibler divergence]]
|--&gt;
&lt;!--|-
|[[Fellegi and Sunters metric]] (SFS)
|--&gt;
&lt;!--|-
|[[Maximal matches]]
|--&gt;
|}

==See also==
* [[approximate string  matching]]
* [[String matching]]
* [http://www.speech.cs.cmu.edu/ Carnegie Mellon University open source library]
* [http://rockymadden.com/stringmetric/ StringMetric project] a [[Scala programming language|Scala]] library of string metrics and phonetic algorithms
* [https://github.com/NaturalNode/natural Natural project] a [[JavaScript]] natural language processing library which includes implementations of popular string metrics

==External links==
*http://www.dcs.shef.ac.uk/~sam/stringmetrics.html {{Dead link|date=July 2011}} A fairly complete overview {{wayback|url=http://www.dcs.shef.ac.uk/~sam/stringmetrics.html#ukkonen}}

{{DEFAULTSORT:String Metric}}
[[Category:String similarity measures| ]]
[[Category:Metrics]]

[[de:Ähnlichkeitsanalyse]]</text>
      <sha1>p486nxokt17opj64yz0gevg5enj2f3r</sha1>
    </revision>
  </page>
  <page>
    <title>Jaccard index</title>
    <ns>0</ns>
    <id>2203756</id>
    <revision>
      <id>634979038</id>
      <parentid>634978944</parentid>
      <timestamp>2014-11-22T16:36:34Z</timestamp>
      <contributor>
        <ip>178.38.76.15</ip>
      </contributor>
      <comment>/* Other definitions of Tanimoto distance */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12549">The '''Jaccard index''', also known as the '''Jaccard similarity coefficient''' (originally coined ''coefficient de communauté'' by [[Paul Jaccard]]), is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the size of the [[Union (set theory)|union]] of the sample sets:

:&lt;math&gt; J(A,B) = {{|A \cap B|}\over{|A \cup B|}}.&lt;/math&gt;

(If ''A'' and ''B'' are both empty, we define ''J''(''A'',''B'')&amp;nbsp;=&amp;nbsp;1.) Clearly, 
:&lt;math&gt; 0\le J(A,B)\le 1.&lt;/math&gt;

The [[MinHash]] min-wise independent permutations [[locality sensitive hashing]] scheme may be used to efficiently compute an accurate estimate of the Jaccard similarity coefficient of pairs of sets, where each set is represented by a constant-sized signature derived from the minimum values of a [[hash function]].

The '''Jaccard distance''', which measures ''dis''similarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union:

:&lt;math&gt; d_J(A,B) = 1 - J(A,B) = { { |A \cup B| - |A \cap B| } \over |A \cup B| }.&lt;/math&gt;

An alternate interpretation of the Jaccard distance is as the ratio of the size of the [[symmetric difference]] &lt;math&gt;A \triangle B = (A \cup B) - (A \cap B)&lt;/math&gt; to the union. 

This distance is a [[Distance function|metric]] on the collection of all finite sets.&lt;ref name=&quot;lipkus&quot;&gt;{{citation |last=Lipkus |first=Alan H
|title=A proof of the triangle inequality for the Tanimoto distance
|journal=J Math Chem |volume=26 |number=1-3 |year=1999 |pages=263–265 }}&lt;/ref&gt;&lt;ref&gt;{{citation |last1=Levandowsky |first1=Michael |last2=Winter |first2=David |title=Distance between sets|journal=Nature |volume=234 |number=5 |year=1971 |pages=34–35 |doi=10.1038/234034a0}}&lt;/ref&gt;

There is also a version of the Jaccard distance for [[measure (mathematics)|measures]], including [[probability measure]]s. If &lt;math&gt;\mu&lt;/math&gt; is a measure on a [[measurable space]] &lt;math&gt;X&lt;/math&gt;, then we define the Jaccard coefficient by &lt;math&gt;J_\mu(A,B) = {{\mu(A \cap B)} \over {\mu(A \cup B)}}&lt;/math&gt;, and the Jaccard distance by &lt;math&gt;d_\mu(A,B) = 1 - J_\mu(A,B) = {{\mu(A \triangle B)} \over {\mu(A \cup B)}}&lt;/math&gt;. Care must be taken if &lt;math&gt;\mu(A \cup B) = 0&lt;/math&gt; or &lt;math&gt;\infty&lt;/math&gt;, since these formulas are not well defined in that case.

== Similarity of asymmetric binary attributes ==
Given two objects, ''A'' and ''B'', each with ''n'' [[binary numeral system|binary]] attributes, the Jaccard coefficient is a useful measure of the overlap that ''A'' and ''B'' share with their attributes.  Each attribute of ''A'' and ''B'' can either be 0 or 1.  The total number of each combination of attributes for both ''A'' and ''B'' are specified as follows:
:&lt;math&gt;M_{11}&lt;/math&gt; represents the total number of attributes where ''A'' and ''B'' both have a value of 1.
:&lt;math&gt;M_{01}&lt;/math&gt; represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.
:&lt;math&gt;M_{10}&lt;/math&gt; represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.
:&lt;math&gt;M_{00}&lt;/math&gt; represents the total number of attributes where ''A'' and ''B'' both have a value of 0.
Each attribute must fall into one of these four categories, meaning that
:&lt;math&gt;M_{11} + M_{01} + M_{10} + M_{00} = n.&lt;/math&gt;

The Jaccard similarity coefficient, ''J'', is given as
:&lt;math&gt;J = {M_{11} \over M_{01} + M_{10} + M_{11}}.&lt;/math&gt;

The Jaccard distance, ''d''&lt;sub&gt;''J''&lt;/sub&gt;, is given as
:&lt;math&gt;d_J = {M_{01} + M_{10} \over M_{01} + M_{10} + M_{11}}.&lt;/math&gt;

== Generalized Jaccard similarity and distance ==

If &lt;math&gt;\mathbf{x} = (x_1, x_2, \ldots, x_n)&lt;/math&gt; and &lt;math&gt;\mathbf{y} = (y_1, y_2, \ldots, y_n)&lt;/math&gt; are two vectors with all real &lt;math&gt;x_i, y_i \geq 0&lt;/math&gt;, then their Jaccard similarity coefficient is defined as
:&lt;math&gt;J(\mathbf{x}, \mathbf{y}) = \frac{\sum_i \min(x_i, y_i)}{\sum_i \max(x_i, y_i)},&lt;/math&gt;
and Jaccard distance
:&lt;math&gt;d_J(\mathbf{x}, \mathbf{y}) = 1 - J(\mathbf{x}, \mathbf{y}).&lt;/math&gt;

With even more generality, if &lt;math&gt;f&lt;/math&gt; and &lt;math&gt;g&lt;/math&gt; are two non-negative measurable functions on a measurable space &lt;math&gt;X&lt;/math&gt; with measure &lt;math&gt;\mu&lt;/math&gt;, then we can define
:&lt;math&gt;J(f, g) = \frac{\int\min(f, g) d\mu}{\int \max(f, g)  d\mu},&lt;/math&gt;
where &lt;math&gt;\max&lt;/math&gt; and &lt;math&gt;\min&lt;/math&gt; are pointwise operators. Then Jaccard distance is
:&lt;math&gt;d_J(f, g) = 1 - J(f, g).&lt;/math&gt;

Then, for example, for two measurable sets &lt;math&gt;A, B \subseteq X&lt;/math&gt;, we have &lt;math&gt;J_\mu(A,B) = J(\chi_A, \chi_B),&lt;/math&gt; where &lt;math&gt;\chi_A&lt;/math&gt; and &lt;math&gt;\chi_B&lt;/math&gt; are the characteristic functions of the corresponding set.

== Tanimoto similarity and distance ==

&lt;!-- [[Tanimoto score]] redirects here, please change that redirect if you change this section title --&gt;

Various forms of functions described as  Tanimoto similarity  and Tanimoto distance occur  in the literature and on the Internet. Most of these are synonyms for Jaccard similarity and Jaccard distance, but some are mathematically different. Many sources&lt;ref&gt;For example {{cite book |first=Huihuan |last=Qian |first2=Xinyu |last2=Wu |first3=Yangsheng |last3=Xu |title=Intelligent Surveillance Systems |publisher=Springer |year=2011 |page=161 |isbn=978-94-007-1137-2 }}&lt;/ref&gt; cite an  unavailable IBM Technical Report&lt;ref&gt;{{cite journal |last=Tanimoto |first=T. |title=An Elementary Mathematical theory of Classification and Prediction |journal=Internal IBM Technical Report |date=17 Nov 1957 |issue=8? |volume=1957 }}&lt;/ref&gt; as the seminal reference.

In &quot;A Computer Program for Classifying Plants&quot;, published in October 1960,&lt;ref&gt;{{cite journal |first=David J. |last=Rogers |first2=Taffee T. |last2=Tanimoto |title=A Computer Program for Classifying Plants |journal=[[Science (journal)|Science]] |volume=132 |issue=3434 |pages=1115–1118 |year=1960 |doi=10.1126/science.132.3434.1115 }}&lt;/ref&gt; a method of classification based on a similarity ratio, and a derived distance function, is given. It seems that this is  the most authoritative  source for the meaning of the terms &quot;Tanimoto similarity&quot; and &quot;Tanimoto Distance&quot;. The similarity ratio is equivalent to Jaccard similarity, but the distance function is ''not'' the same as Jaccard distance.

=== Tanimoto's definitions of similarity and distance ===

In that paper, a &quot;similarity ratio&quot; is  given over [[Bit array|bitmaps]], where each bit of a fixed-size array represents the presence or absence of a characteristic in the plant being modelled. The definition of the ratio is the number of common bits, divided by the number of bits set (i.e. nonzero) in either sample.

Presented in mathematical terms, if samples ''X'' and ''Y'' are bitmaps, &lt;math&gt;X_i&lt;/math&gt; is the ''i''th bit of ''X'', and &lt;math&gt; \land , \lor &lt;/math&gt; are [[bitwise operation|bitwise]] ''[[logical conjunction|and]]'', ''[[logical disjunction|or]]'' operators respectively, then the similarity ratio &lt;math&gt;T_s&lt;/math&gt; is

: &lt;math&gt; T_s(X,Y) =  \frac{\sum_i ( X_i \land Y_i)}{\sum_i ( X_i \lor Y_i)}&lt;/math&gt;

If each sample is modelled instead as a set of attributes, this value is  equal to the Jaccard coefficient of the two sets. Jaccard is not cited in the paper, and it seems likely that the authors were not aware of it.

Tanimoto goes on to define a &quot;distance coefficient&quot; based on this ratio, defined for bitmaps with non-zero similarity:

: &lt;math&gt;T_d(X,Y) = -\log_2 ( T_s(X,Y) ) &lt;/math&gt;

This coefficient is, deliberately, not a distance metric. It is chosen to allow the possibility of two specimens, which are quite different from each other, to both be similar to a third. It is  easy to construct an example which disproves the property of [[Triangle inequality#Metric space|triangle inequality]].

=== Other definitions of Tanimoto distance ===

Tanimoto distance is often referred to, erroneously, as a synonym for Jaccard distance &lt;math&gt; 1 - T_s&lt;/math&gt;. This function is a proper distance metric. &quot;Tanimoto Distance&quot; is often stated as being a proper distance metric, probably because of its confusion with Jaccard distance.

If Jaccard or Tanimoto similarity is expressed over a bit vector, then it can be written as

: &lt;math&gt;
f(A,B) =\frac{ A \cdot B}{\vert A\vert^2 +\vert B\vert^2 -  A \cdot B }
&lt;/math&gt;

where the same calculation is expressed in terms of vector scalar product and magnitude. This representation relies on the fact that, for a bit vector (where the value of each dimension is either 0 or 1) then &lt;math&gt;A \cdot B = \sum_i A_iB_i = \sum_i ( A_i \land B_i)&lt;/math&gt; and &lt;math&gt;{\vert A\vert}^2 = \sum_i A_i^2 = \sum_i A_i &lt;/math&gt;.

This is a potentially confusing representation, because the function as expressed over vectors is more general, unless its domain is explicitly restricted. Properties of &lt;math&gt; T_s &lt;/math&gt; do not necessarily extend to &lt;math&gt;f&lt;/math&gt;. In particular, the difference function &lt;math&gt;1 - f&lt;/math&gt; does not preserve [[triangle inequality]], and is not therefore a proper distance metric, whereas &lt;math&gt;1 - T_s &lt;/math&gt; is.

There is a real danger that the combination of &quot;Tanimoto Distance&quot; being defined using this formula, along with the statement &quot;Tanimoto Distance is a proper distance metric&quot; will lead to the false conclusion that the function &lt;math&gt;1 - f&lt;/math&gt; is in fact a distance metric over vectors or multisets in general, whereas its use in similarity search or clustering algorithms may fail to produce correct results.

Lipkus&lt;ref name=&quot;lipkus&quot; /&gt; uses a definition of Tanimoto similarity which is equivalent to &lt;math&gt;f&lt;/math&gt;, and refers to Tanimoto distance as the function &lt;math&gt; 1 - f&lt;/math&gt;. It is however made clear within the paper that the context is restricted by the use of a (positive) weighting vector &lt;math&gt;W&lt;/math&gt; such that, for any vector ''A'' being considered, &lt;math&gt; A_i \in \{0,W_i\} &lt;/math&gt;. Under these circumstances, the  function  is a proper distance metric, and so a set of vectors governed by such a weighting vector forms a metric space under this function.

== See also ==
* [[Sørensen similarity index]]
* [[simple matching coefficient]]
* [[Mountford's index of similarity]]
* [[Most frequent k characters]]
* [[Hamming distance]]
* [[Dice's coefficient]], which is equivalent: &lt;math&gt;J=D/(2-D)&lt;/math&gt; and &lt;math&gt;D=2J/(1+J)&lt;/math&gt;
* [[Tversky index]]
* [[Correlation]]
* [[Mutual information]], a normalized [[Mutual information#Metric|metricated]] variant of which is an entropic Jaccard distance.

==Notes==
{{reflist}}

{{More footnotes|date=March 2011}}

== References ==
*{{citation|first1=Pang-Ning|last1=Tan|first2=Michael|last2=Steinbach|first3=Vipin|last3=Kumar|title=Introduction to Data Mining|year=2005|isbn=0-321-32136-7}}.
*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1901|title=Étude comparative de la distribution florale dans une portion des Alpes et des Jura|journal=Bulletin de la Société Vaudoise des Sciences Naturelles|volume=37|pages=547–579}}.
*{{citation|first=Paul|last=Jaccard|authorlink=Paul Jaccard|year=1912|title=The distribution of the flora in the alpine zone|journal=New Phytologist|volume=11|pages=37–50|doi=10.1111/j.1469-8137.1912.tb05611.x}}.

== External links ==
* [http://www-users.cs.umn.edu/~kumar/dmbook/dmslides/chap2_data.pdf Introduction to Data Mining lecture notes from Tan, Steinbach, Kumar]
* [http://sourceforge.net/projects/simmetrics/ SimMetrics a sourceforge implementation of Jaccard index and many other similarity metrics]
* [http://www.idea-miner.de/cgi-bin/INT_Tools/ver_vergleich_0_1/cmp_menu2.cgi Web based tool for comparing texts using Jaccard coefficient]
* [http://www.gettingcirrius.com/2011/01/calculating-similarity-part-2-jaccard.html Tutorial on how to calculate different similarities]
* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/JaccardMetric.scala Jaccard] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]

{{DEFAULTSORT:Jaccard Index}}
[[Category:Index numbers]]
[[Category:Measure theory]]
[[Category:Clustering criteria]]
[[Category:String similarity measures]]</text>
      <sha1>sp2n7nryvo89d3nq3gt9agktlg7y47r</sha1>
    </revision>
  </page>
  <page>
    <title>Euclidean distance</title>
    <ns>0</ns>
    <id>53932</id>
    <revision>
      <id>646690583</id>
      <parentid>634192252</parentid>
      <timestamp>2015-02-11T19:50:50Z</timestamp>
      <contributor>
        <username>Loraof</username>
        <id>22399950</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6404">In [[mathematics]], the '''Euclidean distance''' or '''Euclidean metric''' is the &quot;ordinary&quot; [[distance]] between two points in [[Euclidean space]]. With this distance, Euclidean space becomes a [[metric space]]. The associated [[Norm (mathematics)|norm]] is called the '''[[Norm (mathematics)#Euclidean norm|Euclidean norm]].''' Older literature refers to the metric as '''Pythagorean metric'''.

==Definition==
The '''Euclidean distance''' between points '''p''' and '''q''' is the length of the [[line segment]] connecting them (&lt;math&gt;\overline{\mathbf{p}\mathbf{q}}&lt;/math&gt;).

In [[Cartesian coordinates]], if '''p'''&amp;nbsp;=&amp;nbsp;(''p''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''p''&lt;sub&gt;2&lt;/sub&gt;,...,&amp;nbsp;''p''&lt;sub&gt;''n''&lt;/sub&gt;) and '''q'''&amp;nbsp;=&amp;nbsp;(''q''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''q''&lt;sub&gt;2&lt;/sub&gt;,...,&amp;nbsp;''q''&lt;sub&gt;''n''&lt;/sub&gt;) are two points in [[Euclidean space|Euclidean ''n''-space]], then the distance (d) from '''p''' to '''q''', or from '''q''' to '''p''' is given by the [[Pythagorean theorem|Pythagorean formula]]:

{{NumBlk|:|&lt;math&gt;\begin{align}\mathrm{d}(\mathbf{p},\mathbf{q}) = \mathrm{d}(\mathbf{q},\mathbf{p}) &amp; = \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \cdots + (q_n-p_n)^2} \\[8pt]
&amp; = \sqrt{\sum_{i=1}^n (q_i-p_i)^2}.\end{align}&lt;/math&gt;|{{EquationRef|1}}}}

The position of a point in a Euclidean ''n''-space is a [[Euclidean vector]]. So, '''p''' and '''q''' are Euclidean vectors, starting from the origin of the space, and their tips indicate two points. The '''[[Euclidean norm]]''', or '''Euclidean length''', or '''magnitude''' of a vector measures the length of the vector:
:&lt;math&gt;\|\mathbf{p}\| = \sqrt{p_1^2+p_2^2+\cdots +p_n^2} = \sqrt{\mathbf{p}\cdot\mathbf{p}}&lt;/math&gt;
where the last equation involves the [[dot product]].

A vector can be described as a directed line segment from the [[Origin (mathematics)|origin]] of the Euclidean space (vector tail), to a point in that space (vector tip). If we consider that its length is actually the distance from its tail to its tip, it becomes clear that the Euclidean norm of a vector is just a special case of Euclidean distance: the Euclidean distance between its tail and its tip.

The distance between points '''p''' and '''q''' may have a direction (e.g. from '''p''' to '''q'''), so it may be represented by another vector, given by

:&lt;math&gt;\mathbf{q} - \mathbf{p} = (q_1-p_1, q_2-p_2, \cdots, q_n-p_n)&lt;/math&gt;

In a three-dimensional space (''n''=3), this is an arrow from '''p''' to '''q''', which can be also regarded as the position of '''q''' relative to '''p'''. It may be also called a [[displacement (vector)|displacement]] vector if '''p''' and '''q''' represent two positions of the same point at two successive instants of time.

The Euclidean distance between '''p''' and '''q''' is just the Euclidean length of this distance (or displacement) vector:
{{NumBlk|:|&lt;math&gt;\|\mathbf{q} - \mathbf{p}\| = \sqrt{(\mathbf{q}-\mathbf{p})\cdot(\mathbf{q}-\mathbf{p})}.&lt;/math&gt;|{{EquationRef|2}}}}

which is equivalent to equation 1, and also to:

:&lt;math&gt;\|\mathbf{q} - \mathbf{p}\| = \sqrt{\|\mathbf{p}\|^2 + \|\mathbf{q}\|^2 - 2\mathbf{p}\cdot\mathbf{q}}.&lt;/math&gt;

===One dimension===
In one dimension, the distance between two points on the [[real line]] is the [[absolute value]] of their numerical difference.  Thus if ''x'' and ''y'' are two points on the real line, then the distance between them is given by:
:&lt;math&gt;\sqrt{(x-y)^2} = |x-y|.&lt;/math&gt;

In one dimension, there is a single homogeneous, translation-invariant [[Metric (mathematics)|metric]] (in other words, a distance that is induced by a [[Norm (mathematics)|norm]]), up to a scale factor of length, which is the Euclidean distance. In higher dimensions there are other possible norms.

===Two dimensions===
In the [[Euclidean plane]], if '''p'''&amp;nbsp;=&amp;nbsp;(''p''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''p''&lt;sub&gt;2&lt;/sub&gt;) and '''q'''&amp;nbsp;=&amp;nbsp;(''q''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''q''&lt;sub&gt;2&lt;/sub&gt;) then the distance is given by

:&lt;math&gt;\mathrm{d}(\mathbf{p},\mathbf{q})=\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}.&lt;/math&gt;

This is equivalent to the [[Pythagorean theorem]].

Alternatively, it follows from ({{EquationRef|2}}) that if the [[polar coordinates]] of the point '''p''' are (''r''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;θ&lt;sub&gt;1&lt;/sub&gt;) and those of '''q''' are (''r''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;θ&lt;sub&gt;2&lt;/sub&gt;), then the distance between the points is

:&lt;math&gt;\sqrt{r_1^2 + r_2^2 - 2 r_1 r_2 \cos(\theta_1 - \theta_2)}.&lt;/math&gt;

===Three dimensions===
In three-dimensional Euclidean space, the distance  is

:&lt;math&gt;d(p, q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2+(p_3 - q_3)^2}.&lt;/math&gt;

===''n'' dimensions &lt;!-- This is a lower-case italicized &quot;n&quot; for a reason. --&gt;===
In general, for an ''n''-dimensional space, the distance is

:&lt;math&gt;d(p, q) = \sqrt{(p_1- q_1)^2 + (p_2 - q_2)^2+\cdots+(p_i - q_i)^2+\cdots+(p_n - q_n)^2}.&lt;/math&gt;

===Squared Euclidean distance===
The standard Euclidean distance can be squared in order to place progressively greater weight on objects that are farther apart. In this case, the equation becomes

:&lt;math&gt;d^2(p, q) = (p_1 - q_1)^2 + (p_2 - q_2)^2+\cdots+(p_i - q_i)^2+\cdots+(p_n - q_n)^2.&lt;/math&gt;

Squared Euclidean Distance is not a metric as it does not satisfy the [[triangle inequality]], however it is frequently used in optimization problems in which distances only have to be compared.

It is also referred to as [[rational trigonometry#Quadrance|quadrance]] within the field of [[rational trigonometry]].

==See also==
*[[Chebyshev distance]] measures distance assuming only the most significant dimension is relevant.
*[[Euclidean distance matrix]]
*[[Hamming distance]] identifies the difference bit by bit of two strings
*[[Mahalanobis distance]] normalizes based on a covariance matrix to make the distance metric scale-invariant.
*[[Manhattan distance]] measures distance following only axis-aligned directions.
*[[Metric (mathematics)|Metric]]
*[[Minkowski distance]] is a generalization that unifies Euclidean distance, Manhattan distance, and Chebyshev distance.
*[[Pythagorean addition]]

==References==
* {{cite book |first=Elena |last=Deza |first2=Michel Marie |last2=Deza |year=2009 |title=Encyclopedia of Distances |page=94 |publisher=Springer }}
* {{cite web |url=http://www.statsoft.com/textbook/cluster-analysis/ |title=Cluster analysis |date=March 2, 2011 }}

{{DEFAULTSORT:Euclidean Distance}}
[[Category:Metric geometry]]
[[Category:Length]]
[[Category:String similarity measures]]</text>
      <sha1>t8drg0yjae66lwaeqvic5b5my9vcwt7</sha1>
    </revision>
  </page>
  <page>
    <title>Edit distance</title>
    <ns>0</ns>
    <id>406427</id>
    <revision>
      <id>630830517</id>
      <parentid>629915070</parentid>
      <timestamp>2014-10-23T19:18:23Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>/* Basic algorithm */ point out [[Hirschberg's algorithm]] for finding the optimal alignment in linear space</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12351">In [[computer science]], '''edit distance''' is a way of quantifying how dissimilar two [[String (computing)|strings]] (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in [[natural language processing]], where automatic [[Spell checker|spelling correction]] can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In [[bioinformatics]], it can be used to quantify the similarity of [[macromolecule]]s such as [[DNA]], which can be viewed as strings of the letters A, C, G and T.

Several definitions of edit distance exist, using different sets of string operations. One of the most common variants is called [[Levenshtein distance]], named after the Soviet Russian computer scientist [[Vladimir Levenshtein]]. In this version, the allowed operations are the removal or insertion of a single character, or the substitution of one character for another. Levenshtein distance may also simply be called &quot;edit distance&quot;, although several variants exist.&lt;ref name=&quot;navarro&quot;&gt;{{Cite doi/10.1145.2F375360.375365}}&lt;/ref&gt;{{rp|32}}

==Formal definition and properties==
Given two strings {{mvar|a}} and {{mvar|b}} on an alphabet {{mvar|Σ}} (e.g. the set of [[ASCII]] characters, the set of [[byte]]s [0..255], etc.), the edit distance d({{mvar|a}}, {{mvar|b}}) is the minimum-weight series of edit operations that transforms {{mvar|a}} into {{mvar|b}}. One of the simplest sets of edit operations is that defined by Levenshtein in 1966:&lt;ref name=&quot;slp&quot;/&gt;

:'''Insertion''' of a single symbol. If {{mvar|a}} = {{mvar|u}}{{mvar|v}}, then inserting the symbol {{mvar|x}} produces {{mvar|u}}{{mvar|x}}{{mvar|v}}. This can also be denoted ε→{{mvar|x}}, using ε to denote the empty string.
:'''Deletion''' of a single symbol changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|v}} ({{mvar|x}}→ε).
:'''Substitution''' of a single symbol {{mvar|x}} for a symbol {{mvar|y}} ≠ {{mvar|x}} changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|y}}{{mvar|v}} ({{mvar|x}}→{{mvar|y}}).

In Levenshtein's original definition, each of these operations has unit cost (except that substitution of a character by itself has zero cost), so the Levenshtein distance is equal to the minimum ''number'' of operations required to transform {{mvar|a}} to {{mvar|b}}. A more general definition associates non-negative weight functions {{mvar|w}}&lt;sub&gt;ins&lt;/sub&gt;({{mvar|x}}), {{mvar|w}}&lt;sub&gt;del&lt;/sub&gt;({{mvar|x}}) and {{mvar|w}}&lt;sub&gt;sub&lt;/sub&gt;({{mvar|x}}&amp;nbsp;{{mvar|y}}) with the operations.&lt;ref name=&quot;slp&quot;&gt;{{cite book |author1=Daniel Jurafsky |author2=James H. Martin |title=Speech and Language Processing |publisher=Pearson Education International |pages=107–111}}&lt;/ref&gt;

Additional primitive operations have been suggested. A common mistake when typing text is '''transposition''' of two adjacent characters commonly occur, formally characterized by an operation that changes {{mvar|u}}{{mvar|x}}{{mvar|y}}{{mvar|v}} into {{mvar|u}}{{mvar|y}}{{mvar|x}}{{mvar|v}} where {{mvar|x}}, {{mvar|y}} ∈ {{mvar|Σ}}.&lt;ref name=&quot;ukkonen83&quot;&gt;{{cite conference |author=Esko Ukkonen |title=On approximate string matching |conference=Foundations of Computation Theory |year=1983 |pages=487–495 |publisher=Springer}}&lt;/ref&gt;&lt;ref name=&quot;ssm&quot;/&gt;
For the task of correcting [[Optical character recognition|OCR]] output, '''merge''' and '''split''' operations have been used which replace a single character into a pair of them or vice-versa.&lt;ref name=&quot;ssm&quot;&gt;{{cite journal |first1=Klaus U. |last1=Schulz |first2=Stoyan |last2=Mihov |year=2002 |id={{citeseerx|10.1.1.16.652}} |title=Fast string correction with Levenshtein automata |journal=International Journal of Document Analysis and Recognition |volume=5 |issue=1 |pages=67–85 |doi=10.1007/s10032-002-0082-8}}&lt;/ref&gt;

Other variants of edit distance are obtained by restricting the set of operations. [[Longest common subsequence]] (LCS) distance is edit distance with insertion and deletion as the only two edit operations, both at unit cost.&lt;ref name=&quot;navarro&quot;/&gt;{{rp|37}} Similarly, by only allowing substitutions (again at unit cost), [[Hamming distance]] is obtained; this must be restricted to equal-length strings.&lt;ref name=&quot;navarro&quot;/&gt;
[[Jaro–Winkler distance]] can be obtained from an edit distance where only transpositions are allowed.

===Example===
The [[Levenshtein distance]] between &quot;kitten&quot; and &quot;sitting&quot; is 3. The minimal edit script that transforms the former into the latter is:

# '''k'''itten → '''s'''itten (substitution of &quot;s&quot; for &quot;k&quot;)
# sitt'''e'''n → sitt'''i'''n (substitution of &quot;i&quot; for &quot;e&quot;)
# sittin → sittin'''g''' (insertion of &quot;g&quot; at the end).

LCS distance (insertions and deletions only) gives a different distance and minimal edit script:

# delete '''k''' at 0
# insert '''s''' at 0
# delete '''e''' at 4
# insert '''i''' at 4
# insert '''g''' at 6

for a total cost/distance of 5 operations.

===Properties===
Edit distance with non-negative cost satisfies the axioms of a [[Metric (mathematics)|metric]], giving rise to a [[metric space]] of strings, when the following conditions are met:&lt;ref name=&quot;navarro&quot;/&gt;{{rp|37}}

* Every edit operation has positive cost;
* for every operation, there is an inverse operation with equal cost.

With these properties, the metric axioms are satisfied as follows:

:{{mvar|d}}({{mvar|a}}, {{mvar|a}}) = 0, since each string can be trivially transformed to itself using exactly zero operations.
:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) &gt; 0 when {{mvar|a}} ≠ {{mvar|b}}, since this would require at least one operation at non-zero cost.
:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|b}}, {{mvar|a}}) by equality of the cost of each operation and its inverse.
:Triangle inequality: {{mvar|d}}({{mvar|a}}, {{mvar|c}}) ≤ {{mvar|d}}({{mvar|a}}, {{mvar|b}}) + {{mvar|d}}({{mvar|b}}, {{mvar|c}}).&lt;ref&gt;{{cite conference |author1=Lei Chen |author2=Raymond Ng |title=On the marriage of Lₚ-norms and edit distance |conference=Proc. 30th Int'l Conf. on Very Large Databases (VLDB) |volume=30 |year=2004}}&lt;/ref&gt;

Levenshtein distance and LCS distance with unit cost satisfy the above conditions, and therefore the metric axioms. Variants of edit distance that are not proper metrics have also been considered in the literature.&lt;ref name=&quot;navarro&quot;/&gt;

Other useful properties of unit-cost edit distances include:

* LCS distance is bounded above by the sum of lengths of a pair of strings.&lt;ref name=&quot;navarro&quot;/&gt;{{rp|37}}
* LCS distance is an upper bound on Levenshtein distance.
* For strings of the same length, Hamming distance is an upper bound on Levenshtein distance.&lt;ref name=&quot;navarro&quot;/&gt;

Regardless of cost/weights, the following property holds of all edit distances:

* When {{mvar|a}} and {{mvar|b}} share a common prefix, this prefix has no effect on the distance. Formally, when {{mvar|a}} = {{mvar|uv}} and {{mvar|b}} = {{mvar|uw}}, then {{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|v}}, {{mvar|w}}).&lt;ref name=&quot;ssm&quot;/&gt; This allows speeding up many computations involving edit distance and edit scripts, since common prefixes and suffixes can be skipped in linear time.

==Computation==
===Basic algorithm===
{{main|Wagner–Fischer algorithm}}
Using Levenshtein's original operations, the edit distance between &lt;math&gt;a = a_1\ldots a_n&lt;/math&gt; and &lt;math&gt;b = b_1\ldots b_m&lt;/math&gt; is given by &lt;math&gt;d_{mn}&lt;/math&gt;, defined by the recurrence&lt;ref name=&quot;slp&quot;/&gt;

:&lt;math&gt;d_{i0} = \sum_{k=1}^{i} w_\mathrm{del}(b_{k}), \quad  for\; 1 \leq i \leq m&lt;/math&gt;
:&lt;math&gt;d_{0j} = \sum_{k=1}^{j} w_\mathrm{ins}(a_{k}), \quad  for\; 1 \leq j \leq n&lt;/math&gt;
:&lt;math&gt;d_{ij} = \begin{cases} d_{i-1, j-1} &amp; \quad a_{j} = b_{i}\\ \min \begin{cases} d_{i-1, j} + w_\mathrm{del}(b_{i})\\ d_{i,j-1} + w_\mathrm{ins}(a_{j}) \\ d_{i-1,j-1} + w_\mathrm{sub}(a_{j}, b_{i}) \end{cases} &amp; \quad a_{j} \neq b_{i}\end{cases} , \quad  for\; 1 \leq i \leq m, 1 \leq j \leq n.&lt;/math&gt;

This algorithm can be generalized to handle transpositions by adding another term in the recursive clause's minimization.&lt;ref name=&quot;ukkonen83&quot;/&gt;

The straightforward, [[Recursion (computer science)|recursive]] way of evaluating this recurrence takes [[exponential time]]. Therefore, it is usually computed using a [[dynamic programming]] algorithm that is commonly credited to [[Wagner–Fischer algorithm|Wagner and Fischer]],&lt;ref&gt;{{cite journal |author1=R. Wagner |author2=M. Fischer |title=The string-to-string correction problem |journal=J. ACM |volume=21 |year=1974 |pages=168–178 |doi=10.1145/321796.321811}}&lt;/ref&gt; although it has a history of multiple invention.&lt;ref name=&quot;slp&quot;/&gt;&lt;ref name=&quot;ukkonen83&quot;/&gt;
After completion of the Wagner–Fischer algorithm, a minimal sequence of edit operations can be read off as a backtrace of the operations used during the dynamic programming algorithm starting at &lt;math&gt;d_{mn}&lt;/math&gt;.

This algorithm has a [[time complexity]] of Θ({{mvar|m}}{{mvar|n}}). When the full dynamic programming table is constructed, its [[space complexity]] is also Θ({{mvar|m}}{{mvar|n}}); this can be improved to Θ(min({{mvar|m}},{{mvar|n}})) by observing that at any instant, the algorithm only requires two rows (or two columns) in memory. However, this optimization makes it impossible to read off the minimal series of edit operations.&lt;ref name=&quot;ukkonen83&quot;/&gt; A linear-space solution to this problem is offered by [[Hirschberg's algorithm]].&lt;ref&gt;{{cite book |last=Skiena |first=Steven |authorlink=Steven Skiena |title = The Algorithm Design Manual |publisher=[[Springer Science+Business Media]] |edition=2nd |year = 2010 |isbn=1-849-96720-2}}&lt;/ref&gt;{{rp|634}}

===Improved algorithms===
Improving on the Wagner–Fisher algorithm described above, [[Esko Ukkonen|Ukkonen]] describes several variants,&lt;ref&gt;{{cite journal |title=Algorithms for approximate string matching |journal=Information and Control |volume=64 |issue=1–3 |year=1985 |url=http://www.cs.helsinki.fi/u/ukkonen/InfCont85.PDF}}&lt;/ref&gt; one of which takes two strings and a maximum edit distance {{mvar|s}}, and returns min({{mvar|s}}, {{mvar|d}}). It achieves this by only computing and storing a part of the dynamic programming table around its diagonal. This algorithm takes time O({{mvar|s}}×min({{mvar|m}},{{mvar|n}})), where {{mvar|m}} and {{mvar|n}} are the lengths of the strings. Space complexity is O({{mvar|s}}²) or O({{mvar|s}}), depending on whether the edit sequence needs to be read off.&lt;ref name=&quot;ukkonen83&quot;/&gt;

==Applications==
Edit distance finds applications in [[computational biology]] and natural language processing, e.g. the correction of spelling mistakes or OCR errors, and [[approximate string matching]], where the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected.

Various algorithms exist that solve problems beside the computation of distance between a pair of strings, to solve related types of problems.

* [[Hirschberg's algorithm]] computes the optimal [[Sequence alignment|alignment]] of two strings, where optimality is defined as minimizing edit distance.
* [[Approximate string matching]] can be formulated in terms of edit distance. Ukkonen's 1985 algorithm takes a string {{mvar|p}}, called the pattern, and a constant {{mvar|k}}; it then builds a [[deterministic finite state automaton]] that finds, in an arbitrary string {{mvar|s}}, a substring whose edit distance to {{mvar|p}} is at most {{mvar|k}}&lt;ref&gt;{{cite journal |author=Esko Ukkonen |title=Finding approximate patterns in strings |journal=J. Algorithms |volume=6 |pages=132–137 |year=1985 |doi=10.1016/0196-6774(85)90023-9}}&lt;/ref&gt; (cf. the [[Aho–Corasick string matching algorithm|Aho–Corasick algorithm]], which similarly constructs an automaton to search for any of a number of patterns, but without allowing edit operations). A similar algorithm for approximate string matching is the [[bitap algorithm]], also defined in terms of edit distance.
* [[Levenshtein automaton|Levenshtein automata]] are finite-state machines that recognize a set of strings within bounded edit distance of a fixed reference string.&lt;ref name=&quot;ssm&quot;/&gt;

==References==
{{reflist|30em}}

[[Category:String similarity measures]]</text>
      <sha1>omp102km540zvgjwbi6kmwwyom6f4fq</sha1>
    </revision>
  </page>
  <page>
    <title>String-to-string correction problem</title>
    <ns>0</ns>
    <id>1899780</id>
    <revision>
      <id>551511739</id>
      <parentid>464153881</parentid>
      <timestamp>2013-04-21T20:31:00Z</timestamp>
      <contributor>
        <username>Glrx</username>
        <id>2289521</id>
      </contributor>
      <minor/>
      <comment>/* References */ first and last in cite</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1977">{{No footnotes|date=July 2010}}
In [[computer science]], the '''string-to-string correction problem''' refers to the minimum number of edit operations necessary to change one [[String (computer science)|string]] into another. A single edit operation may be changing a single [[Character (computing)|symbol]] of the string into another, deleting, or inserting a symbol. The length of the edit sequence provides a measure of the [[Hamming distance|distance]] between the two strings.

Several [[algorithm]]s exist to provide an efficient way to determine string distance and specify the minimum number of transformation operations required. Such algorithms are particularly useful for [[Delta encoding|delta]] creation operations where something is stored as a set of differences relative to a base version. This allows several versions of a single object to be stored much more efficiently than storing them separately. This holds true even for single versions of several objects if they do not differ greatly, or anything in between. 
Notably, such difference algorithms are used in [[molecular biology]] to provide some measure of kinship between different kinds of organisms based on the similarities of their [[macromolecule]]s (such as [[protein]]s or [[DNA]]).

== See also ==
* [[Delta encoding]]
* [[Levenshtein distance]]
* [[Edit distance]]

== References ==
&lt;div class=&quot;references-small&quot;&gt;
*{{cite journal |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168–173 |doi= 10.1145/321796.321811}}
*{{cite journal |first=Walter F. |last=Tichy |title=The string-to-string correction problem with block moves |journal=ACM Transactions on Computer Systems |volume=2 |issue=4 |year=1984 |pages=309–321 |doi= 10.1145/357401.357404}}
&lt;/div&gt;

[[Category:Problems on strings]]
[[Category:String similarity measures]]</text>
      <sha1>j6jj3to4oyznqm83fcp14c6zfqbmruf</sha1>
    </revision>
  </page>
  <page>
    <title>Jaro–Winkler distance</title>
    <ns>0</ns>
    <id>6782835</id>
    <revision>
      <id>642891539</id>
      <parentid>638678320</parentid>
      <timestamp>2015-01-17T10:16:41Z</timestamp>
      <contributor>
        <ip>41.232.139.240</ip>
      </contributor>
      <comment>/* Definition */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8666">{{About|the measure|other uses|Jaro (disambiguation){{!}}Jaro}}

{{Original research|date=May 2013}}
{{Notability|date=May 2013}}

In [[computer science]] and [[statistics]], the '''Jaro–Winkler distance''' (Winkler, 1990) is a measure of similarity between two [[String (computer science)|strings]].  It is a variant of the '''Jaro distance''' metric (Jaro, 1989, 1995), a type of string [[edit distance]], and was developed in the area of [[record linkage]] (duplicate detection) (Winkler, 1990). The higher the Jaro–Winkler distance for two strings is, the more similar the strings are.  The Jaro–Winkler distance metric is designed and best suited for short strings such as person names.  The score is normalized such that 0 equates to no similarity and 1 is an exact match.

== Definition ==

The Jaro distance &lt;math&gt;d_j&lt;/math&gt; of two given strings &lt;math&gt;s_1&lt;/math&gt; and &lt;math&gt;s_2&lt;/math&gt; is

: &lt;math&gt;d_j = \left\{

\begin{array}{l l}
  0 &amp; \text{if }m = 0\\
  \frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m-t}{m}\right) &amp; \text{otherwise} \end{array} \right.&lt;/math&gt;

Where:

* &lt;math&gt;m&lt;/math&gt; is the number of ''matching characters'' (see below);
* &lt;math&gt;t&lt;/math&gt; is half the number of ''transpositions'' (see below).

Two characters from &lt;math&gt;s_1&lt;/math&gt; and &lt;math&gt;s_2&lt;/math&gt; respectively, are considered ''matching'' only if they are the same and not farther than &lt;math&gt;\left\lfloor\frac{\max(|s_1|,|s_2|)}{2}\right\rfloor-1&lt;/math&gt;.

Each character of &lt;math&gt;s_1&lt;/math&gt; is compared with all its matching
characters in &lt;math&gt;s_2&lt;/math&gt;. The number of matching (but different sequence order) characters
divided by 2 defines the number of ''transpositions''.
For example, in comparing CRATE with TRACE, only 'R'   'A'   'E'  are the matching characters, i.e. m=3. Although 'C', 'T' appear in both strings, they are farther than 1, i.e., floor(5/2)-1=1. Therefore, t=0 . In DwAyNE versus DuANE the matching letters are already in the same order D-A-N-E, so no transpositions are needed.

Jaro–Winkler distance uses a [[prefix]] scale &lt;math&gt;p&lt;/math&gt; which gives more favourable ratings to strings that match from the beginning for a set prefix length &lt;math&gt;\ell&lt;/math&gt;.  Given two strings &lt;math&gt;s_1&lt;/math&gt; and &lt;math&gt;s_2&lt;/math&gt;, their Jaro–Winkler distance &lt;math&gt;d_w&lt;/math&gt; is:

: &lt;math&gt;d_w = d_j + (\ell p (1 - d_j))&lt;/math&gt;

where:

* &lt;math&gt;d_j&lt;/math&gt; is the Jaro distance for strings &lt;math&gt;s_1&lt;/math&gt; and &lt;math&gt;s_2&lt;/math&gt;
* &lt;math&gt;\ell&lt;/math&gt; is the length of common prefix at the start of the string up to a maximum of 4 characters
* &lt;math&gt;p&lt;/math&gt; is a constant [[scaling factor]] for how much the score is adjusted upwards for having common prefixes.  &lt;math&gt;p&lt;/math&gt; should not exceed 0.25, otherwise the distance can become larger than 1.  The standard value for this constant in Winkler's work is &lt;math&gt;p = 0.1&lt;/math&gt;

Although often referred to as a ''distance metric'', the Jaro–Winkler distance is actually not a [[metric (mathematics)|metric]] in the mathematical sense of that term because it does not obey the [[triangle inequality]] [http://richardminerich.com/tag/jaro-winkler/].

In some implementations of Jaro-Winkler, the prefix bonus &lt;math&gt;\ell p (1 - d_j)&lt;/math&gt; is only added when the compared strings have a Jaro distance above a set &quot;boost threshold&quot; &lt;math&gt;b_t&lt;/math&gt;. The boost threshold in Winkler's implementation was 0.7.

: &lt;math&gt;d_w = \left\{

\begin{array}{l l}
  d_j &amp; \text{if }d_j &lt; b_t\\
  d_j + (\ell p (1 - d_j)) &amp; \text{otherwise} \end{array} \right.&lt;/math&gt;

== Example ==

''Note that Winkler's &quot;reference&quot; C code differs in at least two ways from published accounts of the Jaro–Winkler metric. First is his use of a typo table (adjwt) and also some optional additional tolerance for long strings.''

Given the strings &lt;math&gt;s_1&lt;/math&gt; ''MARTHA'' and &lt;math&gt;s_2 &lt;/math&gt; ''MARHTA'' we find:

* &lt;math&gt;m = 6&lt;/math&gt;
* &lt;math&gt;|s_1| = 6&lt;/math&gt;
* &lt;math&gt;|s_2| = 6&lt;/math&gt;
* There are mismatched characters T/H and H/T leading to &lt;math&gt;t = \frac{2}{2} = 1&lt;/math&gt;

We find a Jaro score of:

&lt;math&gt;d_j = \frac{1}{3}\left(\frac{6}{6} + \frac{6}{6} + \frac{6-1}{6}\right) = 0.944&lt;/math&gt;

To find the Jaro–Winkler score using the standard weight &lt;math&gt;p = 0.1&lt;/math&gt;, we continue to find:

* &lt;math&gt;\ell = 3&lt;/math&gt;

Thus:

: &lt;math&gt;d_w = 0.944 + (3 * 0.1 (1 - 0.944)) = 0.961&lt;/math&gt;

Given the strings &lt;math&gt;s_1&lt;/math&gt; ''DWAYNE'' and &lt;math&gt;s_2&lt;/math&gt; ''DUANE'' we find:

* &lt;math&gt;m = 4&lt;/math&gt;
* &lt;math&gt;|s_1| = 6&lt;/math&gt;
* &lt;math&gt;|s_2| = 5&lt;/math&gt;
* &lt;math&gt;t = 0&lt;/math&gt;

We find a Jaro score of:

: &lt;math&gt;d_j = \frac{1}{3}\left(\frac{4}{6} + \frac{4}{5} + \frac{4-0}{4}\right) = 0.822&lt;/math&gt;

To find the Jaro–Winkler score using the standard weight &lt;math&gt;p = 0.1&lt;/math&gt;, we continue to find:

* &lt;math&gt;\ell = 1&lt;/math&gt;

Thus:

: &lt;math&gt;d_w = 0.822 + (1 * 0.1 (1 - 0.822)) = 0.84&lt;/math&gt;

Given the strings &lt;math&gt;s_1&lt;/math&gt; ''DIXON'' and &lt;math&gt;s_2&lt;/math&gt; ''DICKSONX'' we find:

{{elucidate|date=March 2013}}

{| class=&quot;wikitable&quot;
|-
|
| D
| I
| X
| O
| N
|-
| D
| &lt;span style=&quot;background: #ffcc33&quot;&gt;1
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| 0
|-
| I
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;1
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
|-
| C
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
|-
| K
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
|-
| S
| 0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
|-
| O
| 0
| 0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;1
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
|-
| N
| 0
| 0
| 0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;1
|-
| X
| 0
| 0
| 0
| 0
| &lt;span style=&quot;background: #ffcc33&quot;&gt;0
|}

* &lt;math&gt;m = 4&lt;/math&gt;  Note that the two ''X''s are not considered matches because they are outside the match window of 3.

* &lt;math&gt;|s_1| = 5&lt;/math&gt;
* &lt;math&gt;|s_2| = 8&lt;/math&gt;
* &lt;math&gt;t = 0&lt;/math&gt;

We find a Jaro score of:

: &lt;math&gt;d_j = \frac{1}{3}\left(\frac{4}{5} + \frac{4}{8} + \frac{4-0}{4}\right) = 0.767&lt;/math&gt;

To find the Jaro–Winkler score using the standard weight &lt;math&gt;p = 0.1&lt;/math&gt;, we continue to find:

* &lt;math&gt;\ell = 2&lt;/math&gt;

Thus:

: &lt;math&gt;d_w = 0.767 + (2 * 0.1 (1 - 0.767)) = 0.814&lt;/math&gt;

== See also ==

* [[Levenshtein distance]]
* [[Record linkage]]
* [[Census]]

== References ==

* {{cite journal | last=Cohen |first=W. W. |last2=Ravikumar |first2=P. |last3=Fienberg |first3=S. E. |year=2003 |title=A comparison of string distance metrics for name-matching tasks |journal=KDD Workshop on Data Cleaning and Object Consolidation |volume=3 |pages=73-8 |url=https://www.cs.cmu.edu/afs/cs/Web/People/wcohen/postscript/kdd-2003-match-ws.pdf}}
* {{cite journal | author = [[Matthew A. Jaro|Jaro, M. A.]] | title = Advances in record linkage methodology as applied to the 1985 census of Tampa Florida | journal = Journal of the American Statistical Association | year = 1989 | volume = 84 | issue = 406 |pages=414–20| url = | doi = 10.1080/01621459.1989.10478785 }}
* {{cite journal |author=Jaro, M. A. |title=Probabilistic linkage of large public health data file  |journal= Statistics in Medicine |year=1995 |volume=14 |issue=5–7 |pages=491–8  |pmid=7792443 |doi=10.1002/sim.4780140510}}
* {{cite journal

  | author = [[William E. Winkler|Winkler, W. E.]]
  | title = String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage
  | journal = Proceedings of the Section on Survey Research Methods
  | publisher = American Statistical Association
  | pages = 354–359
  | year = 1990
  | url = http://www.amstat.org/sections/srms/Proceedings/papers/1990_056.pdf }}

* {{cite journal | author = [[William E. Winkler|Winkler, W. E.]] | title = Overview of Record Linkage and Current Research Directions | journal = Research Report Series, RRS | year = 2006 | volume = | issue = | url = http://www.census.gov/srd/papers/pdf/rrs2006-02.pdf}}

== External links ==

* [http://web.archive.org/web/20100227020019/http://www.census.gov/geo/msb/stand/strcmp.c strcmp.c - Original C Implementation by the author of the algorithm]

{{DEFAULTSORT:Jaro-Winkler distance}}

[[Category:String similarity measures]]</text>
      <sha1>96f9z62w98v2kx8lrhvg1etivvt5f96</sha1>
    </revision>
  </page>
  <page>
    <title>String kernel</title>
    <ns>0</ns>
    <id>27618564</id>
    <revision>
      <id>587825097</id>
      <parentid>565226133</parentid>
      <timestamp>2013-12-27T00:13:24Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>/* References */Remove stub template(s). Page is start class or higher. Also check for and do General Fixes + Checkwiki fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5476">In [[machine learning]] and [[data mining]], a '''string kernel''' is a [[Positive-definite kernel|kernel function]] that operates on [[String (computer science)|strings]], i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings ''a'' and ''b'' are, the higher the value of a string kernel ''K''(''a'', ''b'') will be.

Using string kernels with [[Kernel trick|kernelized]] learning algorithms such as [[support vector machine]]s allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued [[feature vector]]s.&lt;ref name=&quot;Lodhi&quot;/&gt; String kernels are used in domains where sequence data are to be [[Cluster analysis|clustered]] or [[statistical classification|classified]], e.g. in [[text mining]] and [[bioinformatics|gene analysis]].&lt;ref&gt;
{{Citation
  | title = The spectrum kernel: A string kernel for SVM protein classification
  | last = Leslie
  | first = C.
  | last2 = Eskin
  | first2 = E.
  | last3 = Noble
  | first3 = W.S.
  | booktitle = Proceedings of the Pacific Symposium on Biocomputing
  | volume = 7
  | pages = 566–575
  | year = 2002
}}&lt;/ref&gt;

==Informal introduction==

Suppose one wants to compare some text passages automatically and indicate their relative similarity.
For many applications, it might be sufficient to find some keywords which match exactly.
One example where exact matching is not always enough is found in [[Spam (electronic)|spam]] detection.&lt;ref&gt;
{{Citation
  | title = Improved Online Support Vector Machines Spam Filtering Using String Kernels
  | last = Amayri
  | first = O.
}}&lt;/ref&gt;
Another would be in computational gene analysis, where [[Homology (biology)|homologous]] [[genes]] have [[mutated]], resulting in common subsequences along with deleted, inserted or replaced symbols.
&lt;!--- TODO insert a picture here ---&gt;

==Motivation==

Since several well-proven data clustering, classification and information retrieval
&lt;!--- and other ... see manifold learning ---&gt;
methods (for example support vector machines) are designed to work on vectors
(i.e. data are elements of a vector space), using a string kernel allows the extension of these methods to handle sequence data.

The string kernel method is to be contrasted with earlier approaches for text classification where feature vectors only indicated
the presence or absence of a word.
Not only does it improve on these approaches, but it is an example for a whole class of kernels adapted to data structures, which
began to appear at the turn of the 21st century. A survey of such methods has been compiled by Gärtner.&lt;ref&gt;
{{Citation
  | last = Gärtner
  | first = T.
  | title = A survey of kernels for structured data
  | journal = CM SIGKDD Explorations Newsletter
  | publisher = [[Association for Computing Machinery|ACM]]
  | year = 2003
  | volume = 5
  | number = 1
  | page = 58}}
&lt;/ref&gt;

==Definition==

A [[Kernel trick|kernel]] on a domain &lt;math&gt;D&lt;/math&gt; is a function &lt;math&gt;K: D \times D \rightarrow \mathbb{R}&lt;/math&gt;
satisfying some conditions (being [[symmetric]] in the arguments, [[continuous function|continuous]] and [[Positive-semidefinite function|positive semidefinite]] in a certain sense).

[[Mercer's theorem]] asserts that &lt;math&gt;K&lt;/math&gt; can then be expressed as &lt;math&gt;K(x,y)=\varphi(x)\cdot \varphi(y)&lt;/math&gt; with &lt;math&gt;\varphi&lt;/math&gt; mapping the arguments into an [[inner product space]].

We can now reproduce the definition of a '''string subsequence kernel'''&lt;ref name=&quot;Lodhi&quot;&gt;{{Cite journal
  | last = Lodhi
  | first = Huma
  | last2 = Saunders
  | first2 = Craig
  | last3 = Shawe-Taylor
  | first3 = John
  | last4 = Cristianini
  | first4 = Nello
  | last5 = Watkins
  | first5 = Chris
  | title = Text classification using string kernels
  | journal = [[Journal of Machine Learning Research]]
  | year = 2002
  | pages = 419–444}}&lt;/ref&gt;
on strings over an [[Alphabet (computer science)|alphabet]] &lt;math&gt;\Sigma&lt;/math&gt;. Coordinate-wise, the mapping is defined as follows:

:&lt;math&gt;\varphi_u :
\left\{
\begin{array}{l}
\Sigma^n \rightarrow \mathbb{R}^{\Sigma^n} \\
 s \mapsto \sum_{\mathbf{i} : u=s_{\mathbf{i}}} \lambda^{l(\mathbf{i})}
\end{array}
\right.
&lt;/math&gt;

The &lt;math&gt;\mathbf{i}&lt;/math&gt; are [[multiindices]] and &lt;math&gt;u&lt;/math&gt; is a string of length &lt;math&gt;n&lt;/math&gt;:
subsequences can occur in a non-contiguous manner, but gaps are penalized.
The parameter &lt;math&gt;\lambda&lt;/math&gt; may be set to any value between &lt;math&gt;0&lt;/math&gt; (gaps are not allowed) and &lt;math&gt;1&lt;/math&gt;
(even widely-spread &quot;occurrences&quot; are weighted the same as appearances as a contiguous substring).

&lt;!--- TODO put an example here !!! ---&gt;

For several relevant algorithms, data enters into the algorithm only in expressions involving an inner product of feature vectors,
hence the name [[kernel methods]]. A desirable consequence of this is that one does not need to explicitly calculate the transformation &lt;math&gt;\phi(x)&lt;/math&gt;, only the inner product via the kernel, which may be a lot quicker, especially when [[approximation|approximated]].&lt;ref name=Lodhi/&gt;
&lt;!--- ==Efficitent Computation== ---&gt;
&lt;!--- == See also == ---&gt;

==References==
&lt;!--- cite &quot;alignment kernels&quot;, precursor ---&gt;
{{Reflist}}

[[Category:Algorithms on strings]]
[[Category:Kernel methods for machine learning]]
[[Category:Natural language processing]]
[[Category:String similarity measures]]</text>
      <sha1>evj4vuad70y7ley8y2f0ikv3u2jp3s4</sha1>
    </revision>
  </page>
  <page>
    <title>Damerau–Levenshtein distance</title>
    <ns>0</ns>
    <id>3417630</id>
    <revision>
      <id>640563001</id>
      <parentid>638142953</parentid>
      <timestamp>2015-01-01T21:32:33Z</timestamp>
      <contributor>
        <ip>186.176.52.154</ip>
      </contributor>
      <comment>/* Fraud detection */ Minor grammar fix</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15463">In [[information theory]] and [[computer science]], the '''Damerau–Levenshtein distance''' (named after [[Frederick J. Damerau]] and [[Vladimir I. Levenshtein]]&lt;ref&gt;{{cite conference |last1=Brill |first1=Eric |last2=Moore |first2=Robert C. |year=2000 |title=An Improved Error Model for Noisy Channel Spelling Correction |conference=Proceedings of the 38th Annual Meeting on Association for Computational Linguistics |pages=286–293 |doi=10.3115/1075218.1075255 |url=http://acl.ldc.upenn.edu/P/P00/P00-1037.pdf}}&lt;/ref&gt;&lt;ref name=&quot;bard&quot;/&gt;&lt;ref&gt;{{cite conference |last1=Li |last2=et al. |year=2006|title=Exploring distributional similarity based models for query spelling correction |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1025–1032 |doi=10.3115/1220175.1220304 |url=http://acl.ldc.upenn.edu/P/P06/P06-1129.pdf}}&lt;/ref&gt;) is a [[Metric (mathematics)|distance]] ([[string metric]]) between two [[string (computer science)|strings]], i.e., finite sequence of symbols, given by counting the minimum number of operations needed to transform one string into the other, where an operation is defined as an insertion, deletion, or substitution of a single character, or a [[transposition (mathematics)|transposition]] of two '''adjacent''' characters.  In his seminal paper,&lt;ref&gt;{{Citation |last=Damerau |first=Fred J. |author-link=Frederick J. Damerau |title=A technique for computer detection and correction of spelling errors |journal=Communications of the ACM |publisher=ACM |volume=7 |issue=3 |pages=171–176 |date=March 1964 |doi=10.1145/363958.363994}}&lt;/ref&gt; Damerau not only distinguished these four edit operations but also stated that they correspond to more than 80% of all human misspellings. Damerau's paper considered only misspellings that could be corrected with at most one edit operation.

The Damerau–Levenshtein distance differs from the classical [[Levenshtein distance]] by including transpositions among its allowable operations. The classical Levenshtein distance only allows insertion, deletion, and substitution operations.&lt;ref&gt;{{citation |url= &lt;!-- copylink? http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf--&gt; |first=Vladimir I. |last=Levenshtein |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |date=February 1966 |volume=10 |issue=8 |pages=707&amp;ndash;710}}&lt;/ref&gt; Modifying this distance by including transpositions of adjacent symbols produces a different distance measure, known as the Damerau–Levenshtein distance.&lt;ref name=&quot;bard&quot;&gt;{{citation
 | last = Bard | first = Gregory V.
 | contribution = Spelling-error tolerant, order-independent pass-phrases via the Damerau–Levenshtein string-edit distance metric
 | isbn = 1-920-68285-6
 | location = Darlinghurst, Australia
 | pages = 117–124
 | publisher = Australian Computer Society, Inc.
 | series = Conferences in Research and Practice in Information Technology
 | title = Proceedings of the Fifth Australasian Symposium on ACSW Frontiers : 2007, Ballarat, Australia, January 30 - February 2, 2007
 | url = http://dl.acm.org/citation.cfm?id=1274531.1274545
 | volume = 68
 | year = 2007}}. The isbn produces two hits: a 2007 work and a 2010 work at World Cat.&lt;/ref&gt;

While the original motivation was to measure distance between human misspellings to improve applications such as [[spell checker]]s, Damerau–Levenshtein distance has also seen uses in biology to measure the variation between [[DNA]].&lt;ref&gt;The method used in: {{Citation
| last1  = Majorek         | first1 = Karolina A.
| last2  = Dunin-Horkawicz | first2 = Stanisław
| last3  = Steczkiewicz    | first3 = Kamil
| last4  = Muszewska       | first4 = Anna
| last5  = Nowotny         | first5 = Marcin
| last6  = Ginalski        | first6 = Krzysztof
| last7  = Bujnicki        | first7 = Janusz M.
| display-authors = 2
| title   = The RNase H-like superfamily: new members, comparative structural analysis and evolutionary classification
| journal = Nucleic Acids Research
| volume  = 42
| issue   = 7
| pages   = 4160–4179
| year    = 2013
| doi     = 10.1093/nar/gkt1414
| url     = http://nar.oxfordjournals.org/content/42/7/4160.full
}}&lt;/ref&gt;

== Definition ==
The Damerau–Levenshtein distance between two strings &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; is given by &lt;math&gt;d_{a,b}(|a|,|b|)&lt;/math&gt; where:

&lt;math&gt;\qquad d_{a,b}(i,j) = \begin{cases}
  \max(i,j) &amp; \text{ if} \min(i,j)=0, \\
\min \begin{cases}
          d_{a,b}(i-1,j) + 1 \\
          d_{a,b}(i,j-1) + 1 \\
          d_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)} \\
          d_{a,b}(i-2,j-2) + 1 
       \end{cases} &amp; \text{ if } i,j &gt; 1 \text{ and } a_i = b_{j-1} \text{ and } a_{i-1} = b_j \\
  \min \begin{cases}
          d_{a,b}(i-1,j) + 1 \\
          d_{a,b}(i,j-1) + 1 \\
          d_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} &amp; \text{ otherwise.}
\end{cases}&lt;/math&gt;

where  &lt;math&gt;1_{(a_i \neq b_j)}&lt;/math&gt; is the [[indicator function]] equal to 0 when  &lt;math&gt;a_i = b_j&lt;/math&gt; and equal to 1 otherwise.

Each recursive call matches one of the cases covered by the Damerau–Levenshtein distance:
* &lt;math&gt;d_{a,b}(i-1,j) + 1&lt;/math&gt; corresponds to a deletion (from a to b).
* &lt;math&gt;d_{a,b}(i,j-1) + 1&lt;/math&gt; corresponds to an insertion (from a to b).
* &lt;math&gt;d_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)} &lt;/math&gt; corresponds to a match or mismatch, depending on whether the respective symbols are the same.
* &lt;math&gt;d_{a,b}(i-2,j-2) + 1 &lt;/math&gt; corresponds to a [[transposition (mathematics)|transposition]] between two successive symbols.

== Algorithm ==
Presented here are two algorithms: the first,&lt;ref&gt;{{cite paper | author1 = B. J. Oommen | author2 = R. K. S. Loke | title = Pattern recognition of strings with substitutions, insertions, deletions and generalized transpositions | id = {{citeseerx|10.1.1.50.1459}} | doi=10.1016/S0031-3203(96)00101-X }}&lt;/ref&gt; simpler one, computes what is known as the [[optimal string alignment]]{{Citation needed|date=May 2013}} (sometimes called the ''restricted edit distance''{{Citation needed|date=May 2013}}), while the second one&lt;ref name=&quot;LW75&quot;&gt;{{Citation |first1=Roy |last1=Lowrance |first2=Robert A. |last2=Wagner |title=An Extension of the String-to-String Correction Problem |journal=JACM |volume=22 |issue=2 |pages=177–183 |date=April 1975 |doi=10.1145/321879.321880}}&lt;/ref&gt; computes the Damerau–Levenshtein distance with adjacent transpositions. Adding transpositions adds significant complexity. The difference between the two algorithms consists in that the ''optimal string alignment algorithm'' computes the number of edit operations needed to make the strings equal under the condition that '''no substring is edited more than once''', whereas the second one presents no such restriction.

Take for example the edit distance between '''CA''' and '''ABC'''. The Damerau–Levenshtein distance LD('''CA''','''ABC''') = 2 because '''CA''' → '''AC''' → '''ABC''', but the optimal string alignment distance OSA('''CA''','''ABC''') = 3 because if the operation '''CA''' → '''AC''' is used, it is not possible to use '''AC''' → '''ABC''' because that would require the substring to be edited more than once, which is not allowed in OSA, and therefore the shortest sequence of operations is '''CA''' → '''A''' → '''AB''' → '''ABC'''. Note that for the optimal string alignment distance, the [[triangle inequality]] does not hold: OSA('''CA''','''AC''') + OSA('''AC''','''ABC''') &lt; OSA('''CA''','''ABC'''), and so it is not a true metric.
 
===Optimal string alignment distance===
Firstly, let us consider a direct extension of the formula used to calculate [[Levenshtein distance]].  Below is [[pseudocode]] for a function ''OptimalStringAlignmentDistance'' that takes two strings, ''str1'' of length ''lenStr1'', and ''str2'' of length ''lenStr2'', and computes the optimal string alignment distance between them:

&lt;syntaxhighlight lang=&quot;pascal&quot;&gt;
 int OptimalStringAlignmentDistance(char str1[1..lenStr1], char str2[1..lenStr2])
    // d is a table with lenStr1+1 rows and lenStr2+1 columns
    declare int d[0..lenStr1, 0..lenStr2]

    // i and j are used to iterate over str1 and str2
    declare int i, j, cost

    // for loop is inclusive, need table 1 row/column larger than string length
    for i from 0 to lenStr1
        d[i, 0] := i
    for j from 1 to lenStr2
        d[0, j] := j

    // pseudo-code assumes string indices start at 1, not 0
    // if implemented, make sure to start comparing at 1st letter of strings
    for i from 1 to lenStr1
        for j from 1 to lenStr2
            if str1[i] = str2[j] then cost := 0
                                 else cost := 1
            d[i, j] := minimum(
                                 d[i-1, j  ] + 1,     // deletion
                                 d[i  , j-1] + 1,     // insertion
                                 d[i-1, j-1] + cost   // substitution
                             )
            if(i &gt; 1 and j &gt; 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then
                d[i, j] := minimum(
                                 d[i, j],
                                 d[i-2, j-2] + cost   // transposition
                              )                        
  
    return d[lenStr1, lenStr2]
&lt;/syntaxhighlight&gt;

Basically this is the algorithm to compute [[Levenshtein distance]] with one additional recurrence:

&lt;syntaxhighlight lang=&quot;pascal&quot;&gt;
            if(i &gt; 1 and j &gt; 1 and str1[i] = str2[j-1] and str1[i-1] = str2[j]) then
                d[i, j] := minimum(
                                 d[i, j],
                                 d[i-2, j-2] + cost   // transposition
                              )
&lt;/syntaxhighlight&gt;

===Distance with adjacent transpositions===
Here is the second algorithm that computes the true Damerau–Levenshtein distance with adjacent transpositions (ActionScript 3.0); this function requires as an additional parameter the size of the alphabet (''C''), so that all entries of the arrays are in 0..(''C''&amp;minus;1):

&lt;syntaxhighlight lang='actionscript3'&gt;static public function damerauLevenshteinDistance(a:Array, b:Array, C:uint):uint
{
    // &quot;infinite&quot; distance is just the max possible distance
    var INF:uint = a.length + b.length;

    // make and initialize the character array indices            
    var DA:Array = new Array(C);
    for (var k:uint = 0; k &lt; C; ++k) DA[k]=0;

    // make the distance matrix H[-1..a.length][-1..b.length]
    var H:matrix = new matrix(a.length+2,b.length+2);
    
    // initialize the left and top edges of H
    H[-1][-1] = INF;
    for (var i:uint = 0; i &lt;= a.length; ++i)
    {
        H[i][-1] = INF;
        H[i][ 0] = i;
    }
    for (var j:uint = 0; j &lt;= b.length; ++j)
    {
        H[-1][j] = INF;
        H[ 0][j] = j;
    }

    // fill in the distance matrix H
    // look at each character in a
    for (var i:uint = 1; i &lt;= a.length; ++i)
    {
        var DB:uint = 0;
        // look at each character in b
        for (var j:uint = 1; j &lt;= b.length; ++j)
        {
            var i1:uint = DA[b[j-1]];
            var j1:uint = DB;
            var cost:uint;
            if (a[i-1] == b[j-1])
               {
                 cost = 0;
                 DB   = j;
               }
            else
               cost = 1;
            H[i][j] = Math.min(    H[i-1 ][j-1 ] + cost,  // substitution
                                   H[i   ][j-1 ] + 1,     // insertion
                                   H[i-1 ][j   ] + 1,     // deletion
                                   H[i1-1][j1-1] + (i-i1-1) + 1 + (j-j1-1));
        }
        DA[a[i-1]] = i;
    }
    return H[a.length][b.length];
}
&lt;/syntaxhighlight&gt;

:&lt;small&gt;'''Note''': the algorithm given in the paper uses alphabet 1..C rather than the 0..''C''&amp;minus;1 used here; the paper indexes arrays: H[&amp;minus;1..|A|,&amp;minus;1..|B|] and DA[1..C]; here DA[0..C&amp;minus;1] is used; the paper seems to be missing the necessary line H[&amp;minus;1,&amp;minus;1]&amp;nbsp;=&amp;nbsp;INF&lt;/small&gt;

To devise a proper algorithm to calculate unrestricted Damerau–Levenshtein distance note that there always exists an optimal sequence of edit operations, where once-transposed letters are never modified afterwards. (This holds as long as the cost of a transposition, &lt;math&gt;W_T&lt;/math&gt;, is at least the average of the cost of an insertion and deletion, i.e., &lt;math&gt;2W_T \ge W_I+W_D&lt;/math&gt;.&lt;ref name=&quot;LW75&quot;/&gt;) Thus, we need to consider only two symmetric ways of modifying a substring more than once: (1) transpose letters and insert an arbitrary number of characters between them, or (2) delete a sequence of characters and transpose letters that become adjacent after deletion. The straightforward implementation of this idea gives an algorithm of cubic complexity: &lt;math&gt;O\left (M \cdot N \cdot \max(M, N) \right )&lt;/math&gt;, where ''M'' and ''N'' are string lengths. Using the ideas of Lowrance and Wagner,&lt;ref name=&quot;LW75&quot;/&gt; this naive algorithm can be improved to be &lt;math&gt;O\left (M \cdot N \right)&lt;/math&gt; in the worst case.

It is interesting that the [[bitap algorithm]] can be modified to process transposition. See the information retrieval section of{{ref|itman}} for an example of such an adaptation.

== Applications ==
Damerau–Levenshtein distance plays an important role in [[natural language processing]]. In natural languages, strings are short and the number of errors (misspellings) rarely exceeds 2. In such circumstances, restricted and real edit distance differ very rarely. Oommen and Loke{{ref|OO}} even mitigated the limitation of the restricted edit distance by introducing ''generalized transpositions''. Nevertheless, one must remember that the restricted edit distance usually does not satisfy the [[triangle inequality]] and, thus, cannot be used with [[metric tree]]s.

=== DNA ===
Since [[DNA]] frequently undergoes insertions, deletions, substitutions, and transpositions, and each of these operations occurs on approximately the same timescale, the Damerau–Levenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks is the use of closely related algorithms such as [[Needleman–Wunsch algorithm]] or [[Smith–Waterman algorithm]].

=== Fraud detection ===
The algorithm can be used with any set of words, like vendor names. Since entry is manual by nature there is a risk of entering a false vendor. A fraudster employee may enter one real vendor such as &quot;Rich Heir Estate Services&quot; versus a false vendor &quot;Rich Hier State Services&quot;. The fraudster would then create a false bank account and have the company route checks to the real vendor and false vendor. The Damerau–Levenshtein algorithm will detect the transposed and dropped letter and bring attention of the items to a fraud examiner.

== See also ==
* [[Approximate string matching]]
* [[Levenshtein automata]]
* [[Typosquatting]]

== References ==
{{Reflist|30em}}

== Further reading ==
* {{Citation |first=Gonzalo |last=Navarro |title=A guided tour to approximate string matching |journal=ACM Computing Surveys |volume=33 |issue=1 |pages=31–88 |date=March 2001 |doi=10.1145/375360.375365 }}

{{DEFAULTSORT:Damerau-Levenshtein Distance}}
[[Category:String similarity measures]]
[[Category:Information theory]]
[[Category:Dynamic programming]]</text>
      <sha1>94j0gqchx3rlo9uudwy594s9nudzyw1</sha1>
    </revision>
  </page>
  <page>
    <title>Levenshtein distance</title>
    <ns>0</ns>
    <id>406418</id>
    <revision>
      <id>646635439</id>
      <parentid>646570111</parentid>
      <timestamp>2015-02-11T11:39:47Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>Reverted [[WP:AGF|good faith]] edits by [[Special:Contributions/76.119.30.87|76.119.30.87]] ([[User talk:76.119.30.87|talk]]): Little opportunity for confusion. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15811">{{refimprove|date=February 2010}}

In [[information theory]] and [[computer science]], the '''Levenshtein distance''' is a [[string metric]] for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after [[Vladimir Levenshtein]], who considered this distance in 1965.&lt;ref&gt;{{cite journal |author=Влади́мир И. Левенштейн |script-title=ru:Двоичные коды с исправлением выпадений, вставок и замещений символов |language=Russian |trans_title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Доклады Академий Наук СCCP |volume=163 |issue=4 |pages=845–8 |year=1965}} Appeared in English as: {{cite journal |author=Levenshtein, Vladimir I. |title=Binary codes capable of correcting deletions, insertions, and reversals |journal=Soviet Physics Doklady |volume=10 |number=8 |pages=707–710 |date=February 1966  |url=&lt;!--http://profs.sci.univr.it/~liptak/ALBioinfo/files/levenshtein66.pdf right to publish copy of journal unclear: see http://www.sherpa.ac.uk/romeo/search.php?issn=1028-3358&amp;type=issn&amp;la=en/&amp;fIDnum=%7C&amp;mode=simple ; in any event, liptak does not appear to be the author or the translator --&gt;}}&lt;/ref&gt;

Levenshtein distance may also be referred to as '''edit distance''', although that may also denote a larger [[Edit distance|family of distance metrics]].&lt;ref name=&quot;navarro&quot;&gt;{{Cite doi/10.1145.2F375360.375365}}&lt;/ref&gt;{{rp|32}} It is closely related to [[Sequence alignment#Pairwise alignment|pairwise string alignments]].

== Definition ==
Mathematically, the Levenshtein distance between two strings &lt;math&gt;a, b&lt;/math&gt; is given by &lt;math&gt;\operatorname{lev}_{a,b}(|a|,|b|)&lt;/math&gt; where

:&lt;math&gt;\qquad\operatorname{lev}_{a,b}(i,j) = \begin{cases}
  \max(i,j) &amp; \text{ if} \min(i,j)=0, \\
  \min \begin{cases}
          \operatorname{lev}_{a,b}(i-1,j) + 1 \\
          \operatorname{lev}_{a,b}(i,j-1) + 1 \\
          \operatorname{lev}_{a,b}(i-1,j-1) + 1_{(a_i \neq b_j)}
       \end{cases} &amp; \text{ otherwise.}
\end{cases}&lt;/math&gt;
where  &lt;math&gt;1_{(a_i \neq b_j)}&lt;/math&gt; is the [[indicator function]] equal to 0 when  &lt;math&gt;a_i = b_j&lt;/math&gt; and equal to 1 otherwise.

Note that the first element in the minimum corresponds to deletion (from &lt;math&gt;a&lt;/math&gt; to &lt;math&gt;b&lt;/math&gt;), the second to insertion and the third to match or mismatch, depending on whether the respective symbols are the same.

=== Example ===
For example, the Levenshtein distance between &quot;kitten&quot; and &quot;sitting&quot; is 3, since the following three edits change one into the other, and there is no way to do it with fewer than three edits:

# '''k'''itten → '''s'''itten (substitution of &quot;s&quot; for &quot;k&quot;)
# sitt'''e'''n → sitt'''i'''n (substitution of &quot;i&quot; for &quot;e&quot;)
# sittin → sittin'''g''' (insertion of &quot;g&quot; at the end).

===Upper and lower bounds===
The Levenshtein distance has several simple upper and lower bounds. These include:
* It is always at least the difference of the sizes of the two strings.
* It is at most the length of the longer string.
* It is zero if and only if the strings are equal.
* If the strings are the same size, the [[Hamming distance]] is an upper bound on the Levenshtein distance.
* The Levenshtein distance between two strings is no greater than the sum of their Levenshtein distances from a third string ([[triangle inequality]]).

==Applications==
In [[approximate string matching]], the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected. The short strings could come from a dictionary, for instance. Here, one of the strings is typically short, while the other is arbitrarily long. This has a wide range of applications, for instance, [[spell checker]]s, correction systems for [[optical character recognition]], and software to assist natural language translation based on [[translation memory]].

The Levenshtein distance can also be computed between two longer strings, but the cost to compute it, which is roughly proportional to the product of the two string lengths, makes this impractical.  Thus, when used to aid in [[fuzzy string searching]] in applications such as [[record linkage]], the compared strings are usually short to help improve speed of comparisons.

==Relationship with other edit distance metrics==
{{main|Edit distance}}
There are other popular measures of [[edit distance]], which are calculated using a different set of allowable edit operations. For instance,
* the [[Damerau–Levenshtein distance]] allows insertion, deletion, substitution, and the [[Transposition (mathematics)|transposition]] of two adjacent characters;
* the [[longest common subsequence problem|longest common subsequence]] metric allows only insertion and deletion, not substitution;
* the [[Hamming distance]] allows only substitution, hence, it only applies to strings of the same length.

[[Edit distance]] is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite).  This is further generalized by DNA [[sequence alignment]] algorithms such as the [[Smith–Waterman algorithm]], which make an operation's cost depend on where it is applied.

==Computing Levenshtein distance==

===Recursive===
This is a straightforward, but inefficient, recursive [[pseudocode]] implementation of a &lt;code&gt;LevenshteinDistance&lt;/code&gt; function that takes two strings, ''s'' and ''t'', together with their lengths, and returns the Levenshtein distance between them:

&lt;!--
  Please do not add an additional implementation in your language of choice.
  Many of those have been added to and deleted from this article in the past.
  See the talk page archive for relevant discussion
--&gt;
&lt;source lang=&quot;C&quot;&gt;
// len_s and len_t are the number of characters in string s and t respectively
int LevenshteinDistance(string s, int len_s, string t, int len_t)
{
  /* base case: empty strings */
  if (len_s == 0) return len_t;
  if (len_t == 0) return len_s;

  /* test if last characters of the strings match */
  if (s[len_s-1] == t[len_t-1])
      cost = 0;
  else
      cost = 1;

  /* return minimum of delete char from s, delete char from t, and delete char from both */
  return minimum(LevenshteinDistance(s, len_s - 1, t, len_t    ) + 1,
                 LevenshteinDistance(s, len_s    , t, len_t - 1) + 1,
                 LevenshteinDistance(s, len_s - 1, t, len_t - 1) + cost);
}
&lt;/source&gt;

Unfortunately, this straightforward recursive implementation is very inefficient because it recomputes the Levenshtein distance of the same substrings many times.

A more efficient method would never repeat the same distance calculation. For example, the Levenshtein distance of all possible prefixes might be stored in an array &lt;code&gt;d[][]&lt;/code&gt; where &lt;code&gt;d[i][j]&lt;/code&gt; is the distance between the first &lt;code&gt;i&lt;/code&gt; characters of string &lt;code&gt;s&lt;/code&gt; and the first &lt;code&gt;j&lt;/code&gt; characters of string &lt;code&gt;t&lt;/code&gt;. The table is easy to construct one row at a time starting with row 0. When the entire table has been built, the desired distance is &lt;code&gt;d[len_s][len_t]&lt;/code&gt;. While this technique is significantly faster, it will consume &lt;code&gt;len_s * len_t&lt;/code&gt; more memory than the straightforward recursive implementation.

===Iterative with full matrix===
{{main|Wagner–Fischer algorithm}}
::{{small|Note: This section uses 1-based strings instead of 0-based strings}}
Computing the Levenshtein distance is based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the Levenshtein distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix in a [[dynamic programming]] fashion, and thus find the distance between the two full strings as the last value computed.

This algorithm, an example of bottom-up [[dynamic programming]], is discussed, with variants, in the 1974 article ''The [[String-to-string correction problem]]'' by Robert A. Wagner and Michael J. Fischer.&lt;ref&gt;{{citation |first=Robert A. |last=Wagner |first2=Michael J. |last2=Fischer |author2-link=Michael J. Fischer |title=The String-to-String Correction Problem |journal=Journal of the ACM |volume=21 |issue=1 |year=1974 |pages=168–173 |doi= 10.1145/321796.321811}}&lt;/ref&gt;

This is a straightforward pseudocode implementation for a function ''LevenshteinDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them:

&lt;!--
  Please do not add an additional implementation in your language of choice.
  Many of those have been added to and deleted from this article in the past.
  See the talk page archive for relevant discussion
--&gt;
&lt;!-- choose random language for highlights --&gt;
&lt;source lang=&quot;C&quot;&gt;
int LevenshteinDistance(char s[1..m], char t[1..n])
{
  // for all i and j, d[i,j] will hold the Levenshtein distance between
  // the first i characters of s and the first j characters of t;
  // note that d has (m+1)*(n+1) values
  declare int d[0..m, 0..n]
 
  clear all elements in d // set each element to zero
 
  // source prefixes can be transformed into empty string by
  // dropping all characters
  for i from 1 to m
    {
      d[i, 0] := i
    }
 
  // target prefixes can be reached from empty source prefix
  // by inserting every character
  for j from 1 to n
    {
      d[0, j] := j
    }
 
  for j from 1 to n
    {
      for i from 1 to m
        {
          if s[i] = t[j] then
            d[i, j] := d[i-1, j-1]       // no operation required
          else
            d[i, j] := minimum
                    (
                      d[i-1, j] + 1,  // a deletion
                      d[i, j-1] + 1,  // an insertion
                      d[i-1, j-1] + 1 // a substitution
                    )
        }
    }
 
  return d[m, n]
}
&lt;/source&gt;

Note that this implementation does not fit the [[#Definition|definition]] precisely: it always prefers matches, even if insertions or deletions provided a better score. This is equivalent; it can be shown that for every optimal alignment (which induces the Levenshtein distance) there is another optimal alignment that prefers matches in the sense of this implementation.&lt;ref&gt;[http://cs.stackexchange.com/a/2997 Micro-optimisation for edit distance computation: is it valid?]&lt;/ref&gt;

Two examples of the resulting matrix (hovering over a number reveals the operation performed to get that number):
&lt;center&gt;
{{col-begin|width=auto}}
{{col-break}}
{|class=&quot;wikitable&quot;
|-
| 
| 
!k
!i
!t
!t
!e
!n
|-
| ||0 ||1 ||2 ||3 ||4 ||5 ||6
|-
!s
|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6
|-
!i
|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5
|-
!t
|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4
|-
!t
|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3
|-
!i
|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3
|-
!n
|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}
|-
!g
|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}
|}
{{col-break|gap=1em}}
{|class=&quot;wikitable&quot;
|
|
!S
!a
!t
!u
!r
!d
!a
!y
|-
| 
|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8
|-
!S
|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|delete 'a'|1}} ||{{H:title|delete 't'|2}} ||3 ||4 ||5 ||6 ||7
|-
!u
|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6
|-
!n
|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6
|-
!d
|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5
|-
!a
|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4
|-
!y
|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}
|}
{{col-end}}
&lt;/center&gt;

The [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment &lt;code&gt;s[1..i]&lt;/code&gt; into &lt;code&gt;t[1..j]&lt;/code&gt; using a minimum of &lt;code&gt;d[i,j]&lt;/code&gt; operations. At the end, the bottom-right element of the array contains the answer.

===Iterative with two matrix rows===
It turns out that only two rows of the table are needed for the construction if one does not want to reconstruct the edited input strings (the previous row and the current row being calculated).

The Levenshtein distance may be calculated iteratively using the following algorithm:&lt;ref&gt;{{Citation |title=Fast, memory efficient Levenshtein algorithm |first=Sten |last=Hjelmqvist |date=26 Mar 2012 |url=http://www.codeproject.com/Articles/13525/Fast-memory-efficient-Levenshtein-algorithm}}&lt;/ref&gt;
&lt;syntaxhighlight lang=&quot;CSharp&quot;&gt;
int LevenshteinDistance(string s, string t)
{
    // degenerate cases
    if (s == t) return 0;
    if (s.Length == 0) return t.Length;
    if (t.Length == 0) return s.Length;

    // create two work vectors of integer distances
    int[] v0 = new int[t.Length + 1];
    int[] v1 = new int[t.Length + 1];

    // initialize v0 (the previous row of distances)
    // this row is A[0][i]: edit distance for an empty s
    // the distance is just the number of characters to delete from t
    for (int i = 0; i &lt; v0.Length; i++)
        v0[i] = i;

    for (int i = 0; i &lt; s.Length; i++)
    {
        // calculate v1 (current row distances) from the previous row v0

        // first element of v1 is A[i+1][0]
        //   edit distance is delete (i+1) chars from s to match empty t
        v1[0] = i + 1;

        // use formula to fill in the rest of the row
        for (int j = 0; j &lt; t.Length; j++)
        {
            var cost = (s[i] == t[j]) ? 0 : 1;
            v1[j + 1] = Minimum(v1[j] + 1, v0[j + 1] + 1, v0[j] + cost);
        }

        // copy v1 (current row) to v0 (previous row) for next iteration
        for (int j = 0; j &lt; v0.Length; j++)
            v0[j] = v1[j];
    }

    return v1[t.Length];
}
&lt;/syntaxhighlight&gt;

==See also==
{{colbegin||25em}}
*[[agrep]]
*[[Approximate string matching]]
*[[Bitap algorithm]]
*[[Damerau–Levenshtein distance]]
*[[diff]]
*[[MinHash]]
*[[Dynamic time warping]]
*[[Euclidean distance]]
*[[Fuzzy string searching]]
*[[Hamming weight]]
*[[Hirschberg's algorithm]]
*[[Homology (biology)#Sequence homology|Homology of sequences in genetics]]
*[[Hunt–McIlroy algorithm]]
*[[Jaccard index]]
*[[Jaro–Winkler distance]]
*[[Levenshtein automaton]]
*[[Locality-sensitive hashing]]
*[[Longest common subsequence problem]]
*[[Lucene]] (an open source search engine that implements edit distance)
*[[Manhattan distance]]
*[[Metric space]]
*[[Most frequent k characters]]
*[[Needleman–Wunsch algorithm]]
*[[Optimal matching]] algorithm
*[[Sequence alignment]]
*[[Similarity space]] on [[Numerical taxonomy]]
*[[Smith–Waterman algorithm]]
*[[Sørensen similarity index]]
*[[String distance metric]]
*[[Wagner-Fischer algorithm]]
{{colend}}

==References==
{{reflist|30em}}

==External links==
{{Wikibooks| R_Programming|Text_Processing#Edit_distance|Levenshtein distance in R}}
{{Wikibooks| Algorithm implementation|Strings/Levenshtein distance|Levenshtein distance}}
*[http://www.postgresql.org/docs/current/static/fuzzystrmatch.html Levenshtein in PostgreSQL]
*{{citation |contribution=Levenshtein distance |title=Dictionary of Algorithms and Data Structures [online] |editor-first=Paul E. |editor-last=Black |publisher=U.S. National Institute of Standards and Technology |date=14 August 2008 |accessdate=3 April 2013 |url=http://www.nist.gov/dads/HTML/Levenshtein.html }}

{{DEFAULTSORT:Levenshtein Distance}}
[[Category:String similarity measures]]
[[Category:Dynamic programming]]
[[Category:Articles with example pseudocode]]
[[Category:Quantitative linguistics]]</text>
      <sha1>gpvsvehyuo64zh6ramij7js9frnzbv0</sha1>
    </revision>
  </page>
  <page>
    <title>Wagner–Fischer algorithm</title>
    <ns>0</ns>
    <id>29320010</id>
    <revision>
      <id>617539383</id>
      <parentid>617539251</parentid>
      <timestamp>2014-07-19T04:28:53Z</timestamp>
      <contributor>
        <username>Glrx</username>
        <id>2289521</id>
      </contributor>
      <comment>/* Calculating distance */ not all are rollover active</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9525">In [[computer science]], the '''Wagner–Fischer algorithm''' is a [[dynamic programming]] algorithm that computes the [[edit distance]] between two strings of characters.

==History==
The Wagner–Fischer algorithm has a history of [[multiple invention]]. Navarro lists the following inventors of it, with date of publication, and acknowledges that the list is incomplete:&lt;ref name=&quot;navarro&quot;/&gt;{{rp|43}}
* Vintsyuk, 1968
* [[Needleman–Wunsch algorithm|Needleman and Wunsch]], 1970
* Sankoff, 1972
* Sellers, 1974
* Wagner and Fischer, 1974
* Lowrance and Wagner, 1975

==Calculating distance==
The Wagner–Fischer algorithm computes edit distance based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the edit distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix by [[flood fill]]ing the matrix, and thus find the distance between the two full strings as the last value computed.

A straightforward implementation, as [[pseudocode]] for a function ''EditDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them, looks as follows. Note that the inputs strings are one-indexed, while the matrix ''d'' is zero-indexed, and &lt;code&gt;[i..k]&lt;/code&gt; is a closed range.

  '''int''' EditDistance('''char''' s[1..m], '''char''' t[1..n])
    ''// For all i and j, d[i,j] will hold the Levenshtein distance between''
    ''// the first i characters of s and the first j characters of t.''
    ''// Note that d has (m+1)  x(n+1) values.
    '''let''' d be a 2-d array of '''int''' with dimensions [0..m, 0..n]
   
    '''for''' i '''in''' [0..m]
      d[i, 0] ← i ''// the distance of any first string to an empty second string''
    '''for''' j '''in''' [0..n]
      d[0, j] ← j ''// the distance of any second string to an empty first string''
   
    '''for''' j '''in''' [1..n]
      '''for''' i '''in''' [1..m]
        '''if''' s[i] = t[j] '''then'''  &lt;!-- not: s[i-1] = t[j-1] --&gt;
          d[i, j] ← d[i-1, j-1]       ''// no operation required''
        '''else'''
          d[i, j] ← minimum of
                     (
                       d[i-1, j] + 1,  ''// a deletion''
                       d[i, j-1] + 1,  ''// an insertion''
                       d[i-1, j-1] + 1 ''// a substitution''
                     )
   
    '''return''' d[m,n]

Two examples of the resulting matrix (hovering over an underlined number reveals the operation performed to get that number):
&lt;center&gt;
{|
|
{|class=&quot;wikitable&quot;
|-
| 
| 
!k 
!i 
!t 
!t 
!e 
!n
|-
| ||0 ||1 ||2 ||3 ||4 ||5 ||6
|-
!s
|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6
|-
!i
|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5
|-
!t
|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4
|-
!t
|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 
|-
!i
|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3
|-
!n
|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}
|-
!g
|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}
|}
|
{|class=&quot;wikitable&quot;
|
|
!S
!a
!t
!u
!r
!d
!a
!y
|-
| 
|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8
|-
!S
|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|insert 'a'|1}} ||{{H:title|insert 't'|2}} ||3 ||4 ||5 ||6 ||7
|-
!u
|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6
|-
!n
|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6
|-
!d
|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5 
|-
!a
|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4
|-
!y
|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}
|}
|}
&lt;/center&gt;

The [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment &lt;code&gt;s[1..i]&lt;/code&gt; into &lt;code&gt;t[1..j]&lt;/code&gt; using a minimum of &lt;code&gt;d[i,j]&lt;/code&gt; operations. At the end, the bottom-right element of the array contains the answer.

===Proof of correctness===
As mentioned earlier, the [[invariant (mathematics)|invariant]] is that we can transform the initial segment &lt;code&gt;s[1..i]&lt;/code&gt; into &lt;code&gt;t[1..j]&lt;/code&gt; using a minimum of &lt;code&gt;d[i,j]&lt;/code&gt; operations. This invariant holds since:
* It is initially true on row and column 0 because &lt;code&gt;s[1..i]&lt;/code&gt; can be transformed into the empty string &lt;code&gt;t[1..0]&lt;/code&gt; by simply dropping all &lt;code&gt;i&lt;/code&gt; characters. Similarly, we can transform &lt;code&gt;s[1..0]&lt;/code&gt; to &lt;code&gt;t[1..j]&lt;/code&gt; by simply adding all &lt;code&gt;j&lt;/code&gt; characters.
* If &lt;code&gt;s[i] = t[j]&lt;/code&gt;, and we can transform &lt;code&gt;s[1..i-1]&lt;/code&gt; to &lt;code&gt;t[1..j-1]&lt;/code&gt; in &lt;code&gt;k&lt;/code&gt; operations, then we can do the same to &lt;code&gt;s[1..i]&lt;/code&gt; and just leave the last character alone, giving &lt;code&gt;k&lt;/code&gt; operations.
* Otherwise, the distance is the minimum of the three possible ways to do the transformation:
** If we can transform &lt;code&gt;s[1..i]&lt;/code&gt; to &lt;code&gt;t[1..j-1]&lt;/code&gt; in &lt;code&gt;k&lt;/code&gt; operations, then we can simply add &lt;code&gt;t[j]&lt;/code&gt; afterwards to get &lt;code&gt;t[1..j]&lt;/code&gt; in &lt;code&gt;k+1&lt;/code&gt; operations (insertion).
** If we can transform &lt;code&gt;s[1..i-1]&lt;/code&gt; to &lt;code&gt;t[1..j]&lt;/code&gt; in &lt;code&gt;k&lt;/code&gt; operations, then we can remove &lt;code&gt;s[i]&lt;/code&gt; and then do the same transformation, for a total of &lt;code&gt;k+1&lt;/code&gt; operations (deletion).
** If we can transform &lt;code&gt;s[1..i-1]&lt;/code&gt; to &lt;code&gt;t[1..j-1]&lt;/code&gt; in &lt;code&gt;k&lt;/code&gt; operations, then we can do the same to &lt;code&gt;s[1..i]&lt;/code&gt;, and exchange the original &lt;code&gt;s[i]&lt;/code&gt; for &lt;code&gt;t[j]&lt;/code&gt; afterwards, for a total of &lt;code&gt;k+1&lt;/code&gt; operations (substitution).
* The operations required to transform &lt;code&gt;s[1..n]&lt;/code&gt; into &lt;code&gt;t[1..m]&lt;/code&gt; is of course the number required to transform all of &lt;code&gt;s&lt;/code&gt; into all of &lt;code&gt;t&lt;/code&gt;, and so &lt;code&gt;d[n,m]&lt;/code&gt; holds our result.

This proof fails to validate that the number placed in &lt;code&gt;d[i,j]&lt;/code&gt; is in fact minimal; this is more difficult to show, and involves an [[Reductio ad absurdum|argument by contradiction]] in which we assume &lt;code&gt;d[i,j]&lt;/code&gt; is smaller than the minimum of the three, and use this to show one of the three is not minimal.

===Possible improvements===
Possible improvements to this algorithm include:
* We can adapt the algorithm to use less space, [[Big O notation|''O'']](''m'') instead of ''O''(''mn''), since it only requires that the previous row and current row be stored at any one time.
* We can store the number of insertions, deletions, and substitutions separately, or even the positions at which they occur, which is always &lt;code&gt;j&lt;/code&gt;.
* We can normalize the distance to the interval &lt;code&gt;[0,1]&lt;/code&gt;.
* If we are only interested in the distance if it is smaller than a threshold ''k'', then it suffices to compute a diagonal stripe of width ''2k+1'' in the matrix. In this way, the algorithm can be run in [[Big O notation|''O'']](''kl'') time, where ''l'' is the length of the shortest string.&lt;ref&gt;{{cite book |author=Gusfield, Dan |title=Algorithms on strings, trees, and sequences: computer science and computational biology |publisher=Cambridge University Press |location=Cambridge, UK |year=1997 |isbn=0-521-58519-8 }}&lt;/ref&gt;
* We can give different penalty costs to insertion, deletion and substitution. We can also give penalty costs that depend on which characters are inserted, deleted or substituted.
* This algorithm [[parallel computing|parallelizes]] poorly, due to a large number of [[data dependency|data dependencies]]. However, all the &lt;code&gt;cost&lt;/code&gt; values can be computed in parallel, and the algorithm can be adapted to perform the &lt;code&gt;minimum&lt;/code&gt; function in phases to eliminate dependencies.
* By examining diagonals instead of rows, and by using [[lazy evaluation]], we can find the Levenshtein distance in ''O''(''m'' (1 + ''d'')) time (where ''d'' is the Levenshtein distance), which is much faster than the regular dynamic programming algorithm if the distance is small.&lt;ref&gt;{{cite journal |author=Allison L |title=Lazy Dynamic-Programming can be Eager |journal=Inf. Proc. Letters |volume=43 |issue=4 |pages=207–12 |date=September 1992 |url=http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html |doi=10.1016/0020-0190(92)90202-7}}&lt;/ref&gt;

==Seller's variant for string search==
By initializing the first row of the matrix with zeros, we obtain a variant of the Wagner–Fischer algorithm that can be used for [[fuzzy string searching|fuzzy string search]] of a string in a text.&lt;ref name=&quot;navarro&quot;&gt;{{cite doi|10.1145/375360.375365}}&lt;/ref&gt; This modification gives the end-position of matching substrings of the text. To determine the start-position of the matching substrings, the number of insertions and deletions can be stored separately and used to compute the start-position from the end-position.&lt;ref&gt;Bruno Woltzenlogel Paleo. [http://www.logic.at/people/bruno/Papers/2007-GATE-ESSLLI.pdf An approximate gazetteer for GATE based on levenshtein distance]. Student Section of the European Summer School in Logic, Language and Information ([[European Summer School in Logic, Language and Information|ESSLLI]]), 2007.&lt;/ref&gt;

The resulting algorithm is by no means efficient, but was at the time of its publication (1980) one of the first algorithms that performed approximate search.&lt;ref name=&quot;navarro&quot;/&gt;

== References ==
{{Reflist|30em}}

{{DEFAULTSORT:Wagner-Fischer algorithm}}
[[Category:Algorithms on strings]]
[[Category:String similarity measures]]</text>
      <sha1>2bq06vq7lnlfsg65bj8bid1inw00hds</sha1>
    </revision>
  </page>
  <page>
    <title>Hellinger distance</title>
    <ns>0</ns>
    <id>13035709</id>
    <revision>
      <id>626756138</id>
      <parentid>626756038</parentid>
      <timestamp>2014-09-23T12:35:21Z</timestamp>
      <contributor>
        <ip>137.226.36.51</ip>
      </contributor>
      <comment>Undid revision 626756038 by [[Special:Contributions/137.226.36.51|137.226.36.51]] ([[User talk:137.226.36.51|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7670">In [[probability theory|probability]] and [[mathematical statistics|statistics]], the '''Hellinger distance''' (also called [[Bhattacharyya distance]] as this was originally introduced by [[Anil Kumar Bhattacharya]]) is used to quantify the similarity between two [[probability distributions]]. It is a type of [[f-divergence|''f''-divergence]].  The Hellinger distance is defined in terms of the [[Hellinger integral]], which was introduced by [[Ernst Hellinger]] in 1909.&lt;ref&gt;{{SpringerEOM|title=Hellinger distance|id=h/h046890|first=M.S. |last=Nikulin}}&lt;/ref&gt;&lt;ref&gt;{{Citation 
| last = Hellinger 
| first = Ernst
| author-link = Ernst Hellinger
| title = Neue Begründung der Theorie quadratischer Formen von unendlichvielen Veränderlichen 
| url = http://resolver.sub.uni-goettingen.de/purl?GDZPPN002166941 
| year = 1909 
| journal = [[Journal für die reine und angewandte Mathematik]]
| language = German
| volume = 136 
| pages = 210–271
| jfm = 40.0393.01
| doi=10.1515/crll.1909.136.210
}}&lt;/ref&gt;

==Definition==

===Measure theory===
To define the Hellinger distance in terms of [[measure theory]], let ''P'' and ''Q'' denote two [[probability measure]]s that are [[absolute continuity|absolutely continuous]] with respect to a third probability measure &amp;lambda;.  The square of the Hellinger distance between ''P'' and ''Q'' is defined as the quantity

:&lt;math&gt;H^2(P,Q) = \frac{1}{2}\displaystyle \int \left(\sqrt{\frac{dP}{d\lambda}} - \sqrt{\frac{dQ}{d\lambda}}\right)^2 d\lambda. &lt;/math&gt;

Here, ''dP''&amp;nbsp;/&amp;nbsp;''d&amp;lambda;'' and ''dQ''&amp;nbsp;/&amp;nbsp;''d''&amp;lambda; are the [[Radon–Nikodym derivative]]s of ''P'' and ''Q'' respectively.  This definition does not depend on &amp;lambda;, so the Hellinger distance between ''P'' and ''Q'' does not change if &amp;lambda; is replaced with a different probability measure with respect to which both  ''P'' and ''Q'' are absolutely continuous.  For compactness, the above formula is often written as

:&lt;math&gt;H^2(P,Q) = \frac{1}{2}\int \left(\sqrt{dP} - \sqrt{dQ}\right)^2. &lt;/math&gt;

===Probability theory using Lebesgue measure===
To define the Hellinger distance in terms of elementary probability theory, we take &amp;lambda; to be [[Lebesgue measure]], so that ''dP''&amp;nbsp;/&amp;nbsp;''d&amp;lambda;'' and ''dQ''&amp;nbsp;/&amp;nbsp;''d''&amp;lambda; are simply [[probability density function]]s.  If we denote the densities as ''f'' and ''g'', respectively, the squared Hellinger distance can be expressed as a standard calculus integral

:&lt;math&gt;\frac{1}{2}\int \left(\sqrt{f(x)} - \sqrt{g(x)}\right)^2 dx = 1 - \int \sqrt{f(x) g(x)} \, dx,&lt;/math&gt;

where the second form can be obtained by expanding the square and using the fact that the integral of a probability density over its domain must be one.

The Hellinger distance ''H''(''P'',&amp;nbsp;''Q'') satisfies the property (derivable from the [[Cauchy-Schwarz inequality#L2|Cauchy-Schwarz inequality]])

: &lt;math&gt;0\le H(P,Q) \le 1.&lt;/math&gt;

===Discrete distributions===
For two discrete probability distributions &lt;math&gt;P=(p_1 \ldots p_k)&lt;/math&gt; and &lt;math&gt;Q=(q_1 \ldots q_k)&lt;/math&gt;,
their Hellinger distance is defined as

: &lt;math&gt;
  H(P, Q) = \frac{1}{\sqrt{2}} \; \sqrt{\sum_{i=1}^{k} (\sqrt{p_i} - \sqrt{q_i})^2},
&lt;/math&gt;

which is directly related to the [[Euclidean distance|Euclidean norm]] of the difference of the square root vectors, i.e.
: &lt;math&gt;
H(P, Q) = \frac{1}{\sqrt{2}} \; \bigl\|\sqrt{P} - \sqrt{Q} \bigr\|_2 .
&lt;/math&gt;

== Connection with the statistical distance ==

The Hellinger distance &lt;math&gt;H(P,Q)&lt;/math&gt; and the [[total variation distance]] (or statistical distance) &lt;math&gt;\delta(P,Q)&lt;/math&gt; are related as follows:&lt;ref&gt;[http://www.tcs.tifr.res.in/~prahladh/teaching/2011-12/comm/lectures/l12.pdf Harsha's lecture notes on communication complexity]&lt;/ref&gt;

: &lt;math&gt;
H^2(P,Q) \leq \delta(P,Q) \leq \sqrt 2 H(P,Q)\,.
&lt;/math&gt;

These inequalities follow immediately from the inequalities between the [[Lp space#The p-norm in finite dimensions|1-norm]] and the [[Lp space#The p-norm in finite dimensions|2-norm]].

==Properties==
The maximum distance 1 is achieved when ''P'' assigns probability zero to every set to which ''Q'' assigns a positive probability, and vice versa.

Sometimes the factor 1/2 in front of the integral is omitted, in which case the Hellinger distance ranges from zero to the square root of two.

The Hellinger distance is related to the [[Bhattacharyya distance|Bhattacharyya coefficient]] &lt;math&gt;BC(P,Q)&lt;/math&gt; as it can be defined as

: &lt;math&gt;H(P,Q) = \sqrt{1 - BC(P,Q)}.&lt;/math&gt;

Hellinger distances are used in the theory of [[sequential analysis|sequential]] and [[asymptotic statistics]].&lt;ref&gt;Erik Torgerson (1991) ''Comparison of Statistical Experiments'', volume 36 of Encyclopedia of Mathematics. Cambridge University Press.
&lt;/ref&gt;&lt;ref&gt;{{cite book
  | author = Liese, Friedrich and Miescke, Klaus-J.
  | title = Statistical Decision Theory: Estimation, Testing, and Selection
  | year = 2008
  | publisher = Springer
  | isbn = 0-387-73193-8
  }}
&lt;/ref&gt;

==Examples==
The squared Hellinger distance between two [[normal distribution]]s &lt;math&gt;\scriptstyle P\,\sim\,\mathcal{N}(\mu_1,\sigma_1^2)&lt;/math&gt; and  &lt;math&gt;\scriptstyle Q\,\sim\,\mathcal{N}(\mu_2,\sigma_2^2)&lt;/math&gt; is:
: &lt;math&gt;
  H^2(P, Q) = 1 - \sqrt{\frac{2\sigma_1\sigma_2}{\sigma_1^2+\sigma_2^2}} \,  e^{-\frac{1}{4}\frac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}}.
  &lt;/math&gt;

The squared Hellinger distance  between two [[exponential distribution]]s &lt;math&gt;\scriptstyle P\,\sim \,\rm{Exp}(\alpha)&lt;/math&gt; and &lt;math&gt;\scriptstyle Q\,\sim\,\rm{Exp}(\beta)&lt;/math&gt; is:
: &lt;math&gt;
  H^2(P, Q) = 1 - \frac{2 \sqrt{\alpha \beta}}{\alpha + \beta}.
  &lt;/math&gt;

The squared Hellinger distance  between two [[Weibull distribution]]s &lt;math&gt;\scriptstyle P\,\sim \,\rm{W}(k,\alpha)&lt;/math&gt; and &lt;math&gt;\scriptstyle Q\,\sim\,\rm{W}(k,\beta)&lt;/math&gt; (where &lt;math&gt; k &lt;/math&gt; is a common shape parameter and &lt;math&gt; \alpha\, , \beta &lt;/math&gt; are the scale parameters respectively):
: &lt;math&gt;
  H^2(P, Q) = 1 - \frac{2 (\alpha \beta)^{k/2}}{\alpha^k + \beta^k}.
  &lt;/math&gt;

The squared Hellinger distance between two [[Poisson distribution]]s with rate parameters &lt;math&gt;\alpha&lt;/math&gt; and &lt;math&gt;\beta&lt;/math&gt;, so that &lt;math&gt;\scriptstyle P\,\sim \,\rm{Poisson}(\alpha)&lt;/math&gt; and &lt;math&gt;\scriptstyle Q\,\sim\,\rm{Poisson}(\beta)&lt;/math&gt;, is:
: &lt;math&gt;
  H^2(P,Q) = 1-e^{-\frac{1}{2}(\sqrt{\alpha} - \sqrt{\beta})^2}.
  &lt;/math&gt;

The squared Hellinger distance between two [[Beta distribution]]s &lt;math&gt;\scriptstyle P\,\sim\,\text{Beta}(a_1,b_1)&lt;/math&gt; and  &lt;math&gt;\scriptstyle Q\,\sim\,\text{Beta}(a_2, b_2)&lt;/math&gt; is:
: &lt;math&gt;
H^{2}(P,Q)	=1-\frac{B\left(\frac{a_{1}+a_{2}}{2},\frac{b_{1}+b_{2}}{2}\right)}{\sqrt{B(a_{1},b_{1})B(a_{2},b_{2})}}
  &lt;/math&gt;
where &lt;math&gt;B&lt;/math&gt; is the [[Beta function]].

==See also==
* [[Kullback Leibler divergence]]
* [[Fisher information metric]]

==Notes==
{{reflist}}

==References==
* {{cite book |author=Yang, Grace Lo; Le Cam, Lucien M. |title=Asymptotics in Statistics: Some Basic Concepts |publisher=Springer |location=Berlin |year=2000 |pages= |isbn=0-387-95036-2 |oclc= |doi=}}
* {{cite book |author=Vaart, A. W. van der |title=Asymptotic Statistics (Cambridge Series in Statistical and Probabilistic Mathematics) |publisher=Cambridge University Press |location=Cambridge, UK |year= |pages= |isbn=0-521-78450-6 |oclc= |doi=}}
* {{cite book |author=Pollard, David E. |title=A user's guide to measure theoretic probability |publisher=Cambridge University Press |location=Cambridge, UK |year=2002 |pages= |isbn=0-521-00289-3 |oclc= |doi=}}

[[Category:Probability theory]]
[[Category:F-divergences]]
[[Category:Statistical distance measures]]
[[Category:String similarity measures]]</text>
      <sha1>m1d5nr6lsn8mrqwaojyuq9m5sjxib7m</sha1>
    </revision>
  </page>
  <page>
    <title>Most frequent k characters</title>
    <ns>0</ns>
    <id>42232886</id>
    <revision>
      <id>640352355</id>
      <parentid>639082800</parentid>
      <timestamp>2014-12-31T07:27:45Z</timestamp>
      <contributor>
        <ip>123.17.135.249</ip>
      </contributor>
      <comment>Relink to correct article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12547">{{multiple issues|
{{Third-party|date=March 2014}}
{{Notability|date=March 2014}}
}}

In [[information theory]], '''MostFreqKDistance''' is a [[string metric]] technique for quickly estimating how [[Similarity measure|similar]] two [[Order theory|ordered sets]] or [[String (computer science)|strings]] are. The scheme was invented by {{harvs|first=Sadi Evren|last=SEKER|authorlink=Sadi Evren SEKER|year=2014|txt}},&lt;ref name=&quot;mfkc&quot;/&gt; and initially used in [[text mining]] applications like [[author recognition]].&lt;ref name=&quot;mfkc&quot;&gt;{{citation
 | last1 = SEKER | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Altun | first2 = Oguz
 | last3 = Ayan | first3 = Ugur
 | last4 = Mert | first4 = Cihan
 | contribution = A Novel String Distance Function based on Most Frequent K Characters
 | volume = 4
 | issue = 2
 | pages = 177–183
 | publisher = [[International Association of Computer Science and Information Technology Press (IACSIT Press)]]
 | title = [[International Journal of Machine Learning and Computing (IJMLC)]]
 | contribution-url = http://arxiv.org/abs/1401.6596
 | year = 2014}}&lt;/ref&gt;
Method is originally based on a hashing function MaxFreqKChars &lt;ref name=&quot;hashfunc&quot;&gt;{{citation
 | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Mert | first2 = Cihan
 | contribution = A Novel Feature Hashing For Text Mining
 | url = http://journal.ibsu.edu.ge/index.php/jtst/article/view/428
 | pages = 37–41
 | publisher = [[International Black Sea University]]
 | title = Journal of Technical Science and Technologies
 | ISSN = 2298-0032
 | volume = 2
 | issue = 1
 | year = 2013}}&lt;/ref&gt; classical [[author recognition]] problem and idea first came out while studying on [[data stream mining]].&lt;ref name=&quot;author&quot;&gt;{{citation
 | last1 = Seker | first1 = Sadi E. | author1-link = Sadi Evren SEKER
 | last2 = Al-Naami | first2 = Khaled
 | last3 = Khan | first3 = Latifur
 | contribution = Author attribution on streaming data
 | doi = 10.1109/IRI.2013.6642511
 | url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6642511
 | pages = 497–503
 | publisher = [[IEEE]]
 | title = Information Reuse and Integration (IRI), 2013 IEEE 14th International Conference on, San Francisco, USA, Aug 14-16, 2013
 | year = 2013}}&lt;/ref&gt; Algorithm is suitable for coding in most of the programming languages like [[Java (programming language)|Java]], [[Tcl]], [[Python (programming language)|Python]] or [[J (programming language)|J]]. &lt;ref&gt;
{{Citation |title=Rosetta Code: Most frequent k chars distance , code sources for Python, Java, Tcl and J |accessdate=16 Oct 2014 |url=http://rosettacode.org/wiki/Most_frequent_k_chars_distance}}
&lt;/ref&gt;


==Definition==
Method has two steps.
* [[Hash function|Hash]] input strings str1 and str2 separately using MostFreqKHashing and output hstr1 and hstr2 respectively
* Calculate string distance (or string similarity coefficient) of two hash outputs, hstr1 and hstr2 and output an integer value

===Most frequent K hashing===
The first step of algorithm is calculating the hashing based on the most frequent k characters. The hashing algorithm has below steps:
&lt;syntaxhighlight lang=&quot;Java&quot;&gt;
String function MostFreqKHashing (String inputString, int K)
    def string outputString
    for each distinct character
        count occurrence of each character
    for i := 0 to K
        char c = next most freq ith character  (if two chars have same frequency than get the first occurrence in inputString)
        int count = number of occurrence of the character
        append to outputString, c and count
    end for
    return outputString
&lt;/syntaxhighlight&gt;

Above function, simply gets an input string and an integer K value and outputs the most frequent K characters from the input string. The only condition during the creation of output string is adding the first occurring character first, if the frequencies of two characters are equal. Similar to the most of [[hashing function]]s, ''Most Frequent K Hashing'' is also a [[one way function]].

===Most frequent K distance===
The second step of algorithm works on two outputs from two different input strings and outputs the similarity coefficient (or distance metric).
&lt;syntaxhighlight lang=&quot;Java&quot;&gt;
int function MostFreqKSimilarity (String inputStr1, String inputStr2, int limit)
    def int similarity
    for each c = next character from inputStr1
        lookup c in inputStr2
        if c is null
             continue
             similarity += frequency of c in inputStr1
    return limit-similarity
&lt;/syntaxhighlight&gt;
Above function, simply gets two input strings, previously outputted from the &lt;code&gt;MostFreqKHashing&lt;/code&gt; function. From the most frequent k hashing function, the characters and their frequencies are returned. So, the similarity function calculates the similarity based on characters and their frequencies by checking if the same character appears on both strings. The limit is usually taken to be 10 and in the end the function returns the result of the subtraction of the sum of similarities from limit.

In some implementations, the distance metric is required instead of similarity coefficient. In order to convert the output of above similarity coefficient to distance metric, the output can be subtracted from any constant value (like the maximum possible output value). For the case, it is also possible to implement a [[wrapper function]] over above two functions.

===String distance wrapper function===
In order to calculate the distance between two strings, below function can be implemented
&lt;syntaxhighlight lang=&quot;Java&quot;&gt;
int function MostFreqKSDF (String inputStr1, String inputStr2, int K, int maxDistance)
    return maxDistance - MostFreqKSimilarity(MostFreqKHashing(inputStr1, K), MostFreqKHashing(inputStr2, K))
&lt;/syntaxhighlight&gt;

Any call to above string distance function will supply two input strings and a maximum distance value. The function will calculate the similarity and subtract that value from the maximum possible distance. It can be considered as a simple [[additive inverse]] of similarity.

==Examples==
Let's consider maximum 2 frequent hashing over two strings ‘research’ and ‘seeking’.
MostFreqKHashing('research', 2) = r2e2
because we have 2 'r' and 2 'e' characters with the highest frequency and we return in the order they appear in the string.
MostFreqKHashing('seeking', 2) = e2s1
Again we have character 'e' with highest frequency and rest of the characters have same frequency of 1, so we return the first character of equal frequencies, which is 's'.
Finally we make the comparison:
MostFreqKSimilarity('r2e2', 'e2s1') = 2
We simply compared the outputs and only character occurring in both input is character 'e' and the occurrence in both input is 2.
Instead running the sample step by step as above, we can simply run by using the string distance wrapper function as below:
MostFreqKSDF('research', 'seeking', 2) = 2

Below table holds some sample runs between example inputs for K=2:
{|class=&quot;wikitable&quot;
|-
! Inputs
! Hash Outputs
! SDF Output (max from 10)
|-
|'night'
'nacht'
|n1i1
n1a1
|9
|-
|'my'
'a'
|m1y1
a1NULL0
|10
|-
|‘research’
‘research’	
|r2e2
r2e2	
|6
|-
|‘aaaaabbbb’
‘ababababa’	
|a5b4
a5b4	
|1
|-
|‘significant’
‘capabilities’	
|i3n2
i3a2	
|7
|}

Method is also suitable for bioinformatics to compare the genetic strings like in [[FASTA format]].

Str1 = LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV

Str2 = EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG

MostFreqKHashing(str1, 2) = L9T8

MostFreqKHashing(str2, 2) = F9L8

MostFreqKSDF(str1, str2, 2, 100) = 83

==Algorithm complexity and comparison==
The motivation behind algorithm is calculating the similarity between two input strings. So, the hashing function should be able to reduce the size of input and at the same time keep the characteristics of the input. Other hashing algorithms like [[MD5]] or [[SHA-1]], the output is completely unrelated with the input and those hashing algorithms are not suitable for string similarity check.

On the other hand string similarity functions like [[Levenshtein distance]] have the algorithm complexity problem.

Also algorithms like [[Hamming distance]], [[Jaccard coefficient]] or [[Tanimoto coefficient]] have relatively low algorithm complexity but the success rate in [[text mining]] studies are also low.

===Time complexity===
The calculation of time complexity of 'most frequent k char string similarity' is quite simple. In order to get the maximum frequent K characters from a string, the first step is sorting the string in a lexiconical manner. After this sort, the input with highest occurrence can be achieved with a simple pass in linear time complexity. Since major classical sorting algorithms are working in O(nlogn) complexity like [[merge sort]] or [[quick sort]], we can sort the first string in O(nlogn) and second string on O(mlogm) times. The total complexity would be O(nlog n ) + O (m log m) which is O(n log n) as the upper bound [[worst case analysis]].

===Comparison===
Below table compares the complexity of algorithms:
{|class=&quot;wikitable&quot;
|-
! Algorithm
! Time Complexity
|-
| [[Levenshtein distance]]
| O(nm) = O(n^2)
|-
| [[Jaccard index]]
| O(n+m) = O(n)
|-
| MostFreqKSDF
| O(nlogn+mlogm) = O(n log n)
|}

For the above table, n is the length of first string and m is the length of second string.

==Success on text mining==
The success of string similarity algorithms are compared on a study. The study is based on IMDB62 dataset which is holding 1000 comment entries in [[Internet Movie Database]] from each 62 people. The data set is challenged for three string similarity functions and the success rates are as below:

{|class=&quot;wikitable&quot;
|-
! Algorithm
! Running Time
! Error (RMSE)
! Error (RAE)
|-
|[[Levenshtein distance]]
|3647286.54 sec
|29
|0.47
|-
|[[Jaccard index]]
|228647.22 sec
|45
|0.68
|-
|MostFreqKSDF
|2712323.51 sec
|32
|0.49
|}

The running times for [[author recognition]] are in seconds and the error rates are [[root mean square error]] (RMSE) and [[relative absolute error]] (RAE).

Above table shows, the 'most frequent k similarity' is better than [[Levenshtein distance]] by time and [[Jaccard index]] by success rate.

For the time performance and the success rates, the bitwise similarity functions like [[Dice's coefficient|Sørensen–Dice index]], [[Tversky index]] or [[Hamming Distance]] are all in the same category with similar success rates and running times. There are obviously slight differences but the idea behind bitwise operation, looses the string operations like deletion or addition. For example a single bit addition to the front of one of the input strings would yield a catastrophic result on the similarity for bitwise operators while Levenshtein distance is successfully catching.

Unfortunately, [[big data]] studies requires a faster algorithm with still acceptable success. Here the 'max frequent k characters' is an easy and simple algorithm (as in [[Occams razor]]), which is straight forward to implement.

==See also==
[http://rosettacode.org/wiki/Most_frequent_k_chars_distance RosettaCode,Code reposistory of Most Frequent K Chars Distance Algorithm in Java, Python, TCL or J languages] (Retrieved Oct. 16 2014)
&lt;div class= style=&quot;-moz-column-count:2; column-count:2;&quot;&gt;
* [[agrep]]
* [[Approximate string matching]]
* [[Bitap algorithm]]
* [[Damerau–Levenshtein distance]]
* [[diff]]
* [[MinHash]]
* [[Dynamic time warping]]
* [[Euclidean distance]]
* [[Fuzzy string searching]]
* [[Hamming weight]]
* [[Hirschberg's algorithm]]
* [[Sequence homology|Homology of sequences in genetics]]
* [[Hunt–McIlroy algorithm]]
* [[Jaccard index]]
* [[Jaro–Winkler distance]]
* [[Levenshtein distance]]
* [[Longest common subsequence problem]]
* [[Lucene]] (an open source search engine that implements edit distance)
* [[Manhattan distance]]
* [[Metric space]]
* [[Needleman–Wunsch algorithm]]
* [[Optimal matching]] algorithm
* [[Sequence alignment]]
* Similarity space on [[Numerical taxonomy]]
* [[Smith–Waterman algorithm]]
* [[Sørensen similarity index]]
* [[String distance metric]]
* [[String similarity function]]
* [[Wagner-Fischer algorithm]]
* [[Locality-sensitive hashing]]
&lt;/div&gt;

==References==
{{reflist}}

[[Category:String similarity measures]]
[[Category:Dynamic programming]]
[[Category:Articles with example pseudocode]]
[[Category:Quantitative linguistics]]
[[Category:Hash functions]]
[[Category:Hashing]]</text>
      <sha1>oieegvtni5fqz8mxy8t4teoij8i8gm7</sha1>
    </revision>
  </page>
  <page>
    <title>Simple matching coefficient</title>
    <ns>0</ns>
    <id>45040494</id>
    <revision>
      <id>643622564</id>
      <parentid>643402670</parentid>
      <timestamp>2015-01-22T04:31:23Z</timestamp>
      <contributor>
        <username>Wcherowi</username>
        <id>13428914</id>
      </contributor>
      <comment>/* top */ ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1205">The '''Simple Matching Coefficient (SMC)''' is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets.&lt;ref&gt;http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sdaugherty/similarity.html&lt;/ref&gt;

Given two objects, A and B, each with n binary attributes, SMC is defined as:
:&lt;math&gt; SMC = {\text{Number of Matching Attributes}\over \text{Number of Attributes}} = {{M_{00}+M_{11}}\over{M_{00}+M_{01}+M_{10}+M_{11}}}&lt;/Math&gt;

Where:
:&lt;math&gt;M_{11}&lt;/math&gt; represents the total number of attributes where ''A'' and ''B'' both have a value of 1.
:&lt;math&gt;M_{01}&lt;/math&gt; represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.
:&lt;math&gt;M_{10}&lt;/math&gt; represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.
:&lt;math&gt;M_{00}&lt;/math&gt; represents the total number of attributes where ''A'' and ''B'' both have a value of 0.

== See also ==
* [[Jaccard index]]

==Notes==
{{reflist}}

[[Category:Index numbers]]
[[Category:Measure theory]]
[[Category:Clustering criteria]]
[[Category:String similarity measures]]</text>
      <sha1>l8lbusov9k2my0j5errzknvc1h5n2io</sha1>
    </revision>
  </page>
  <page>
    <title>Social Sciences Citation Index</title>
    <ns>0</ns>
    <id>6853403</id>
    <revision>
      <id>646584955</id>
      <parentid>639351544</parentid>
      <timestamp>2015-02-11T01:33:56Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>removed [[Category:Bibliographic databases]] using [[WP:HC|HotCat]] already in [[Category:Citation indices]], which is a member of [[Category:Bibliographic databases]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1992">The '''Social Sciences Citation Index''' ('''SSCI''') is an interdisciplinary  [[citation index]] product of  [[Thomson Reuters]]' Healthcare &amp; Science division. It was developed by the [[Institute for Scientific Information]] (ISI) from the [[Science Citation Index]].

This citation database covers some 2,474 of the world's leading [[academic journal|journals]] of [[social sciences]] across more than 50 [[academic discipline|disciplines]].&lt;ref&gt;{{cite web
  | title = Social Sciences Citation Index 
  | url = http://scientific.thomson.com/products/ssci/
  | accessdate = 2008-06-11 }}&lt;/ref&gt; It is made available online through the [[Web of Science]] service for a fee.  This database product provides information to identify the articles  cited most frequently and by what publisher and author.

== Criticism ==
In 2004 economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.&lt;ref&gt;Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Box—with an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134-165.&lt;/ref&gt;

==See also==
* [[Arts and Humanities Citation Index]]
* [[Science Citation Index]]

==References==
{{reflist}}

==External links==
* [http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]

{{Thomson Reuters}}
[[Category:Thomson family]]
[[Category:Thomson Reuters]]
[[Category:Social sciences literature]]
[[Category:Citation indices]]

{{database-stub}}
{{sci-stub}}</text>
      <sha1>t7518iezjk9holtckrw2hqxw5hm8bi3</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Legal citators</title>
    <ns>14</ns>
    <id>24447352</id>
    <revision>
      <id>389225528</id>
      <parentid>315734059</parentid>
      <timestamp>2010-10-07T02:14:01Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor/>
      <comment>Merging catmore1/catmore2 per [[Wikipedia:Templates for discussion/Log/2010 September 10|TFD]], and renaming catmore/catmore2 per [[Template talk:cat main|discussion]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="107">{{Cat main|Citator}}

[[Category:Citation indices]]
[[Category:Legal research]]
[[Category:Legal citation]]</text>
      <sha1>rtwjcp6ezchgu1mcxoud4orozv4mdjv</sha1>
    </revision>
  </page>
  <page>
    <title>Arts and Humanities Citation Index</title>
    <ns>0</ns>
    <id>2209985</id>
    <revision>
      <id>646684823</id>
      <parentid>646585242</parentid>
      <timestamp>2015-02-11T19:05:46Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>removed [[Category:Bibliography]] using [[WP:HC|HotCat]] unclear membership</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4354">{{ infobox bibliographic database
| image       = 
| caption     = 
| producer    =Thomson Reuters 
| country     =United States 
| history     = 
| languages   = 
| providers   =Web of Science, Dialog Bluesheets 
| cost        =Subscription 
| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio 
| depth       =Index, abstract, citation indexing, author 
| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances 
| temporal    =1975 to present 
| geospatial  =global 
| number      = 
| updates     = 
| p_title     = 
| p_dates     = 
| ISSN        = 
| web         = 
| titles      =  
}}

The '''''Arts &amp; Humanities Citation Index''''' ('''A&amp;HCI'''), also known as '''''Arts &amp; Humanities Search''''', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore the print counterpart is Current Contents.

Subjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio. 

Available citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances. 

This database can be accessed online through ''[[Web of Science]]''. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.

According to Thomson Reuters, the ''Arts &amp; Humanities Search'', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.&lt;ref name=dialog-blue&gt;
{{Cite web
  | title =Arts &amp; Humanities Search (File 255) 
  | publisher =Dialog bluesheets  
  | date = 
  | url =http://library.dialog.com/bluesheets/html/bl0439.html 
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=Iowa&gt;
Description of Arts &amp; Humanities Search. 
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/artshm.html
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=Iowa-wos&gt;
Description of Web of Science coverage.  
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/websci.html
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;&lt;ref name=TR&gt;
See the page entitled &quot;Tech Specs&quot; 
{{Cite web
  | title =Database description
  | publisher =Thomson Reuters  
  | year = 
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3
  | format =Online web page 
  | accessdate =2011-07-03}}&lt;/ref&gt;
==History==
The index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]' IP &amp; Science division.

==See also==
* [[Science Citation Index]]
* [[Social Sciences Citation Index]]

==References==
{{Reflist}}

== External links ==
* {{Official|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.
* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.

{{Thomson Reuters}}
[[Category:Citation indices]]
[[Category:Thomson Reuters]]

{{DEFAULTSORT:Arts And Humanities Citation Index}}</text>
      <sha1>1n0c44y7bujwjat9argftpvvmlcpzyb</sha1>
    </revision>
  </page>
  <page>
    <title>Science Citation Index</title>
    <ns>0</ns>
    <id>6852678</id>
    <revision>
      <id>646584571</id>
      <parentid>643492978</parentid>
      <timestamp>2015-02-11T01:31:07Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>removed [[Category:Bibliographic databases]] using [[WP:HC|HotCat]] already in [[Category:Citation indices]], which is a member of [[Category:Bibliographic databases]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10230">{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 1964-present
| languages = 
| providers = 
| cost = 
| disciplines = Science, medicine, and technology
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 0036-827X
| web = http://thomsonreuters.com/science-citation-index-expanded/
| titles = 
}}
The '''Science Citation Index''' ('''SCI''') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Thomson Reuters]].&lt;ref name=dimension&gt;
{{cite journal
|doi=10.1126/science.122.3159.108
|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas
|url=http://ije.oxfordjournals.org/content/35/5/1123.full
|format=Free web article download
|year=1955
|last1=Garfield
|first1=E.
|journal=Science
|volume=122
|issue=3159
|pages=108–11
|pmid=14385826|bibcode=1955Sci...122..108G
}}&lt;/ref&gt;&lt;ref name=evolve&gt;
{{cite journal
 |last = Garfield 
 |first = Eugene
 |doi=10.2436/20.1501.01.10
 |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf
 |format=Free PDF download
 |title=The evolution of the Science Citation Index|doi_brokendate = 2015-01-21
 }} International microbiology '''10.'''1 (2010): 65-69.&lt;/ref&gt;&lt;ref name=gOverview&gt;
{{cite web
 | last = Garfield 
 | first = Eugene
 | authorlink =
 | coauthors =
 | title = Science Citation Index
 | work = Science Citation Index 1961
 | publisher = Garfield Library - UPenn
 | date = 1963
 | url = http://garfield.library.upenn.edu/papers/80.pdf
 | format = Free PDF download
 | doi =
 | accessdate = 2013-05-27}} 
* Originally published by the Institute of Scientific Information in 1964
* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.&lt;/ref&gt;&lt;ref name=history-cite-indexing&gt;
{{cite web
 | title =History of Citation Indexing 
 | work =Needs of researchers create demand for citation indexing 
 | publisher =Thomson Ruters 
 | date =November 2010 
 | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ 
 | format =Free HTML download 
 | accessdate =2010-11-04}}&lt;/ref&gt; The larger version ('''Science Citation Index Expanded''') covers more than 6,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternately described as the world's leading journals of [[science]] and [[technology]], because of a rigorous selection process.{{citation needed|date=August 2013}}&lt;ref name=Expanded&gt;
{{cite web 
|url=http://thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index_expanded/ 
|title=Science Citation Index Expanded 
|work= |accessdate=2009-08-30}}&lt;/ref&gt;
&lt;ref name=wetland&gt;{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)&lt;/ref&gt;&lt;ref name=shan&gt;
{{cite journal 
| doi= 10.1007/s11192-012-0837-z 
|title= The top-cited research works in the Science Citation Index Expanded 
|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf 
| year= 2012 
| last1= Ho 
| first1= Yuh-Shan 
| journal= Scientometrics 
| volume= 94 
| issue= 3 
| page= 1297}}&lt;/ref&gt;

The index is made available online through different platforms, such as the [[Web of Science]]&lt;ref name=AtoZ&gt;{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}&lt;/ref&gt;&lt;ref&gt;[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]&lt;/ref&gt; and SciSearch.&lt;ref&gt;{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}&lt;/ref&gt; (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed &quot;Specialty Citation Indexes&quot;,&lt;ref name=SpCI&gt;
{{cite web 
|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ 
|title=Specialty Citation Indexes 
|work= |accessdate=2009-08-30}}&lt;/ref&gt; 
such as the '''Neuroscience Citation Index'''&lt;ref name=NCI&gt;
{{cite web 
|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD 
|title=Journal Search - Science - |work= |accessdate=2009-08-30}}&lt;/ref&gt; and the '''Chemistry Citation Index'''.&lt;ref&gt;{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD 
|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}&lt;/ref&gt;

==Chemistry Citation Index==

The Chemistry Citation Index was first introduced by Eugene Garfield, a chemist. His original &quot;search examples were based on [his] experience as a chemist&quot;.&lt;ref name=Garcci/&gt; In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.&lt;ref name=Garcci&gt;Garfield, Eugene. &quot;[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus].&quot; Current Contents 3 (1992): 5-9.&lt;/ref&gt;

By 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.&lt;ref&gt;
[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&amp;VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.&lt;/ref&gt;

One 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to &quot;observe&quot; chemistry subfields over time.&lt;ref&gt;
{{cite journal
|doi=10.1007/BF02016348
|title=Science citation index and chemistry
|year=1980
|last1=Dewitt
|first1=T. W.
|last2=Nicholson
|first2=R. S.
|last3=Wilson
|first3=M. K.
|journal=Scientometrics
|volume=2
|issue=4
|page=265}}&lt;/ref&gt;

==See also==
* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.
* [[Impact factor]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.

== References ==
{{Reflist|2}}

==Further reading==
*{{cite journal
|doi= 10.1002/aris.1440360102
|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf
|title=Scholarly Communication and Bibliometrics
|year= 2005
|last1= Borgman
|first1= Christine L.
|last2= Furner
|first2= Jonathan
|journal= Annual Review of Information Science and Technology
|volume= 36
|issue= 1 
|pages=3–72}}

*{{cite journal
|doi= 10.1002/asi.20677
|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf
|format= Free PDF download
|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar
|year= 2007
|last1= Meho
|first1= Lokman I.
|last2= Yang
|first2= Kiduk
|journal= Journal of the American Society for Information Science and Technology
|volume= 58
|issue= 13
|page= 2105}}

*{{cite journal
|doi= 10.1002/asi.5090140304
|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf
|format= Free PDF download
|title= New factors in the evaluation of scientific literature through citation indexing
|year= 1963
|last1= Garfield
|first1= E.
|last2= Sher
|first2= I. H.
|journal= American Documentation
|volume= 14
|issue= 3
|page= 195}}

*{{cite journal
|doi= 10.1038/227669a0
|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf
|format= Free PDF download
|title= Citation Indexing for Studying Science
|year= 1970
|last1= Garfield
|first1= E.
|journal= Nature
|volume= 227
|issue= 5259
|pages= 669–71
|pmid= 4914589|bibcode= 1970Natur.227..669G
}}

*{{cite book
 | last =Garfield
 | first =Eugene 
 | authorlink =
 | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities
 | publisher =Wiley-Interscience
 | series = Information Sciences Series
 | edition = 1st
 | origyear = 1979| year = 1983
 | location = New York
 | isbn =9780894950247}}

==External links==
* [http://scientific.thomson.com/products/wos/ Introduction to SCI]
* [http://science.thomsonreuters.com/mjl/ Master journal list]
* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. 
* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. 
* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&amp;VER=E Chemistry Citation Index]. Chinweb.

{{Thomson Reuters}}

[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Thomson Reuters]]</text>
      <sha1>mogd9dywjmeiud893k4ddotf4alsoa1</sha1>
    </revision>
  </page>
  <page>
    <title>Scopus</title>
    <ns>0</ns>
    <id>582311</id>
    <revision>
      <id>642340835</id>
      <parentid>641836553</parentid>
      <timestamp>2015-01-13T18:41:22Z</timestamp>
      <contributor>
        <username>Spiderjerky</username>
        <id>22569803</id>
      </contributor>
      <comment>removed [[Category:Scholarly search engines]]; added [[Category:Scholarly search services]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5056">{{Use dmy dates|date=August 2013}}
{{other uses}}
{{infobox bibliographic database
| title = Scopus
| image = [[File:Scopus_type_logo.jpg]]
| caption = Scopus logo
| producer = [[Elsevier]]
| country = 
| history = 
| languages = English
| providers = 
| cost = Subscription
| disciplines= 
| depth = 
| formats = 
| temporal = 1995-present
| geospatial = Worldwide
| number = 55 million
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://www.scopus.com
| titles = 
}}
'''Scopus''' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).&lt;ref&gt;{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}&lt;/ref&gt; It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.&lt;ref&gt;{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092–6 |pmid=19738094}}&lt;/ref&gt;

Since Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.&lt;ref&gt;{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}&lt;/ref&gt; The board consists of scientists and subject librarians.

A 2008 study  compared [[PubMed]], Scopus, [[Web of Science]], and [[Google Scholar]] and concluded: &lt;blockquote&gt;&quot;PubMed and Google Scholar are accessed for free [...] Scopus offers about 20% more coverage than Web of Science, whereas Google Scholar offers results of inconsistent accuracy. PubMed remains an optimal tool in biomedical electronic research. Scopus covers a wider journal range [...] but it is currently limited to recent articles (published after 1995) compared with Web of Science. Google Scholar, as for the Web in general, can help in the retrieval of even the most obscure information but its use is marred by inadequate, less often updated, citation information.&quot;&lt;ref&gt;{{Cite journal |pmid=17884971 |year=2008 |last1=Falagas |first1=ME |last2=Pitsouni |first2=EI |last3=Malietzis |first3=GA |last4=Pappas |first4=G |title=Comparison of PubMed, Scopus, Web of Science, and Google Scholar: Strengths and weaknesses |volume=22 |issue=2 |pages=338–42 |doi=10.1096/fj.07-9492LSF |journal=[[FASEB Journal]]}}&lt;/ref&gt;&lt;/blockquote&gt;

Evaluating ease of use and coverage of Scopus and the Web of Science (WOS), a 2006 study concluded that &quot;Scopus is easy to navigate, even for the novice user. [...] The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline&quot; and &quot;One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive. [...]&quot;.&lt;ref&gt;{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}&lt;/ref&gt;

Scopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors' [[h-index]].

Scopus can be integrated with [[ORCID]].&lt;ref name=&quot;Scopus&quot;&gt;{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}&lt;/ref&gt;

== References ==
{{reflist|30em}}

== External links ==
* {{Official website|http://www.scopus.com/}}
* [http://www.elsevier.com/online-tools/scopus Scopus information]


{{Reed Elsevier}}

[[Category:Bibliographic databases]]
[[Category:Elsevier]]
[[Category:Citation indices]]
[[Category:Library cataloging and classification]]
[[Category:Scholarly search services]]</text>
      <sha1>65291l2wyessp3ir0qes4lh1uy1ewj7</sha1>
    </revision>
  </page>
  <page>
    <title>Islamic World Science Citation Database</title>
    <ns>0</ns>
    <id>24783829</id>
    <revision>
      <id>605098291</id>
      <parentid>545754487</parentid>
      <timestamp>2014-04-21T03:09:43Z</timestamp>
      <contributor>
        <username>Fadesga</username>
        <id>5042921</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1686">'''Islamic World Science Citation Database''' (ISC) is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].

It was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.&lt;ref&gt;{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}&lt;/ref&gt;  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].

In 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.&lt;ref&gt;{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}&lt;/ref&gt;

== References ==
{{Reflist}}

==See also==
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Impact factor]]

== External links ==
* {{Official website|http://www.isc.gov.ir/isce.htm}}

[[Category:Bibliographic databases]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Research management]]
[[Category:Databases in Iran]]


{{science-journal-stub}}
{{islam-stub}}</text>
      <sha1>dzn25bkh7yku7148i94rj3ctqr5b1ot</sha1>
    </revision>
  </page>
  <page>
    <title>Materials Science Citation Index</title>
    <ns>0</ns>
    <id>27789063</id>
    <revision>
      <id>566727005</id>
      <parentid>555985955</parentid>
      <timestamp>2013-08-01T15:18:37Z</timestamp>
      <contributor>
        <username>Fadesga</username>
        <id>5042921</id>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2980">{{Third-party|date=February 2013}}
'''The Materials Science Citation Index''' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science &amp; engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.&lt;ref name=msci-est&gt;Pemberton, Julia K. &quot;''Two new databases from ISI''.&quot; CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.&lt;/ref&gt;

''Materials Science Citation Index'' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.&lt;ref name=msci-jnlList&gt;[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.&lt;/ref&gt;

==Editions==
Coverage of Materials science is accomplished with the following editions:&lt;ref name=MS-indexes&gt;[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.&lt;/ref&gt;&lt;ref&gt;[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010&lt;/ref&gt;
*Materials Science, Ceramics
*Materials Science, Characterization &amp; Testing
*Materials Science, Biomaterials
*Materials Science, Coatings &amp; Films
*Materials Science, Composites
*Materials Science, Paper &amp; Wood
*Materials Science, Multidisciplinary
*Materials Science, Textiles

==See also==
* [[Science Citation Index]]
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956
* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975
* [[Impact factor]]
* [[VINITI Database RAS]]

==References==
{{Reflist}}

{{Thomson Reuters}}

[[Category:Thomson family]]
[[Category:Bibliographic databases]]
[[Category:Online databases]]
[[Category:Citation indices]]


{{science-journal-stub}}</text>
      <sha1>ifpv3ioc7xuq4wmil6vcwecnrfgxvtb</sha1>
    </revision>
  </page>
  <page>
    <title>SPIN bibliographic database</title>
    <ns>0</ns>
    <id>28010203</id>
    <revision>
      <id>541142705</id>
      <parentid>468647184</parentid>
      <timestamp>2013-02-28T04:38:22Z</timestamp>
      <contributor>
        <username>RockMagnetist</username>
        <id>12961884</id>
      </contributor>
      <comment>/* See also */ Replaced links to individual databases by link to list of databases</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5114">{{Infobox Bibliographic Database
|title =SPIN  (Searchable Physics Information Notices)  
|image = 
|caption = 
|producer =[[American Institute of Physics]] (AIP) 
|country =USA, Russia, Ukraine
|history = 
|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] 
|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] 
|cost = 
|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science &amp; Technology 
|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   
|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia
|temporal =1975 to the present  
|geospatial =International 
|number =over 1.5 million 
|updates =Weekly 
|p_title =No print counterparts 
|p_dates = 
|ISSN =
|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp 
|titles =  
}}

'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.&lt;ref name=DialogSpin/&gt;&lt;ref name=AIP-SPIN/&gt;

==Journals==
Delivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.&lt;ref name=DialogSpin/&gt;&lt;ref name=AIP-SPIN&gt; {{Cite web
  | title =What is the SPIN database? 
  | work =Information about SPIN 
  | publisher =[[American Institute of Physics]] 
  | date =July 2010 
  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&amp;TYPE=HELP/FAQ#ques3 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;

==Sources==
Overall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.&lt;ref name=DialogSpin/&gt;&lt;ref name=pub-coverage&gt;{{Cite web
  | title =SPIN Publication Coverage 
  | work =Complete list of publications covered and coverage years. 
  | publisher =American Institute of Physics 
  | date =July 2010 
  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;  

==Scope==
Subject coverage encompasses the following: &lt;ref name=DialogSpin&gt;  {{Cite web
  | title =Indexes and Databases 
  | work =SPIN: Searchable Physics Information Notices
  | publisher =Raymond H. Fogler Library, The University of Maine
  | date =October 2010 
  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&amp;search=SPIN:+Searchable+Physics+Information+Notices 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;

*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] 
*[[Atomic physics]] and [[Molecular physics]] 
*[[Biological physics]] and [[Medical physics]] 
*[[Classical physics]] and [[Quantum physics]] 
*[[Condensed matter physics]] 
*[[Elementary particle physics]] 
*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] 
*[[Geophysics]], [[Astronomy]], [[Astrophysics]] 
*[[Materials science]] 
*[[Nuclear physics]] 
*[[Plasma physics]] 
*[[Physical chemistry]]

==See also==
*[[List of academic databases and search engines]]

==References==
{{Reflist}}

==External links==
*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.
*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.
*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.


[[Category:Bibliographic databases]]
[[Category:Bibliographic indexes]]
[[Category:Citation indices]]
[[Category:Scientific databases]]</text>
      <sha1>6aqdh1vvywhw72uvh65ey61ar5hamey</sha1>
    </revision>
  </page>
  <page>
    <title>CAB Direct (database)</title>
    <ns>0</ns>
    <id>28077130</id>
    <revision>
      <id>644166135</id>
      <parentid>614959323</parentid>
      <timestamp>2015-01-25T22:43:23Z</timestamp>
      <contributor>
        <username>Shortride</username>
        <id>3046099</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15414">{{Redirect|Global Health|other uses|Global health}}
{{italic title}}
{{Infobox Bibliographic Database
|title =CAB Abstracts 
|image = 
|caption = 
|producer =[[CABI (organisation)|CABI]]
|country =United Kingdom 
|history =1973 to present 
|languages =Fifty languages, English abstracts 
|providers =Datastar, Dialog bluesheets, STN International, CAB Direct (CABI's own platform), Thomson-Reuters [[Web of Knowledge]], EBSCO, OvidSP, Dimdi 
|cost = 
|disciplines =applied life sciences - agriculture, environment, veterinary sciences, applied economics, food science and nutrition 
|depth =bibliographic, abstracting and indexing 
|formats =journal articles, abstracts, proceedings, books, book chapters, monographs, annual reports, handbooks, bulletins, newsletters, discussion papers, field notes, technical information, thesis papers  
|temporal =1973-Present 
|geospatial =Global - international 
|number =6 million + 
|updates =
|p_title = 
|p_dates = 
|ISSN =
|web = 
|titles =http://www.cabi.org/default.aspx?site=170&amp;page=1028  
}}
{{Infobox Bibliographic Database
|title =Global Health bibliographic database 
|image = 
|caption = 
|producer = 
|country = 
|history = 
|languages = 50 languages (158 countries)
|providers =CAB Direct, SilverPlatter, Web of Knowledge, EBSCO, OvidSP, Dialog, Dimdi 
|cost = 
|disciplines =international health research (medical and public)
|depth =bibliographic, abstracting and indexing 
|formats =scientific journals, reports, books and conferences 
|temporal =1973 to present 
|geospatial =global-international 
|number =1.2 million scientific records 
|updates = 
|p_title = 
|p_dates = 
|ISSN =
|web = 
|titles =  
}}

'''CAB Direct''' is a source of references for the ''[[life sciences|applied life sciences]]'' It incorporates two  bibliographic databases: '''''CAB Abstracts''''' and '''''Global Health'''''. CAB Direct is an access point for multiple [[bibliographic databases]] produced by ''CABI''. This database contains 8.8 million [[bibliographic record]]s, which includes  85,000 full text articles. It also includes noteworthy literature reviews. News articles and reports are also part of this combined database.&lt;ref name=direct&gt;{{cite web
  | title =CAB Direct 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabdirect.org/ 
  | accessdate =2010-07-18}}&lt;/ref&gt;

In the U.K., in 1947, the ''Imperial Agricultural Bureaux'' became the ''Commonwealth Agricultural Bureaux'' or ''CAB''. In 1986 the ''Commonwealth Agricultural Bureaux'' became ''[[CAB International]]'' or ''CABI''  &lt;ref name=history-cabi&gt;{{cite web
  | title =Our history 
  | work =Bulleted history 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&amp;page=1388 
  | accessdate =2010-07-18}}&lt;/ref&gt;

==CAB Abstracts==
'''CAB Abstracts''' is an applied life sciences bibliographic database emphasising [[agricultural]] literature, which is international in scope. It contains 6 million records, with coverage from 1973 to present day, adding 300,000 abstracts per year. Subject coverage includes [[agriculture]], [[environmental science|environment]], [[veterinary]] sciences, [[applied economics]], [[food science]] and nutrition. Database covers international issues in agriculture, [[forestry]], and allied disciplines in the life sciences. Indexed publications are from 150 countries in 50 languages, including English abstracts for most articles. Literature coverage includes journals, proceedings,  books, and a large collection of agricultural serials. Other non-journal formats are also indexed.&lt;ref name=cabAb&gt;
{{cite web
  | title =CAB Abstracts 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&amp;page=1016&amp;pid=125 
  | accessdate =2010-07-18}}&lt;/ref&gt;&lt;ref name=TRcababse&gt;{{cite web
  | title =CAB Abstracts (Web of Knowledge) 
  | work = 
  | publisher =Thomson Reuters 
  | date =July 2010 
  | url =http://science.thomsonreuters.com/training/cab/#overview 
  | accessdate =2010-07-18}}&lt;/ref&gt;&lt;ref name=ovidCABabs&gt;{{cite web
  | title =CAB Abstracts 
  | work =Coverage is 1973-Present 
  | publisher =Ovid Technologies, Inc  
  | date =December 2010 
  | url =http://www.ovid.com/site/catalog/DataBase/31.jsp?top=2&amp;mid=3&amp;bottom=7&amp;subsection=10  
  | accessdate =2010-12-10}}&lt;/ref&gt; 
===CAB Abstracts Archive===
'''CAB Abstracts Archive''' is a searchable database produced by ''CABI''. It is created from 600 volumes of printed abstracts,  which are the collected and published [[scientific research]] from 1910 to 1972, and then digitized to form the archive. This archive database contains more than 1.8 million records which covers agriculture, [[veterinary]] science, nutrition and the environment. Subject coverage also includes [[biodiversity]], [[pest control]], [[environmental pollution]], [[animal disease]] (including [[zoonotic disease]]s), [[nutrition]], and [[food production]]. [[Natural resource management]] includes plant and [[animal breeding]]. CAB Abstracts Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].

The following print journals (digitized) comprise CAB Abstracts Archive:
                                                
:Animal Breeding Abstracts, Dairy Science Abstracts, Field Crop Abstracts, 
:Forestry Abstracts, Horticultural Science Abstracts, Nematological Abstracts, 
:Nutrition Abstracts and Reviews Series A: Human and Experimental, 
:Nutrition Abstracts and Reviews Series B: Livestock Feeds and Feeding,  
:Plant Breeding Abstracts, Review of Agricultural Entomology, 
:Review of Medical and Veterinary Mycology, Review of Plant Pathology, 
:Review of Medical and Veterinary Entomology, Review of Plant Pathology, 
:Soils and Fertilizers, Tropical Veterinary Bulletin, Veterinary Bulletin  
:and Weed Abstracts.

===Weed Abstracts===
'''''Weed Abstracts''''', derived from CAB Abstracts, is an abstracts database focused on [[scientific journal|published research]] regarding [[weed]]s and [[herbicides]]. This includes [[plant biology|weed biology]], encompassing [[research|research areas]] from [[genetics]] to [[ecology]], including [[parasitic]], [[poisonous]], [[allergenic]] and [[aquatic plant|aquatic]] weeds. Further coverage includes all topics related to [[weed control]], in both [[farming|crop]] and non-crop situations. Research on herbicides, includes formulations, [[herbicide resistance]] and the effects of [[herbicide residues]] in the environment. 10,000 records are add to this database per year. 

'''''Weed Abstracts''''' is updated weekly with summaries from notable English and foreign language journal articles, reports, conferences and books about weeds and herbicides. With the back-file, coverage is from 1990 to present day bringing the total of available research summaries to 130,000 records.&lt;ref name=weedAb&gt;{{cite web
  | title =Weed Abstracts 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&amp;page=1016&amp;pid=2203
  | accessdate =2010-07-20}}&lt;/ref&gt;

==Global Health database==
'''''Global Health''''' is a bibliographic database which focuses on [[scientific literature|research literature]] in [[public health]] and [[Health science|medical health]] science sectors (including practice). Information (see infobox above) in indexed in more than 5000 [[academic journals]], and indexed from other sources such as reports, books and conferences.  Global Health contains over 1.2 million [[scientific]] records from 1973 to the present, with an addition of  90,000 indexed and abstracted records per year. Sources are abstracted from publications in 158 countries written in 50 languages. Any relevant non-English-language papers are translated into English. Proceedings, patents, thesis papers, electronic publications and relevant but difficult-to-find literature sources are also part of this database.&lt;ref name=glbl-hlth-cabi&gt;{{cite web
  | title =Global Health overview 
  | work = 
  | publisher =CABI  
  | date =July 2010 
  | url =http://www.cabi.org/default.aspx?site=170&amp;page=1016&amp;pid=328 
  | accessdate =2010-07-18}}&lt;/ref&gt;&lt;ref name=TRglblhlth&gt;{{cite web
  | title =Global Health (Web of Knowledge) 
  | work = 
  | publisher =Thomson Reuters 
  | date =July 2010 
  | url =http://thomsonreuters.com/content/PDF/scientific/globalhealth_fs.pdf 
  | format =Free PDF download 
  | accessdate =2010-07-18}}&lt;/ref&gt;&lt;ref name=ovidGH&gt;{{cite web
  | title =Global Health (Ovid) 
  | work = 
  | publisher =Ovid Technologies Inc. 
  | date =July 2010 
  | url =http://www.ovid.com/site/catalog/DataBase/30.jsp?top=2&amp;mid=3&amp;bottom=7&amp;subsection=10 
  | accessdate =2010-07-18}}&lt;/ref&gt; 

===Global Health Archive===
'''''Global Health Archive''''' is a searchable database produced by CABI. It is created from 800,000 records, from six printed abstract journals,  which are collected published scientific research from 1910 to 1972, digitized to form the archive. Global Health Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].&lt;ref name=ghArchive/&gt;

When combined with the ''Global Health'' database indexing coverage can be from 1910 to present day. Hence, coverage is made up of past [[epidemics]], from rates and patterns of disease [[Transmission (medicine)|transmission]], duration of [[pandemics]], timing of epidemiological peaks, [[geographic distribution]] of diseases, and [[World Health Organization|government preparedness]] and [[quarantine]] provisions.  The following can also be taken  into account:  effects on different age and [[social groups]], severity in developing vs. developed countries, [[symptoms]], causes of [[Human|mortality]] - such as secondary problems like [[pneumonia]] - and mortality rates.&lt;ref name=ghArchive&gt;{{cite web
  | title =Global Health Archive 
  | work = 
  | publisher =CABI 
  | date =March 2010  
  | url =http://www.cabi.org/default.aspx?site=170&amp;page=1016&amp;pid=2221 
  | accessdate =2010-07-18}}&lt;/ref&gt;&lt;ref name=ovidGHA&gt;{{cite web
  | title =Global Health Archive (Ovid)
  | work = 
  | publisher =Ovid Technologies Inc. 
  | date =July 2010 
  | url =http://www.ovid.com/site/catalog/DataBase/1748.jsp?top=2&amp;mid=3&amp;bottom=7&amp;subsection=10 
  | accessdate =2010-07-18}}&lt;/ref&gt; 

====Journal and topic coverage====
Records for this database are derived from the following journals throughout certain years:&lt;ref name=ghArchive/&gt;&lt;ref name=ovidGHA/&gt;

:Tropical Diseases Bulletin (1912-83),
:Abstracts on Hygiene and Communicable Diseases (1926-83), 
:Review of Veterinary and Medical Entomology (1913-72), 
:Review of Veterinary and Medical Mycology (1943-72) 
:Nutrition Abstracts and Reviews (1931-72), and Helminthological Abstracts (1932-72).

Subject coverage includes [[Public health]], [[tropical disease|Tropical]] and [[Communicable disease]]s, Nutrition, [[Parasitology]], [[Entomology]], and [[Mycology]].

===Tropical Diseases Bulletin===
'''''Tropical Diseases Bulletin''''' is a bibliographic and abstracts database which focuses on research published regarding [[infectious disease]]s and [[public health]] in [[developing countries]] and the [[tropics]] and [[subtropics]]. This includes research areas from [[epidemiology]] to [[diagnosis]], [[therapy]] to [[disease prevention]], [[tropical medicine]], and related aspects of [[travel medicine]]. Published research coverage on [[patients]] and populations encompasses the health of marginalized populations: [[immigrant]]s, [[refugee]]s, and [[indigenous peoples]].&lt;ref name=tropical/&gt;

Back-file coverage is from 1990 to present day, with an accessible base of 195,000 abstracts and the addition of 11,000 records per year. As a monthly journal '''''Tropical Diseases Bulletin''''' is also available in print. This print journal has author, subject and serials cited indexes.  Coverage of the print back-file is to 1912. A searchable, electronic database version of this journal is part of the ''Global Health Archive'' (see above).&lt;ref name=tropical&gt;
{{cite web
  | title =Tropical Diseases Bulletin
  | work = 
  | publisher =CABI 
  | year =2010  
  | url =http://www.cabi.org/default.aspx?site=170&amp;page=1016&amp;pid=2201
  | accessdate =2010-07-18}}&lt;/ref&gt;

==Organic Research Database==
This indexing database focuses on scientific literature pertaining to all topics in  [[organic farming]], in both the [[temperate zone|temperate]] and [[tropical zone]]s. This includes [[sustainability|sustainability issues]] and [[soil science|soil fertility]]. Coverage is global; literature is obtained from 125 countries. The temporal coverage spans 30 years, 180,000 organic research abstracts, along with the addition of 8000 records per year. Linking to full text articles, guided searches, broad subject categorization along with subject refinement are also provided. The editorial advisory board of this database also commission reviews pertaining to organic farming.&lt;ref name=organic&gt;
{{cite web
  | title =Organic Research Database
  | work =Description and bibliographic information 
  | publisher =CABI 
  | year =2011  
  | url =http://www.cabi.org/organicresearch/default.aspx?site=154&amp;page=932
  | accessdate =2011-01-03}}&lt;/ref&gt;&lt;ref name=usda&gt;
{{cite web
  | title =Primary Research and Literature Databases
  | work = focus on sustainable and alternative agricultural topics
  | publisher =[[USDA]] - [[National Agriculture Library]] - [[AFSIC]] 
  | year =2011  
  | url =http://afsic.nal.usda.gov/nal_display/index.php?info_center=2&amp;tax_level=2&amp;tax_subject=288&amp;level3_id=0&amp;level4_id=0&amp;level5_id=0&amp;topic_id=1597&amp;&amp;placement_default=0
  | accessdate =2011-01-03}}&lt;/ref&gt;

==CABI full text repository==
'''''CABI full text repository''''' is integrated into all ''CABI databases'' including CAB Abstracts, and Global Health. Both of these are online and print journals. Coverage includes 70,000 full text articles, through agreements with third party publishers. Eighty percent of the content is exclusive to CABI.&lt;ref name=full-text/&gt;  

The full text repository is made up of fifty percent journal articles, and equal percentage of conference (proceeding) papers, and other accessible literature is also included. Eighty percent of the articles are in English and coverage includes 56 countries. Also included in this database are relevant but hard to find materials which crosses disciplines consisting of [[agriculture]], [[health sciences|health]] and the [[life sciences]]. Main stream literature and hard to find materials of equal relevance are given equal access.&lt;ref name=full-text&gt;{{cite web
  | title =CABI full text 
  | work = 
  | publisher =CABI 
  | date =March 2010  
  | url =http://www.cabi.org/default.aspx?site=170&amp;page=1016&amp;pid=2227 
  | accessdate =2010-07-18}}&lt;/ref&gt;

''CABI full text repository'' is indexed in other databases, which also serve as access points, consisting of ''Web of Knowledge (Thomson Reuters)'', ''CAB Direct'', ''OvidSP, Dialog, Dimdi, and EBSCOhost''.

==References==
{{Reflist}}

[[Category:Bibliographic database providers]]
[[Category:Bibliographic indexes]]
[[Category:Citation indices]]
[[Category:Environmental science]]
[[Category:Global health]]</text>
      <sha1>8dw437buj6iqmxce5u96xkslor8i58x</sha1>
    </revision>
  </page>
  <page>
    <title>Citation index</title>
    <ns>0</ns>
    <id>423362</id>
    <revision>
      <id>642274976</id>
      <parentid>641821376</parentid>
      <timestamp>2015-01-13T07:27:25Z</timestamp>
      <contributor>
        <username>Steel1943</username>
        <id>2952402</id>
      </contributor>
      <comment>Template update</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13986">A '''citation index''' is a kind of [[bibliographic database]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]].

==History==

The earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study.  The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.&lt;ref&gt;Bella Hass Weinberg, &quot;The Earliest Hebrew Citation Indexes&quot; in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998,  p. 51''ff''&lt;/ref&gt;&lt;ref&gt;Bella Hass Weinberg, &quot;Predecessors of Scientific Indexing Structures in the Domain of Religion&quot; in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''&lt;/ref&gt; Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.

In English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.&lt;ref name='shapiro'/&gt;

The first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].&lt;ref name='shapiro'&gt;Fred R. Shapiro, &quot;Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature&quot; ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)&lt;/ref&gt;

==Major citation indexing services==
{{main|Indexing and abstracting service}}
{{main cat|Citation indices}}

General-purpose academic citation indexes include:

*ISI (now part of [[Thomson Reuters]]) publishes the ISI citation indexes in print and [[compact disc]]. They are now generally accessed through the Web under the name '' [[Web of Science]]'', which is in turn part of the group of databases in the ''[[Web of Knowledge]].''
*[[Elsevier]] publishes [[Scopus]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].
*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.
Each of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: the ISI databases and Scopus are available by subscription (generally to libraries).

In addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.

==Citation analysis==
{{main|Citation analysis}}
{{merge|section=yes|Citation analysis|date=December 2013}}
{{duplication|dupe=Citation analysis|date=December 2013}}

While citation indexes were originally designed for [[information retrieval]], they are increasingly used for [[bibliometrics]] and other studies involving research evaluation. Citation data is also the basis of the popular [[journal impact factor]].

There is a large body of literature on [[citation analysis]], sometimes called [[scientometrics]], a term invented by [[Vasily Nalimov]], or more specifically [[bibliometrics]]. The field blossomed with the advent of the [[Science Citation Index]], which now covers source literature from 1900 on. The leading journals of the field are ''[[Scientometrics]],'' ''Informetrics,'' and the ''[[Journal of the American Society of Information Science and Technology]]''. [[American Society for Information Science and Technology|ASIST]] also hosts an [[electronic mailing list]] called SIGMETRICS at  ASIST.&lt;ref&gt;{{cite web | title=The American Society for Information Science &amp; Technology | work=The Information Society for the Information Age | url=http://www.asis.org| accessdate=2006-05-21}}&lt;/ref&gt; This method is undergoing a resurgence based on the wide dissemination of the Web of Science and Scopus subscription databases in many universities, and the universally available free citation tools such as  [[CiteBase]],  [[CiteSeerX]], [[Google Scholar]], and the former [[Windows Live Academic]] (now available with extra features as [[Microsoft Academic Search]]).

[[Legal citation]] analysis is a citation analysis technique for analyzing [[legal documents]] to facilitate the understanding of the inter-related regulatory compliance documents by the exploration the citations that connect provisions to other provisions within the same document or between different documents. Legal citation analysis uses a [[citation graph]] extracted from a regulatory document, which could supplement [[E-discovery]] - a process that leverages on technological innovations in [[big data analytics]].&lt;ref&gt;[http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=5070630&amp;tag=1 ]{{dead link|date=December 2013}}&lt;/ref&gt;&lt;ref&gt;Mohammad Hamdaqa and A. Hamou-Lhadj, &quot;Citation Analysis: An Approach for Facilitating the Understanding and the Analysis of Regulatory Compliance Documents&quot;,  In Proc. of the 6th International Conference on Information Technology, Las Vegas, USA&lt;/ref&gt;&lt;ref name=BD-HB-R-01&gt;{{cite web|title=E-Discovery Special Report: The Rising Tide of Nonlinear Review|url=http://hudsonlegalblog.com/e-discovery/e-discovery-special-report-rising-tide-nonlinear-review.html|publisher=[[Hudson Global]]|accessdate=1 July 2012}} by Cat Casey and Alejandra Perez&lt;/ref&gt;&lt;ref name=BD-HB-R-02&gt;{{cite web|title=What Technology-Assisted Electronic Discovery Teaches Us About The Role Of Humans In Technology - Re-Humanizing Technology-Assisted Review|url=http://www.forbes.com/sites/benkerschberg/2012/01/09/what-technology-assisted-electronic-discovery-teaches-us-about-the-role-of-humans-in-technology/|publisher=[[Forbes]]|accessdate=1 July 2012}}&lt;/ref&gt;

===History===
In a 1965 paper, [[Derek J. de Solla Price]] described the inherent linking characteristic of the SCI as &quot;Networks of Scientific Papers&quot;.&lt;ref&gt;{{cite journal | author=Derek J. de Solla Price | title=Networks of Scientific Papers | journal=[[Science (journal)|SCIENCE]] | date=July 30, 1965 | volume=149 | issue=3683| pages=510&amp;ndash;515 | url=http://garfield.library.upenn.edu/papers/pricenetworks1965.pdf | pmid=14325149 | doi=10.1126/science.149.3683.510|format=PDF}}&lt;/ref&gt; The links between citing and cited papers became dynamic when the SCI began to be published online. The [[Social Sciences Citation Index]] became one of the first databases to be mounted on the [[Dialog]] system&lt;ref&gt;{{cite web | title=Dialog, A Thomson Business | work=&quot;Dialog invented online information services&quot; | url=http://www.dialog.com| accessdate=2006-05-21}}&lt;/ref&gt; in 1972. With the advent of the [[CD-ROM]] edition, linking became even easier and enabled the use of [[bibliographic coupling]] for finding related records. In 1973, Henry Small published his classic work on [[Co-Citation analysis]] which became a [[self-organizing]] classification system that led to [[document clustering]] experiments and eventually an &quot;Atlas of Science&quot; later called &quot;Research Reviews&quot;.

The inherent topological and graphical nature of the worldwide citation network which is an inherent property of the [[scientific literature]] was described by [[Ralph Garner]] ([[Drexel University]]) in 1965.&lt;ref&gt;http://www.garfield.library.upenn.edu/rgarner.pdf&lt;/ref&gt;

The use of citation counts to rank journals was a technique used in the early part of the nineteenth century but the systematic ongoing measurement of these counts for scientific journals was initiated by Eugene Garfield at the Institute for Scientific Information who also pioneered the use of these counts  to rank authors and [[academic paper|papers]].  In a landmark paper of 1965 he and [[Irving Sher]] showed the correlation between citation frequency and eminence in demonstrating that [[Nobel Prize]] winners published five times the average number of papers while their work was cited 30 to 50 times the average. In a long series of essays on the Nobel and other prizes Garfield reported this phenomenon.  The usual summary measure is known as [[impact factor]], the number of citations to a journal for the previous two years, divided by the number of articles published in those years. It is widely used, both for appropriate and inappropriate purposes—in particular, the use of this measure alone for  ranking authors and papers is therefore [[Impact factor#|quite controversial.]]

In an early study in 1964 of the use of Citation Analysis in writing the history of [[DNA]], Garfield and Sher demonstrated the potential for generating [[historiograph]]s, [[topological map]]s of the most important steps in the history of scientific topics. This work was later automated by E. Garfield, [[A. I. Pudovkin]] of the [[Institute of Marine Biology]], [[Russian Academy of Sciences]] and [[V. S. Istomin]] of [[Center for Teaching, Learning, and Technology]], [[Washington State University]] and led to the creation of the [[Histcite|HistCite]] &lt;ref&gt;{{cite web | author=Eugene Garfield, A. I. Pudovkin,  V. S. Istomin | year=2002 | title=Algorithmic Citation-Linked Historiography—Mapping the Literature of Science | work=Presented the ASIS&amp;T 2002: Information, Connections and Community. 65th Annual Meeting of ASIST in Philadelphia, PA. November 18–21, 2002  | url=http://www.garfield.library.upenn.edu/papers/asis2002/asis2002presentation.html | accessdate=2006-05-21}}&lt;/ref&gt; software around 2002.

Automatic citation indexing was introduced in 1998 by [[Lee Giles]], [[Steve Lawrence]] and [[Kurt Bollacker]] &lt;ref&gt;C.L. Giles, K. Bollacker, S. Lawrence, &quot;CiteSeer: An Automatic Citation Indexing System,&quot; DL'98 Digital Libraries, 3rd ACM Conference on Digital Libraries, pp. 89-98, 1998.&lt;/ref&gt; and enabled automatic algorithmic extraction and grouping of citations for any digital academic and scientific document. Where previous citation extraction was a manual process, citation measures could now scale up and be computed for any scholarly and scientific field and document venue, not just those selected by organizations such as ISI. This led to the creation of new systems for public and automated citation indexing, the first being [[CiteSeer]] (now [[CiteSeerX]], soon followed by Cora, which focused primarily on the field of [[computer science]] and [[information science]]. These were later followed by large scale academic domain citation systems such as the Google Scholar and Microsoft Academic. Such autonomous citation indexing is not yet perfect in citation extraction or citation clustering with an error rate estimated by some at 10% though a careful statistical sampling has yet to be done. This has resulted in such authors as [[Ann Arbor, Michigan|Ann Arbor]], [[Milton Keynes]], and [[Walton Hall, Milton Keynes|Walton Hall]] being credited with extensive academic output.&lt;ref name=&quot;pmid18354457&quot;&gt;{{cite journal |author=Postellon DC |title=Hall and Keynes join Arbor in the citation indexes |journal=[[Nature (journal)|Nature]] |volume=452 |issue=7185 |page=282 |date=March 2008 |pmid=18354457 |doi=10.1038/452282b}}&lt;/ref&gt;  SCI claims to create automatic citation indexing through purely programmatic methods. Even the older records have a similar magnitude of error.

==See also==
* [[Impact factor]]
* [[Citation impact]]
* [[Eigenfactor]]
* [[Microsoft Academic Search]]
* [[Google Scholar]]
* [[Scopus]]
* [[H-index]] or [[Hirsch number]]
* [[Citation analysis]]
* [[Acknowledgment index]]
* [[CiteSeer]]
* [[CiteSeerX]]
* [[Scientific journal]]
* [[Science Citation Index]]
* [[Indian Citation Index]]

==References==
{{Reflist}}

==External links ==
* Official [http://admin-apps.isiknowledge.com/JCR/JCR Journal Citation Report] from the [http://www.isinet.com ISI website]
* [http://www.librijournal.org/2005-4toc.html Google Scholar: The New Generation of Citation Indexes]
* [http://www.atlasofscience.net/ Atlas of Science: Mapping Science by means of citation relations]
* [http://www.dlib.org/dlib/september05/bauer/09bauer.html An Examination of Citation Counts in a New Scholarly Communication Environment]
* [http://cids.fc.ul.pt/ CIDS] online tool that calculates the h-index and [[g-index]] based on [[Google Scholar]] data and discerning self-citations

{{DEFAULTSORT:Citation Index}}
[[Category:Academic publishing]]
[[Category:Bibliometrics]]
[[Category:Library science]]
[[Category:Reputation management]]
[[Category:Citation indices]]</text>
      <sha1>l4aoppwafgje69enxl6dd3iimdpypnm</sha1>
    </revision>
  </page>
  <page>
    <title>SCImago Journal Rank</title>
    <ns>0</ns>
    <id>24429136</id>
    <revision>
      <id>625522550</id>
      <parentid>625522433</parentid>
      <timestamp>2014-09-14T14:04:09Z</timestamp>
      <contributor>
        <ip>69.250.123.49</ip>
      </contributor>
      <comment>/* Criticism */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4536">'''SCImago Journal Rank''' (SJR indicator) is a measure of scientific influence of [[academic journal|scholarly journal]]s that accounts for both the number of [[citation]]s received by a journal and the importance or prestige of the journals where such citations come from. The SJR indicator is a variant of the [[centrality|eigenvector centrality measure]] used in network theory. Such measures establish the importance of a node in a network based on the principle that connections to high-scoring nodes contribute more to the score of the node. The SJR indicator, which is inspired by the [[PageRank]] algorithm, has been developed to be used in extremely large and heterogeneous journal citation networks. It is a size-independent indicator and its values order journals by their &quot;average prestige per article&quot; and can be used for journal comparisons in science evaluation processes.

The ''SJR indicator'' is a free journal metric which uses an algorithm similar to [[PageRank]] and provides an alternative to the [[impact factor]] (IF), which is based on data from the [[Science Citation Index]].&lt;ref&gt;{{cite journal | url = http://www.nature.com/news/2008/080102/full/451006a.html | title= Free journal-ranking tool enters citation market | journal = [[Nature (journal)|Nature]] | date= 2 January 2008 | volume= 451 | issue= 6 | doi= 10.1038/451006a | author = Declan Butler | pmid= 18172465 | pages= 6 |accessdate=14 May 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | url = http://www.fasebj.org/cgi/content/short/22/8/2623 | title = Comparison of SCImago journal rank indicator with journal impact factor | author= Matthew E. Falagas et al | doi = 10.1096/fj.08-107938 | journal = [[The FASEB Journal]] | year = 2008 | issue = 22 | pages = 2623–2628 | pmid = 18408168 | volume = 22 }}&lt;/ref&gt; Average citations per document in a 2-year period, abbreviated as Cites per Doc. (2y), is another index that measures the scientific impact of an average article published in the journal. It is computed using the same formula that journal [[impact factor]] ([[Thomson Reuters]]).

== Rationale ==
If scientific impact is considered related to the number of endorsements, in the form of citations, a journal receives, then prestige can be understood as a combination of the number of endorsements and the prestige or importance of the journals issuing them. The ''SJR indicator'' assigns different values to citations depending on the importance of the journals where they come from. This way, citations coming from highly important journals will be more valuable and hence will provide more prestige to the journals receiving them. The calculation of the ''SJR indicator'' is very similar to the ''[[Eigenfactor]] score'', with the former being based on the [[Scopus]] database and the latter on the ISI [[Web of Science]] database.&lt;ref&gt;{{cite web | title=SCImago Journal &amp; Country Rank (SJR) as an alternative to Thomson Reuters's Impact Factor and EigenFactor | url=http://www.scimagojr.com/news.php?id=41 | date=21 Aug 2008 | accessdate=20 September 2012}}&lt;/ref&gt;

== Computation ==
The SJR indicator computation is carried out using an iterative [[algorithm]] that distributes prestige values among the journals until a steady-state solution is reached. The SJR algorithm begins by setting an identical amount of prestige to each journal, then using an iterative procedure, this prestige is redistributed in a process where journals transfer their achieved prestige to each other through citations. The process ends up when the difference between journal prestige values in consecutive iterations do not reach a minimum threshold value any more. The process is developed in two phases, (a) the computation of ''Prestige SJR'' (''PSJR'') for each journal: a size-dependent measure that reflects the whole journal prestige, and (b) the normalization of this measure to achieve a size-independent measure of prestige, the ''SJR indicator''.

== See also ==
* [[Journal Citation Reports]]
* [[Citation index]]
* [[Eigenfactor]]

== References ==
{{reflist}}

== External links ==
* {{official website|http://www.scimagojr.com/}}
* [http://blogs.openaccesscentral.com/blogs/bmcblog/entry/scimago_a_new_source_of SCImago – a new source of journal metrics offering a wealth of free data on open access journals]
* [http://www.earlham.edu/~peters/fos/2008/01/more-on-scimago-journal-rank-v-impact.html More on SCImago Journal Rank v. Impact Factors]

{{DEFAULTSORT:Scimago journal rank}}
[[Category:Citation indices]]
[[Category:Academic publishing]]</text>
      <sha1>mtr076pzgk3t9f052gg1shw9gme1cym</sha1>
    </revision>
  </page>
  <page>
    <title>Russian Science Citation Index</title>
    <ns>0</ns>
    <id>35108736</id>
    <revision>
      <id>564703231</id>
      <parentid>564694707</parentid>
      <timestamp>2013-07-17T21:08:47Z</timestamp>
      <contributor>
        <username>Randykitty</username>
        <id>17843555</id>
      </contributor>
      <comment>Undid revision 564694707 by [[Special:Contributions/109.165.125.248|109.165.125.248]] ([[User talk:109.165.125.248|talk]])not directly relevant here</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2389">{{primary sources|date=March 2012}}
'''Russian Science Citation Index''' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.

== Purpose ==
From 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain &quot;invisible&quot; and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.

== Functionality ==
In Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: 
* Scientific capacity and effectiveness of research, and
* Publication activity
through the following indicators:
* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,
* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and
* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.

== See also ==
*[[List of academic databases and search engines]]
*[[Science Citation Index]]
*[[Scopus]]

==External links==
* [http://elibrary.ru/ Scientific Electronic Library]


[[Category:Citation indices]]</text>
      <sha1>s53x3zdxitizxrwy2g3a08hgbclgc2c</sha1>
    </revision>
  </page>
  <page>
    <title>Latin American Bibliography</title>
    <ns>0</ns>
    <id>20378765</id>
    <revision>
      <id>641841774</id>
      <parentid>641424540</parentid>
      <timestamp>2015-01-10T07:32:35Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>added [[Category:Citation indices]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7721">{{Multiple issues|
{{orphan|date=March 2010}}
{{advert|date=August 2010}}
}}

The '''Latin American Bibliography''' refers to the set of [[databases]] and information services on [[academic journals]] from [[Latin America]] and the [[Caribbean]] created by the [[National Autonomous University of Mexico]] (UNAM) in the decade of the seventies.{{Clarify|date=August 2009}}

Nowadays, the Latin-American Bibliography is composed by the following databases: CLASE (''Latin-American Citations in [[Social Sciences]] and [[Humanities]]''); PERIODICA (''Index of Latin-American Journals in [[Science]]''); [[Latindex]] (''Regional Co-operative Information System for Scholarly Journals from [[Latin America]], the Caribbean, Spain and Portugal'').

These databases were created by a group of information professionals, who identified the need to register, preserve and give access to the Latin-American knowledge published in the main academic [[Academic journal|journals]] of the region. Within UNAM, the fostering institution of these information products was the Science and Humanities Information Center (CICH) created in 1971.

For the size of its collection of Latin-American journals, for the quantity of compiled records and for the duration and consistency of the project, the Latin-American Bibliography produced in the UNAM constitutes one of the most valuable resources for scholars and experts specializing in Latin-American affairs.{{Citation needed|date=August 2009}}

==Products==

Three databases are available through the web site of UNAM’s General Directorate for Libraries [http://dgb.unam.mx General Directorate for Libraries]:

'''CLASE''' (''Latin-American Citations in Social Sciences and Humanities''). Bibliographical database, with more than 280,000 records, of which nearly 14,000 provide abstracts and links to the full text of the documents. It includes more than 1,400 journals specializing in Social Sciences, Humanities and Arts, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/clase.html CLASE website]

'''PERIODICA''' (Index of Latin-American Journals in Science). Bibliographical database with more than 315,000 records, of which near 60,000 provide abstracts and links to the full text of the documents. The database indexes more than 1,500 journals specializing in Science and Technology, from more than 20 countries of Latin America and the Caribbean. Documents not available in full text can be retrieved through the Document Supply Service of the Latin-American Serials Collection (Hemeroteca Latinoamericana) of the DGB. Direct link: [http://dgb.unam.mx/periodica.html PERIODICA website]

'''[[Latindex]]''' (Regional Co-operative Information System for Scholarly Journals from Latin America, the Caribbean, Spain and Portugal). This initiative provides relevant information and data of the scholarly journals edited in the [[Iberoamerica]]n region. Three databases are produced through the collaborative work of the member institutions: '''Directory''',: with more than 17,000 records; '''Catalogue''', with more than 3,500 selected journals that fulfill international quality criteria and an '''Index of Electronic Journals''', offering nearly 3,000 links to available resources in full text. Direct link: [http://www.latindex.org Latindex website]

Currently, the Department of Latin-American Bibliography contributes to the production of two other Latin-American information products:

'''ASFA''' (''Aquatic Sciences and Fisheries Abstracts''). Bibliographical international database on Aquatic Sciences and [[Fisheries]], covering subject areas such as [[technology]] and [[Public administration|administration]] of the marine environments and its resources (salt and sweet waters), including its socioeconomic and juridical aspects. It offers abstracts of articles published in approximately 7,000 periodic publications, besides thesis, monographs and other not conventional literature. The contribution relative to the Mexican journals is produced in the Department of Latin-American Bibliography from 1981. Link: [http://www.fao.org/fishery/asfa ASFA website]

'''[[SciELO]] Mexico''' (''Scientific Electronic Library Online''). Open access electronic journals collection that includes a selection of the most recognized academic publications of the country in all areas of knowledge, previously selected accordingly to the most accepted criteria related to content and editorial standards. Currently it offers the full text of more than 2,500 articles from 28 academic Mexican journals. Direct link: [http://www.scielo.org.mx/scielo.php Scielo México website]

Over the time, other databases were produced by the Department of Latin-American Bibliography during its more than 30 years of existence, namely:

'''BLAT''' (''Latin-American Bibliography I and II''), with information compiled from international sources, mainly documents from Latin-American origin (produced by Latin American authors and institutions) or those in which their object of study was related to the region. The database ceased in 1997. Another one was '''MEXINV''', as a subset of CLASE, offered bibliographical records of documents relative only to [[Mexico]]. This database ceased in the decade of the nineties.

===Institution===

Currently, the databases described above are produced by the Department of Latin-American Bibliography, part of the Assistant Office for Information Services of the General Directorate for Libraries (DGB) of the National Autonomous University of Mexico (UNAM). The original databases (BLAT, CLASE, PERIODICA, MEXINV and Latindex) were created by the Science and Humanities Information Center (CICH). Since the incorporation of the CICH to UNAM’s General Directorate for Libraries in 1997, this institution acts as Responsible Editor.

==References==

*Alonso Gamboa, José Octavio. Servicios, productos, docencia e investigación en información: la experiencia del Centro de Información Científica y Humanística de la Universidad Nacional Autónoma de México. Ciencias de la Información, vol. 24, no. 4, diciembre, 1993. p.&amp;nbsp;201-208. URL: [http://www.bibliociencias.cu/gsdl/cgi-bin/library?e=d-000-00---0revistas--00-0-0--0prompt-10---4------0-1l--1-es-50---20-about---00031-001-1-0utfZz-8-00&amp;cl=CL2.772&amp;d=HASH01caacf727585263378aa110&amp;x=1]

*Alonso Gamboa, José Octavio. Accesso a revistas latinoamericanas en Internet. Una opción a través de las bases de datos Clase y Periódica. Ciencia da Informação, vol. 27, no. 1, Janeiro-abril, 1998, p.&amp;nbsp;90-95. URL: http://www.scielo.br/pdf/ci/v27n1/12.pdf

*Alonso Gamboa, José Octavio y Felipe Rafael Reyna Espinosa. Compilación de datos bibliométricos regionales usando las bases de datos CLASE y PERIÓDICA. Revista Interamericana de Bibliotecología, 2005. Vol. 28, no. 1, enero-junio: 63-78. URL: http://bibliotecologia.udea.edu.co/revinbi/Numeros/2801/doc3_28.html

*Russell, Jane M.; Madera-Jaramillo, María J.; Hernández- García, Yoscelina y Ainsworth, Shirley. Mexican collaboration networks in the international and regional arenas. En: Kretschmer, H. &amp; Havemann, F. (Eds.): Proceedings of WIS 2008, Berlin. Fourth International Conference on Webometrics, Informetrics and Scientometrics &amp; Ninth COLLNET Meeting, Humboldt-Universität zu Berlin, Institute for Library and Information Science (IBI). URL: http://www.collnet.de/Berlin-2008/RussellWIS2008mcn.pdf

[[Category:Bibliographic databases]]
[[Category:Scientific databases]]
[[Category:Citation indices]]</text>
      <sha1>1khu8cxybe9d856ik6wpcogzzzdrndb</sha1>
    </revision>
  </page>
  <page>
    <title>Web of Knowledge</title>
    <ns>0</ns>
    <id>6853791</id>
    <revision>
      <id>642634319</id>
      <parentid>642634206</parentid>
      <timestamp>2015-01-15T18:15:30Z</timestamp>
      <contributor>
        <username>Randykitty</username>
        <id>17843555</id>
      </contributor>
      <minor/>
      <comment>/* Included databases */ typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5147">{{Mergeto|Web of Science|date=June 2014|discuss=Talk:Web of Science#Merge}}
[[File:Web of Science Logo.png|thumb|The current Web of Science logo]]
[[Image:ISI Web of Knowledge updated.png|thumb|400px|An example search result from Web of Knowledge version 3.0]]

'''Web of Knowledge''' (formerly known as [[Institute for Scientific Information|ISI]] Web of Knowledge) is an academic [[citation index]]ing and search service, which is combined with web linking and is provided by [[Thomson Reuters]]. Web of Knowledge covers the sciences, [[social science]]s, arts and [[humanities]]. It provides [[bibliography|bibliographic]] content and tools to access, analyze, and manage research information. Multiple databases can be searched simultaneously.&lt;ref name=describe/&gt;&lt;ref name=tutor&gt;[http://science.thomsonreuters.com/tutorials/wok4/wok4tut3.html Tutorial]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24&lt;/ref&gt;

==Overview==
Web of Knowledge is described as a unifying research tool which enables the user to acquire, analyze, and disseminate database information in a timely manner. This is accomplished because of the creation of a common vocabulary, called [[Ontology (information science)|ontology]], for varied search terms and varied data. Moreover, search terms generate related information across categories.

Acceptable content for Web of Knowledge is determined by an evaluation and selection process based on the following criteria: impact, influence, timeliness, [[peer review]], and geographic representation.&lt;ref name=describe/&gt;

===Search and analysis===
&lt;!-- Deleted image removed: [[File:ISI Web of knowledge logo.jpg|thumb||Former Web of Knowledge logo]] --&gt;

Web of Knowledge employs various search and analysis capabilities. First, citation indexing is employed, which is enhanced by the capability to search for results across disciplines. The influence, impact, history, and [[methodology]] of an idea can be followed from its first instance, notice, or referral to the present day. This technology points to a deficiency with the [[Index term|keyword]]-only method of searching. 

Second, subtle trends and patterns relevant to the literature or research of interest, become apparent. Broad trends indicate significant topics of the day, as well as the history relevant to both the work at hand, and particular areas of study. 

Third, trends can be [[mathematical modeling|graphically]] represented.&lt;ref name=describe&gt;[http://thomsonreuters.com/content/science/pdf/Web_of_Knowledge_factsheet.pdf Overview and Description]. ISI Web of Knowledge. Thomson Reuters. 2010. Accessed on 2010-06-24&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://wokinfo.com/realfacts/qualityandquantity/|title=Web of Knowledge &gt; Real Facts &gt; Quality and Quantity|accessdate = 2010-05-05}}&lt;/ref&gt;

=== Content ===
The combined databases includes the following:
*23,000 [[Academic journal|academic]] and [[scientific journal]]s (including [[Web of Science]] journal listings)
*23,000,000 [[patent]]s
*110,000 conference [[proceedings]]
*9,000 websites
*Coverage from the year 1900 to present day (with Web of Science)
*Over 40 million source items
*Integrated and simultaneous searching across multiple databases&lt;ref name=describe/&gt;

=== Included databases ===
The Web of Knowledge suite encompasses the following databases:&lt;ref name=dbase-List&gt;{{Cite web| last =''ISI Web of Knowledge''| title =Suite of databases| publisher =Thomson Reuters| year =2010| url = http://thomsonreuters.com/products_services/science/science_products/a-z/isi_web_of_knowledge?parentKey=555184 | format =List of databases that are part of the Web of Knowledge suite.| accessdate =2010-06-24}}&lt;/ref&gt;&lt;ref name=AtoZ&gt;{{Cite web| last = ISI Web of Knowledge platform| title =Available databases A to Z| publisher =Thomson Reuters| year =2010| url =http://wokinfo.com/products_tools/products/ | format =Choose databases on method of discovery and analysis| accessdate =2010-06-24}}&lt;/ref&gt;&lt;ref&gt;[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]&lt;/ref&gt;
{{columns-list|colwidth=30em|
*[[Biological Abstracts]]
*[[Biosis Previews]] 
*[[CAB Abstracts]]
*[[CAB Direct|CAB Global Health]]
*[[Chinese Science Citation Database]]
*[[Conference Proceedings Citation Index]] 
*[[Current Contents|Current Contents Connect]]
*[[Data Citation Index]]
*[[Derwent Innovations Index]]
*[[Essential Science Indicators]]
*[[Food Science and Technology Abstracts]]
*[[Inspec]] 
*[[ISI Highly Cited]]
*[[Journal Citation Reports]]
*[[MEDLINE]] 
*[[Web of Science]]
**[[Arts &amp; Humanities Citation Index]]
**[[Book Citation Index]] 
**[[Current Chemical Reactions]]
**[[Index Chemicus]]
**[[Science Citation Index Expanded]]
**[[Social Sciences Citation Index]]
*[[The Zoological Record]]
}}

==See also==
*[[List of academic journal search engines]]

==References==
{{Reflist|30em}}

==External links==
* {{Official website|http://wokinfo.com/}}

{{Thomson Reuters}}

[[Category:Bibliographic databases]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]
[[Category:Citation indices]]
[[Category:Scholarly search services]]</text>
      <sha1>r0wv6p447je6f51hwn2qv7xthzid0j6</sha1>
    </revision>
  </page>
  <page>
    <title>Web of Science</title>
    <ns>0</ns>
    <id>13227429</id>
    <revision>
      <id>647055219</id>
      <parentid>642341740</parentid>
      <timestamp>2015-02-14T05:14:17Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor/>
      <comment>/* Coverage */replaced: Over view → Overview using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12510">{{Merge from|Web of Knowledge|date=December 2014|discuss=Talk:Web of Science#Merge}}
{{Infobox Bibliographic Database
|title = Web of Science
|image = [[File:Web of science next generation.png|thumb|350px|Web of Science]]
|caption = 
|producer = Thomson Reuters 
|country = United States 
|history = 
|languages = 
|providers = Various institutions and commercial organizations 
|cost = 
|disciplines = Science, social science, arts, humanities (supports 256 disciplines) 
|depth = citation indexing,  author, topic title, subject keywords, abstract,  periodical title, author's address, publication year 
|formats = full text articles, reviews, editorials, chronologies, abstracts, proceedings (journals and book-based ), technical papers 
|temporal = 1900 to present 
|geospatial = 
|number = 90 million + 
|updates = 
|p_title = 
|p_dates = 
|ISSN =
|web =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science 
|titles =http://thomsonreuters.com/content/science/pdf/Web_of_Science_factsheet.pdf
}}
'''Web of Science''' (WoS, previously known as [[Web of Knowledge]]) is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] that provides a comprehensive citation search. It gives access to multiple databases that reference cross-disciplinary research, which allows for in-depth exploration of specialized sub-fields within an [[academic discipline|academic or scientific discipline]].&lt;ref&gt;Drake, Miriam A. Encyclopedia of Library and Information Science. New York, N.Y.: Marcel Dekker, 2004.&lt;/ref&gt;

==Background==
A citation index is built on the fact that citations in science serve as linkages between similar research items, and lead to matching or related scientific literature, such as [[academic journal|journal articles]], [[conference proceedings]], abstracts, etc. In addition, literature which shows the greatest impact in a particular field, or more than one discipline, can be easily located through a citation index. For example, a paper's influence can be determined by linking to all the papers that have cited it. In this way, current trends, patterns, and emerging fields of research can be assessed. [[Eugene Garfield]], the &quot;father of citation indexing of academic literature,&quot;&lt;ref&gt;Jacso, Peter. The impact of Eugene Garfield through the prizm of Web of Science. Annals of Library and Information Studies, Vol. 57, September 2010, P. 222. [http://nopr.niscair.res.in/bitstream/123456789/10235/4/ALIS%2057%283%29%20222-247.pdf PDF]&lt;/ref&gt; who launched the [[Science Citation Index]] (SCI), which in turn led to the Web of Science,&lt;ref&gt;Garfield, Eugene, Blaise Cronin, and Helen Barsky Atkins. The Web of Knowledge: A Festschrift in Honor of Eugene Garfield. Medford, N.J.: Information Today, 2000.&lt;/ref&gt; wrote: 

{{Quote|Citations are the formal, explicit linkages between papers that have particular points in common. A citation index is built around these linkages. It lists publications that have been cited and identifies the sources of the citations. Anyone conducting a literature search can find from one to dozens of additional papers on a subject just by knowing one that has been cited. And every paper that is found provides a list of new citations with which to continue the search.
The simplicity of citation indexing is one of its main strengths. &lt;ref&gt;Garfield, Garfield, Eugene. Citation indexing: Its theory and application in science, technology, and humanities. New York: Wiley, 1979, P. 1. [http://garfield.library.upenn.edu/ci/chapter1.PDF PDF]&lt;/ref&gt;}}

==Coverage==
[[File:Web_of_Science_Core_Collection.png|thumb|200px|Accessing the Web of Science via the [[Web of Knowledge]]]]
Expanding the coverage of Web of Science, in November 2009 Thomson Reuters introduced ''Century of Social Sciences''. This service contains files which trace social science research back to the beginning of the 20th century,&lt;ref name=InfoToNov2009&gt;&quot;''Thomson Reuters introduces century of social sciences''&quot;. Information Today 26.10 (2009): 10. General OneFile. Web. 23 June 2010.  [http://find.galegroup.com/gps/infomark.do?&amp;contentSet=IAC-Documents&amp;type=retrieve&amp;tabID=T003&amp;prodId=IPS&amp;docId=A211794482&amp;source=gale&amp;srcprod=ITOF&amp;userGroupName=mlin_c_marlpl&amp;version=1.0  Document URL].&lt;/ref&gt;&lt;ref name=ComLibNov2009&gt;Thomson Reuters introduces century of social sciences.&quot; Computers in Libraries 29.10 (2009): 47. General OneFile. Internet. 23 June 2010. [http://find.galegroup.com/gps/infomark.do?&amp;contentSet=IAC-Documents&amp;type=retrieve&amp;tabID=T003&amp;prodId=IPS&amp;docId=A211236981&amp;source=gale&amp;srcprod=ITOF&amp;userGroupName=mlin_c_marlpl&amp;version=1.0 Document URL]&lt;/ref&gt; and Web of Science now has indexing coverage from the year 1900 to the present.&lt;ref name=oview&gt;
{{Cite web |title =Overview - Web of Science| publisher =Thomson Reuters| year = 2010
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science
  | format =Overview of coverage gleaned from promotional language.  
  | accessdate =2010-06-23}}&lt;/ref&gt;&lt;ref name=UoOL&gt;
{{Cite web| last = Lee| first =Sul H.| title =Citation Indexing and ISI's Web of Science 
  | publisher =The University of Oklahoma Libraries| year =2010
  | url =http://www.ou.edu/webhelp/librarydemos/isi/ | format =Discussion of finding literature manually. Description of [[citation index]]ing, and Web of Science.| accessdate =2010-06-23}}&lt;/ref&gt; The multidisciplinary coverage of the Web of Science encompasses over 50,000 scholarly books, 12,000 journals and 160,000 conference proceedings&lt;ref name=&quot;Web of Science&quot;&gt;[http://wokinfo.com/citationconnection/realfacts/#regional Web of Science. Thomson Reuters, 2014]&lt;/ref&gt; (as of September 3, 2014). The selection is made on the basis of [[impact factor|impact evaluations]] and comprise [[open-access journal]]s, spanning multiple [[academic discipline]]s. The coverage includes: the [[science]]s, [[social science]]s, [[the arts|arts]], and humanities, and goes across disciplines.&lt;ref name=oview/&gt;&lt;ref name=facts/&gt; However, Web of Science does not index all journals, and its coverage in some fields is less complete than in others.

Furthermore, as of September 3, 2014 the total file count of the Web of Science was 90 million records, which included over a billion cited references. This citation service on average indexes around 65 million items per year, and it is described as the largest accessible citation database.&lt;ref name=facts&gt;[http://wokinfo.com/citationconnection/  Bulleted fact sheet]. Thomson Reuters. 2014.&lt;/ref&gt;

Titles of foreign-language publications are translated into English and so cannot be found by searches in the original language.&lt;ref name=harvard-search&gt;{{Cite web
  |title =Some Searching Conventions
  | publisher =President and Fellows of Harvard College   | date = December 3, 2009   | url =http://hcl.harvard.edu/research/guides/citationindex/#some   | format =    | accessdate =2010-06-23}}&lt;/ref&gt;

==Citation databases==
Web of Science consist of seven online databases:&lt;ref name=included/&gt;&lt;ref name=&quot;Web of Science&quot;&gt;[http://wokinfo.com/media/pdf/WoSFS_08_7050.pdf Jo Yong-Hak. Web of Science. Thomson Reuters, 2013]&lt;/ref&gt;
*[[Conference Proceedings Citation Index]] covers more than 160,000 conference titles in the Sciences starting from 1990 to the present day
*[[Science Citation Index Expanded]] covers more than 8,500 notable journals encompassing 150 disciplines. Coverage is from the year 1900 to the present day.
*[[Social Sciences Citation Index]] covers more than 3,000 journals in social science disciplines.  Range of coverage is from the year 1900 to the present day.
*[[Arts &amp; Humanities Citation Index]] covers more than  1,700 arts and humanities journals starting from 1975. In addition, 250 major scientific and social sciences journals are also covered. 
*[[Index Chemicus]] lists more than 2.6 million compounds. The time of coverage is from 1993 to present day.
*[[Current Chemical Reactions]] indexes over one million reactions, and the range of coverage is from 1986 to present day. The '' INPI '' archives from 1840 to 1985 are also indexed in this database.
*[[Book Citation Index]] covers more than 60,000 editorially selected books starting from 2005.

=== Contents ===
The seven [[citation index|citation indices]] listed above contain references which have been cited by other articles. One may use them to undertake cited reference search, that is, locating articles that cite an earlier, or current publication. One may search citation databases by topic, by author, by source title, and by location. Two chemistry databases,  ''Index Chemicus'' and  ''Current Chemical Reactions'' allow for the creation of structure drawings, thus enabling users to locate [[chemical compound]]s and reactions. Institutions such as universities and research departments generally access the Web of Science through the [[Web of Knowledge]] platform. (An example of a typical search.&lt;ref&gt;[http://cires.colorado.edu/~jjose/P-Cited/DeCarlo06_ISI.pdf A typical Web of Science search example.]&lt;/ref&gt;)

===Abstracting and indexing===
The following  types of literature are indexed: scholarly books, [[peer review]]ed journals, original research articles, reviews, editorials, chronologies, abstracts, as well as other items. Disciplines included in this index are  [[agriculture]], [[biological sciences]], [[engineering]], medical and [[life sciences]], [[physics|physical]] and [[chemical sciences]], [[anthropology]], law, [[library science]]s, [[architecture]], dance, music, film, and theater. Seven citation databases encompasses coverage of the above disciplines.&lt;ref name=UoOL/&gt;&lt;ref name=included&gt;
{{Cite web |title =Coverage - Web of Science| publisher =Thomson Reuters| year = 2010
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science
  | format =Overview of coverage gleaned from promotional language.  
  | accessdate =2010-06-23}}&lt;/ref&gt;&lt;ref name=&quot;Web of Science&quot; /&gt;

==Limitations in the use of citation analysis==
As with other scientific approaches, scientometrics and bibliometrics have their own limitations. Recently, a criticism was voiced pointing toward certain deficiencies of the journal impact factor (JIF) calculation process, based on Thomson Reuters Web of Science, such as: journal citation distributions usually are highly skewed towards established journals; journal impact factor properties are field-specific and can be easily manipulated by editors, or even by changing the editorial policies; this makes the entire process essentially nontransparent.&lt;ref name=&quot;Declaration&quot;&gt;[http://am.ascb.org/dora/ San Francisco Declaration on Research Assessment: Putting science into the assessment of research, December 16, 2012]&lt;/ref&gt;

Regarding the more objective journal metrics, there is a growing view that for greater accuracy it must be supplemented with an article-based assessment and peer-review.&lt;ref name=&quot;Declaration&quot; /&gt; Thomson Reuters replied to criticism in general terms by stating that &quot;no one metric can fully capture the complex contributions scholars make to their disciplines, and many forms of scholarly achievement should be considered.&quot;&lt;ref&gt;Thomson Reuters Statement Regarding the San Francisco Declaration on Research Assessment [http://researchanalytics.thomsonreuters.com/]&lt;/ref&gt;

== See also ==

{{Div col|3}}
*[[List of academic journal search engines]]
*[[CSA (database company)|CSA databases]]
*[[Dialog (online database)]]
*[[Energy Citations Database]]
*[[Energy Science and Technology Database]]
*[[ETDEWEB]]
*[[Geographic Names Information System]]
*[[Materials Science Citation Index]]
*[[PASCAL (database)|PASCAL database]]
* [[PubMed Central]]
* [[SciELO]]
* [[VINITI Database RAS]]
* [[Web development tools]]
{{Div col end}}

== References ==
{{Reflist}}

==External links==
* [http://scientific.thomson.com/products/wos/ Web of Science]
* [http://www.webofknowledge.com/ Web of Knowledge]
* [http://web.archive.org/web/20110521161422/http://hcl.harvard.edu/research/guides/citationindex/ Searching the Citation Indexes (Web of Science)] Harvard College Library. 2010. (archive)
* [http://video.mit.edu/watch/web-of-science-12339/ MIT Web of Science video tutorial]. 2008.

{{Thomson Reuters}}
{{DEFAULTSORT:Web Of Science}}
[[Category:Bibliographic databases]]
[[Category:Full text scholarly online databases]]
[[Category:Thomson family]]
[[Category:Thomson Reuters]]
[[Category:Citation indices]]
[[Category:Scholarly search services]]</text>
      <sha1>mchnxzo05nslamt8zg0tbthcrqkvulu</sha1>
    </revision>
  </page>
  <page>
    <title>Google Web History</title>
    <ns>0</ns>
    <id>10909304</id>
    <revision>
      <id>645669249</id>
      <parentid>644917785</parentid>
      <timestamp>2015-02-04T22:51:59Z</timestamp>
      <contributor>
        <ip>99.119.76.185</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1759">'''Google Web History''' (previously '''Google Search History''') is a feature of [[Google Search]] and provided by [[Google]], in which all search queries and results that a user clicks on are recorded. The feature is only available for users logged into a [[Google Account]]. The feature was renamed from Search History to Web History on April 19, 2007.&lt;ref&gt;[http://searchengineland.com/google-search-history-expands-becomes-web-history-11016 &quot;Google Search History Expands, Becomes Web History&quot;]. Like all web hostory, google web history take up space and data on your phone, which is why many people choose to clear their hostory.  Search Engine Land. Retrieved July 12, 2010.&lt;/ref&gt; A user's Web History is used to personalize search results with the help of [[Google Personalized Search]]&lt;ref&gt;[http://www.businessweek.com/the_thread/techbeat/archives/2009/12/google_gets_real-time_personalized_search.html &quot;Google Gets Real-Time, Personalized Search&quot;]. ''Business Week''. Retrieved July 12, 2010.&lt;/ref&gt; and in [[Google Now]].

==References== 
Google search engine searches more than just your questions, it matches the words you search with other online posts and files that have the same words, so you get more options and more answers, if you don't find what you're searching for, try rewording your question, often you will discover your question popping up after you type only a few words, and then you can go directly to the answer you were searching for. Google makes it possible and easy for everyone.
{{Reflist}}

==External links==
* [http://history.google.com/history/ Google Web History] Also via redirect at [http://google.com/psearch]

{{Google Inc.}}

{{Google-stub}}

[[Category:Google Search|Web History]]
[[Category:Personalized search]]</text>
      <sha1>b1l80kov3mqpn4ckizdhxjlxgkotzks</sha1>
    </revision>
  </page>
  <page>
    <title>Query by humming</title>
    <ns>0</ns>
    <id>1080984</id>
    <revision>
      <id>638440954</id>
      <parentid>619157439</parentid>
      <timestamp>2014-12-17T02:31:49Z</timestamp>
      <contributor>
        <username>Mojo Hand</username>
        <id>1453997</id>
      </contributor>
      <comment>Remove backlinks to delete article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3815">{{inline|date=August 2012}}
'''Query by humming''' ('''QbH''') is a music retrieval system that branches off the original classification systems of title, artist, composer, and genre. It normally applies to songs or other music with a distinct single theme or melody. The system involves taking a user-hummed [[melody]] (input [[Information retrieval|query]]) and comparing it to an existing [[database]].  The system then returns a ranked list of music closest to the input query. 

One example of this would be a system involving a [[portable media player]] with a built-in [[microphone]] that allows for faster [[Search engine technology|searching]] through [[Digital media|media]] files.

The [[MPEG-7]] standard includes provisions for QbH music searches.

== Examples of QbH systems ==
SoundHound and Midomi are the only commercially available query by humming services available online at Midomi.com or on the mobile app called SoundHound. 
Both are powered by the same backend and are capable of recognizing humming and singing as well as recorded tracks. 
For the singing and humming search, the searchable database is based on Midomi.com's user contributions. Midomi has collected about one million tracks based on user contributions in multiple languages, making it the largest database of its kind by a large margin. The top four languages are: English, Japanese, Chinese and Spanish. 

&quot;Musipedia&quot; is an example of a QbH system that uses a variety of input methods such as humming, tapping the keyboard, keyboard search (a virtual piano keyboard), draw notes, and a contour search, using [[Parsons_code|Parsons Code]] to encode the music pieces.

[[Tunebot]] is a music search engine that uses queries from humming, lyrics, and melody. People can contribute to the database and expand the variety of searchable songs. Tunebot also serves as the back-end for a game called [[Karaoke Callout]], in which players' performances are compared by the engine with songs in the database.

== External links ==
===Online demos===
* [http://www.midomi.com/ Midomi]
* [http://www.soundhound.com/ SoundHound (mobile app)] 
* [http://www.musipedia.org/query_by_humming.0.html QbH system] from Musipedia
* [http://querybyhum.cs.nyu.edu/ QbH research project at NYU]
* [http://www.sloud.com/technology/query_by_humming/ Query by Humming at Sloud Inc], [http://www.sloud.com/ QbH applet (Active X)] 
* [http://www.musicline.de/de/melodiesuche/input Musicline QbH based on technology from Fraunhofer Institut] {{de icon}}
* [http://maart.sourceforge.net/ MaART at Sourceforge]
* [http://tunebot.cs.northwestern.edu/ Tunebot at Northwestern University]

===General info and articles===
* {{Wayback|url=http://mirsystems.info/index.php?id=mirsystems|title=Comprehensive list of Music Information Retrieval systems (apparently last updated ca 2003)|date=20081221191111}}
* [http://www.cs.cornell.edu/zeno/papers/humming/humming.html Query By Humming – Musical Information Retrieval in an Audio Database], paper by Asif Ghias, Jonathan Logan, David Chamberlin, Brian C. Smith; [[ACM Multimedia]] 1995
* [http://cs.nyu.edu/~eugenew/publications/humming-summary.pdf A survey presentation of QBH by Eugene Weinstein, 2006]
* [http://www.dlib.org/dlib/may97/meldex/05witten.html The New Zealand Digital Library MELody inDEX], article by Rodger J. McNab, Lloyd A. Smith, David Bainbridge and Ian H. Witten; [[D-Lib Magazine]] 1997
* [http://deepblue.lib.umich.edu/bitstream/handle/2027.42/35292/10373_ftp.pdf?sequence=1 Name that Tune: A Pilot Study in Finding a Melody from a Sung Query], article by Bryan Pardo, Jonah Shifrin, and William Birmingham, Journal of the American Society for Information Science and Technology, vol. 55 (4), pp. 283-300, 2004

[[Category:Music search engines]]
[[Category:Acoustic fingerprinting]]</text>
      <sha1>jixh6yv6d4eso997w19jb1hknixlj90</sha1>
    </revision>
  </page>
  <page>
    <title>Tunebot</title>
    <ns>0</ns>
    <id>22694659</id>
    <revision>
      <id>481373269</id>
      <parentid>441944738</parentid>
      <timestamp>2012-03-11T19:22:53Z</timestamp>
      <contributor>
        <username>Helpful Pixie Bot</username>
        <id>14216826</id>
      </contributor>
      <minor/>
      <comment>Fixed header External Links =&gt; External links (Build J2)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3912">'''Tunebot''' is a music search engine developed by the Interactive Audio Lab at [[Northwestern University]]. Users can search the database by humming or singing a melody into a microphone, playing the melody on a virtual keyboard, or by typing some of the lyrics. This allows users to finally identify that song that was stuck in their head.

==Searching Techniques==

Tunebot is a [[Query by humming]] system. It compares a sung query to a database of musical themes by using the intervals between each note. This allows a user to sing in a different key than the target recording and still produce a match. The intervals are also unquantized to allow for other tunings besides the standard A=440Hz, since not many people in the world have [[perfect pitch]].

In addition to note intervals, Tunebot compares a query with potential targets by using rhythmic ratios between notes. Since ratios between note lengths are used, the tempo of the performance does not affect the rhythmic similarity measure. 

Queries and targets are then matched by a weighted string alignment algorithm between the note intervals and rhythmic ratios.
&lt;!--Note segmentation, then to pitches and then use Pitch intervals (instead of melodic contour - measured frequency at given times). Pitch intervals are relative (unquantized) to adjust for singing in the wrong key or wrong tempo. Faster and more reliable search.

Model singer error: gaussian distribution because wider interval and lower intervals seem to be more prone to singer error. Combination of gaussians with 5 parameters to tweak: pitch weight, rhythm weight, sensitivity to distance for pitch, sensisitivity to distance for rhythm, octave decay

Do we use rhythmic ratios? (LIR)

Genetic algorithm to tune system parameters--&gt;

==The Database==
The database consists of unaccompanied melodies sung by contributors (a capella). Contributors log into the website and sing their examples to the system. Each of these recordings is associated with a corresponding song on [[Amazon.com|Amazon]]. A sung query is compared to these examples. A capella sung examples are used as search keys because it is much easier to compare one unaccompanied vocal (the sung query) to another (an example search key) than it is to compare an unaccompanied vocal to a full band recording, which may contain guitar, drums, other singers, sound effects, etc.

==Distinguishing Features==

Tunebot learns from user input, and it improve its results as each user submits more queries. Since no human can sing perfectly in tune every time they sing, the search engine must take that into account. By choosing a song from a list of ranked results, users tell Tunebot which song was correct. Tunebot then pairs that song with the user's query, analyzes the differences, and runs a [[Genetic Algorithm]]. This process tweaks the parameters that control how the system compares the user's query to the targets. For instance, if a user has no sense of rhythm, that factor of the comparison is lowered for future queries.

==References==

* B. Pardo. [http://127.0.0.1/publications/pardo-IEEE-signal-processing-mag-06.pdf Finding Structure in Audio for Music Information Retrieval.] IEEE Signal Processing Magazine. vol. 49 (8), pp. 49-52, 2006
* D. Little, D. Raffensperger, B. Pardo. [http://music.cs.northwestern.edu/files/ISMIR%202007%20v2.pdf A Query by Humming System that Learns from Experience.] Proceedings of the 8th International Conference on Music Information Retrieval, Vienna, Austria, September 23-27, 2007.
* D. Little, D. Raffensperger and B. Pardo.[http://www.eecs.northwestern.edu/docs/techreports/2007_TR/NWU-EECS-07-03.pdf Online Training of a Music Search Engine.] Northwestern University, Evanston, IL, NWU-EECS-07-03, 2007

==External links==
*[http://tunebot.cs.northwestern.edu Tunebot @ Northwestern]


[[Category:Music search engines]]
[[Category:Acoustic fingerprinting]]</text>
      <sha1>8kz6vkdd22zhmde3szpck3vkr6j87mg</sha1>
    </revision>
  </page>
  <page>
    <title>Rapid Evolution</title>
    <ns>0</ns>
    <id>16458710</id>
    <revision>
      <id>627043251</id>
      <parentid>604532991</parentid>
      <timestamp>2014-09-25T16:12:47Z</timestamp>
      <contributor>
        <username>Delirium</username>
        <id>6827</id>
      </contributor>
      <minor/>
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4405">{{Infobox Software
| name                   = Rapid Evolution
| screenshot             = &lt;!--  Commented out: [[Image:RapidEvolutionScreenshot1.jpg|thumb|right|250px]] --&gt;
| caption                = Screenshot of Rapid Evolution 2.9.0
| developer              = [[Jesse Bickmore]]
| frequently_updated     = yes
| operating system       = Any OS that supports Java
| genre                  = Music Software
| website                = [http://www.mixshare.com/ Mixshare]
}}
'''Rapid Evolution''' (also known as RE) is an [[open source]] [[software]] tool for [[DJs]], providing filtering and searching features suitable for musicians.  It can analyze audio files and automatically determine properties such as the musical key, [[beats per minute]] (BPM), beat intensity and [[ReplayGain]]. 

It supports file types [[MP3]], [[MP4]], [[WAV]], [[FLAC]], [[Ogg|OGG]], [[Advanced Audio Coding|AAC]] and [[APE tag|APE]].  It helps [[DJs]] to organize and profile their music, and assists in the process of mixing music by utilizing song metadata to be able to show harmonically compatible songs and songs of a similar style.  It allows DJs to save and remember which songs are good matches (like a personal, digital mixing journal) and to plan entire mix sets.

One of its uses is to assist in a [[DJ]] technique called [[harmonic mixing]]. Once the musical key and BPM is known for a set of songs, [[DJs]] can use [[music theory]] (such as the [[Circle of Fifths]]) to identify songs that are harmonically compatible.  The act of mixing harmonically can help eliminate [[consonance and dissonance|dissonant]] tones while mixing songs together.  Since identifying whether songs can be made harmonically compatible can be quite complex (once features such as pitch lock are introduced), the software assists DJs by being able to show them which songs in their collection can be made harmonically compatible with any particular song.  It can also assist DJs in the act of [[beatmatching]] by showing which songs are in a compatible BPM range, and the percent of BPM difference.

Rapid Evolution is created and released through Mixshare.com.  The metadata generated by Rapid Evolution is shared through the central servers at Mixshare.com, which can be browsed online.  There are 1 million songs added to the database sharing information such as key, BPM, styles and ratings.

==History==
Rapid Evolution was developed for the [[Microsoft Windows|Windows]] environment and released in 2003.  Starting in version 2.0 it was switched to run on the Java platform, allowing it to run in virtually any environment.  It is still actively developed.

Several improvements to the key detection algorithm have been introduced over the years.  Rapid Evolution is the only program which can detect advanced key modes, such as aeolian, ionian, dorian, phrygian, lydian and mixolydian.  To date, there has only been one serious comparison of key detection accuracy (including programs such as [[Mixed In Key]] and Mixmeister).  It was shown that Rapid Evolution is the most accurate.&lt;ref&gt;{{cite web|title=Key Detection Software Comparison|url=http://www.mixingonbeat.com/phpbb/viewtopic.php?t=2268|date=2006-04-26|accessdate=2008-03-21|publisher=MixingOnBeat}}&lt;/ref&gt;

The program was open-sourced on November 2013. &lt;ref&gt;{{cite web |url=http://www.mixshare.com/cgi-bin/yabb2/YaBB.pl?num=1381954407|title=Open-sourcing forum thread |date=2013-10-13 |accessdate=2014-04-17 |publisher=Mixshare}}&lt;/ref&gt;

==Community interest==
Rapid Evolution was originally a freeware program.&lt;ref&gt;{{cite web |url=http://www.mixshare.com/wiki/doku.php?id=testimonials|title=DJ Testimonials |date=2007-01-01 |accessdate=2008-03-21 |publisher=Mixshare}}&lt;/ref&gt;Due to its vast feature set, Rapid Evolution tends to be suited more for experienced DJs versus beginners.  

== See also ==
*[[Harmonic mixing]]
*[[Music Theory]]
*[[DJing]]

== External links ==
*[http://www.mixshare.com/software Download Rapid Evolution]
*[http://www.mixshare.com Mixshare's Official website]
*[http://www.harmonic-mixing.com Harmonic-Mixing.com]
*[https://github.com/djqualia/RapidEvolution2 Source code for version 2]
*[https://github.com/djqualia/RapidEvolution3 Source code for version 3]

== References ==
{{reflist}}

[[Category:Music search engines]]
[[Category:OS X multimedia software]]
[[Category:Windows multimedia software]]
[[Category:Audio mixing software]]</text>
      <sha1>86w6b6eq6ohres9sjvp2e6tfeeajwpt</sha1>
    </revision>
  </page>
  <page>
    <title>SensMe</title>
    <ns>0</ns>
    <id>18959809</id>
    <revision>
      <id>645456767</id>
      <parentid>627585709</parentid>
      <timestamp>2015-02-03T14:36:16Z</timestamp>
      <contributor>
        <ip>46.120.2.197</ip>
      </contributor>
      <comment>/* Sony handsets */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5443">{{Refimprove|date=April 2010}}
{{Infobox Software|
|name                   = SensMe
|logo                   = [[File:SensMelogo.png|center|64px|SensMe Logo]]
|screenshot             = [[File:SensMe.jpg|200px|center]]
|caption                = Screenshot of SensMe on Media Go
|developer              = [[Sony Corporation]]
|genre                  = Mood detection software for music data.
|platform               = [[Sony Walkman]] MP3/4 players&lt;br /&gt;[[Sony Ericsson]]&lt;br /&gt;[[Media Go]]&lt;br /&gt;[[PlayStation Portable]]
|license                = [[Proprietary software|Proprietary]]
|website                = [http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w910i?cc=gb&amp;lc=en http://www.sonyericsson.com]
}}

'''SensMe''' is a [[Proprietary software|proprietary]] music mood and tempo detection system created by [[Sony|Sony Corporation]], and employed in numerous Sony branded products, most notably the [[Walkman]] MP3/MP4 players (E &lt;ref&gt;[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6052&amp;NewsAreaId=2 Your cool, colourful music partner Feature-packed WALKMAN® E450 Video MP3 player from Sony with premium sound for young music fans (15 July 2010 )]&lt;/ref&gt; and S series&lt;ref&gt;[http://presscentre.sony.eu/content/detail.aspx?ReleaseID=6203&amp;NewsAreaId=2 Sony introduces super-slim WALKMAN® S750 (15 September 2010)]&lt;/ref&gt;), [[Media Go]], [[PlayStation Portable]], and [[Sony Ericsson]] phone series, .

==Technical specifications==

''SensMe'' works by mapping music to a dual axis map based on the mood and tempo of music tracks.&lt;ref&gt;What is SensMe? http://www.sonyericsson.com/cws/support/phones/detailed/whatissenseme/w980&lt;/ref&gt; Mood and tempo is determined by using the appropriate Sony compatible software which analyzes music tracks individually and computes the relevant track information. Analyzed tracks can then be plotted onto an intuitive dual axis map through which the music library on the device can be navigated, and playlists can be generated based on relative speed and mood. The horizontal axis is based on mood and the vertical axis is based on [[tempo]].

==PlayStation Portable==

SensMe was made available on the PlayStation Portable as of system software version 6.10.&lt;ref name=&quot;update610&quot; /&gt; It can be downloaded via the [[XrossMediaBar|XMB]] or by using a computer.&lt;ref name=&quot;pspdownload&quot;&gt;SensMe PSP Download http://www.playstation.com/psp-app/sensme/en/&lt;/ref&gt; The application features twelve channels by which music is categorized. These include Favorites, Newly Added, Dance, Extreme, Lounge, Emotional, Mellow, Upbeat, Relax, Energetic, Morning/Day/Night/Midnight, and Shuffle All.

===PlayStation Portable Version History===
{| class=&quot;wikitable&quot;
!width=&quot;180&quot;|Version&lt;br&gt; Release date (UTC)
!class=&quot;unsortable&quot;|Description
|-
|align=center|'''1.50'''&lt;br&gt;March 31, 2010
|
* Music tracks transferred using a PlayStation 3 system or music management application other than Media Go are now also categorized into channels.
* Users can now add music tracks to a block list so they do not play.
* Users can now activate or deactivate the [Dynamic Normalizer] feature.
|-
|align=center|'''1.01'''&lt;br&gt;October 22, 2009
|
* Descriptions of some menu items in some languages have been revised.
|-
|align=center|'''1.00'''&lt;br&gt;October 1, 2009
|
* Initial release.
|}

==SensMe compatible products==
* [[Walkman]]
* [[Media Go]]
* [[PlayStation Portable]]&lt;ref name=&quot;update610&quot;&gt;PSP Firmware Update (v6.10) http://blog.us.playstation.com/2009/09/psp-firmware-update-v6-10/&lt;/ref&gt;

[[File:Sony Ericsson W760i running SensMe.JPG|thumb|right|Screenshot of SensMe on a Sony Ericsson]]

===Sony Ericsson handsets===
*''[[Sony Ericsson Aino|Aino]]''
*''[http://www.sony.co.uk/product/nws-s-series/nwz-s639f Sony NWZ-S639F Media Player]''
*''[[Sony Ericsson Elm|elm]]''
*''[[Sony Ericsson W380|W380]]''
*''[[Sony Ericsson W518a|W508]]''
*''[[Sony Ericsson W518a|W518a]]''
*''[[Sony Ericsson W595|W595]]''
*''[[Sony Ericsson W705|W705]]''
*''[[Sony Ericsson W705|W715]]''
*''[[Sony Ericsson W760|W760]]''
*''[[Sony Ericsson W890i|W890i]]''
*''[[Sony Ericsson W902|W902]]''
*''[[Sony Ericsson W910|W910i]]''
*''[[Sony Ericsson W980|W980]]''
*''[[Sony Ericsson W995|W995]]''
*''[[Sony Ericsson Xperia X10|Xperia X10]]''
*''[[Sony Ericsson Xperia Neo|Xperia Neo]]''
*''[[Sony Ericsson Xperia Play|Xperia Play]]''
*''[[Sony Ericsson Xperia ray|Xperia Ray]]''
===Sony handsets===
*''[[Sony Xperia E|Xperia E]]''
*''[[Sony Xperia M|Xperia M]]''
*''[[Sony Xperia Sola|Xperia Sola]]''
* [[Sony_Xperia_L|Xperia L]]
*''[[Sony Xperia S|Xperia S]]''
*''[[Sony Xperia P|Xperia P]]''
*''[[Sony Xperia U|Xperia U]]''
*''[[Sony Xperia T|Xperia T]]''
*''[[Sony Xperia TX|Xperia TX]]''
*''[[Sony Xperia TL|Xperia TL]]''
*''[[Sony Xperia tipo|Xperia tipo]]''
*''[[Sony Xperia T|Xperia Go]]''
*''[[Sony Xperia V|Xperia V]]''
*''[[Sony Xperia Z|Xperia Z]]''
*''[[Sony Xperia Z1|Xperia Z1]]''
*''[[Sony Xperia Z1 Compact|Xperia Z1 Compact]]''
*''[[Sony Xperia Z Ultra|Xperia Z Ultra]]''
*''[[Sony Xperia Z1f|Xperia Z1f/Z1s]]''
*''[[Sony Xperia ZL|Xperia ZL]]''
*''[[Sony Xperia ZL|Xperia SP]]''
*''[[Sony Xperia Z2|Xperia Z2]]''
*''[[Sony Xperia Z3|Xperia Z3]]''
*''[[Sony Xperia Z3 Compact|Xperia Z3 Compact]]''
*''[http://www.sonyericsson.com/cws/products/mobilephones/overview/zylo?cc=ph&amp;lc=en#view=features_specifications Zylo (W20i)]''

==References==
{{reflist}}

[[Category:Sony software]]
[[Category:Music search engines]]</text>
      <sha1>qn7jsr5xocg52fcv69dsivm2w3gvtw6</sha1>
    </revision>
  </page>
  <page>
    <title>Shazam (service)</title>
    <ns>0</ns>
    <id>16810425</id>
    <revision>
      <id>647486450</id>
      <parentid>647485565</parentid>
      <timestamp>2015-02-17T01:48:39Z</timestamp>
      <contributor>
        <username>MeanMotherJr</username>
        <id>15995683</id>
      </contributor>
      <minor/>
      <comment>/* Desktop App */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26370">{{EngvarB|date=February 2014}}
{{Use dmy dates|date=February 2014}}
{{Infobox company
| name             = Shazam Entertainment Ltd.
| logo             = [[File:Shazam logo.png|160px]]
| caption          = 
| type             = 
| traded_as        = 
| genre            = &lt;!-- Only used with media and publishing companies --&gt;
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = United States ({{Start date|1999}})
| founder          = {{unbulleted list|Chris Barton|Philip Inghelbrecht|Dhiraj Mukherjee|Avery Wang}}
| defunct          = &lt;!-- {{End date|df=yes|YYYY|MM|DD}} --&gt;
| location_city    = London
| location_country = United Kingdom
| location         = 
| locations        = 7 offices (2014)
| area_served      = Worldwide
| key_people       = {{unbulleted list|Rich Riley (CEO)|Andrew Fisher (Executive chairman)}}
| industry         = 
| products         = [[Application software|Apps]]
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| aum              = &lt;!-- Only used with financial services companies --&gt;
| assets           = 
| equity           = 
| num_employees    = 
| divisions        = 
| subsid           = 
| homepage         = {{URL|//www.shazam.com/}} 
| footnotes        = 
| intl             = 
}}
'''Shazam''' is a British app for smartphones, PCs&lt;ref&gt;{{Cite web|url = http://apps.microsoft.com/windows/en-au/app/shazam/5593d150-02c7-4714-ab8f-007d5d251688|title = Shazam|date = |accessdate = 7 January 2015|website = Shazam app for Windows in the Windows Store|publisher = Microsoft Corporation|last = |first = }}&lt;/ref&gt; and Macs, which is best known for its music identification capabilities. Shazam Entertainment Limited was founded in 1999 by Chris Barton, Philip Inghelbrecht, Avery Wang and Dhiraj Mukherjee.&lt;ref name=&quot;DirectorDec2009&quot;&gt;{{cite news | url=http://www.director.co.uk/magazine/2009/11%20December/shazam_63_04.html | title=Shazam names that tune | date=December 2009 | accessdate=26 September 2012 | last=Woodward | first=David | newspaper=Director}}&lt;/ref&gt; The company is best known for its music identification technology, but has expanded to integrations with cinema, advertising, TV and retail environments.&lt;ref&gt;http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform&lt;/ref&gt;

Shazam uses a smartphone or Mac's built-in microphone to gather a brief sample of audio being played.  It creates an [[acoustic fingerprint]] based on the sample, and compares it against a central database for a match.  If it finds a match, it sends information such as the artist, song title, and album back to the user. Some implementations of Shazam incorporate relevant links to services such as [[iTunes]], [[YouTube]], [[Spotify]] or [[Zune]]. In December of 2013, Shazam was one of the top ten apps in the world, according to its CEO.&lt;ref&gt;http://video.cnbc.com/gallery/?video=3000222563#.&lt;/ref&gt; The Shazam app has more than 100 million monthly active users and has been used on more than 500 million mobile devices.&lt;ref&gt;http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/&lt;/ref&gt; In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.&lt;ref&gt;http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been&lt;/ref&gt;
==Features==
Shazam offers two types of applications; a free app simply called Shazam and a paid app called Shazam Encore. The service was expanded in September 2012 to enable TV users in the US to identify featured music, access cast information and get links to show information online, as well as adding social networking capabilities.&lt;ref name=&quot;TV tags&quot;/&gt;

In February of 2014, Shazam announced a redesign of the app, which included a new look and additional features, including lyric-viewing options, access to music videos and related videos, unique recommendations, improved biographies and discographies and additional functionality for use with TV shows. The update also featured a News Feed, and Auto-Shazam, a feature introduced in December of 2013, which runs in the background of users’ mobile devices to automatically identify media.&lt;ref&gt;http://www.digitaltveurope.net/151662/shazam-unveils-app-redesign/&lt;/ref&gt;  

In July of 2014, Shazam announced the launch of Shazam for Mac, a desktop version of the app, which when enabled, runs in the background and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.&lt;ref&gt;http://mashable.com/2014/07/31/shazam-mac-app/&lt;/ref&gt; Apple’s launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apple’s intelligent personal assistant Siri function.&lt;ref&gt;http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html&lt;/ref&gt; 

==Devices==
Shazam is a free or low-cost application that runs on [[Android (operating system)|Android]], [[Apple Inc.|Apple]] [[iPhone]] iOS, [[BlackBerry]] OS, and [[Windows Phone|Windows]] systems. The application is similar on most phones and the result is shown on the screen complete with details on Artist, Album, Title, Genre, Music label, lyrics, a thumbnail image of the song/album artwork, links to download the song on [[iTunes]] or the [[Amazon MP3]] store and, where relevant, show the song's video on YouTube and give the option of playing the song on [[Rdio]]. Shazam is also available for Mac, as a desktop application.&lt;ref&gt;http://mashable.com/2014/07/31/shazam-mac-app/ &lt;/ref&gt; 

== Function ==
Shazam works by analyzing the captured sound and seeking a match based on an [[acoustic fingerprint]] in a database of more than 11 million songs.&lt;ref&gt;[//www.shazam.com/music/web/about.html Shazam – About Shazam&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

[[File:Spectrogram of violin.png|thumb|A spectrogram of the sound of a violin.]]
[[File:Target zone2.png|thumb|The target zone of a song scanned by Shazam.{{clarify|date=September 2012}}]]
Shazam identifies songs based on an audio fingerprint based on a time-frequency graph called a [[spectrogram]].

Shazam stores a catalogue of audio fingerprints in a database. The user tags a song for 10 seconds and the application creates an audio fingerprint.

Once it creates the fingerprint of the audio, Shazam starts the search for matches in the database. If there is a match, it returns the information to the user; otherwise it returns a &quot;song not known&quot; dialogue.&lt;ref&gt;[http://soyoucode.com/2011/how-does-shazam-recognize-song How does Shazam work to recognize a song ? | So, you code ?&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

Shazam can identify prerecorded music being broadcast from any source, such as a radio, television, cinema or music in a club, provided that the background noise level is not high enough to prevent an acoustic fingerprint being taken, and that the song is present in the software's database.

==History==
The company was founded in 1999 by Barton and Inghelbrecht, who were students at [[University of California, Berkeley]], and Mukherjee, who worked at a London-based internet consulting firm called Viant.{{Citation needed|date=September 2013}} In need of a digital signal processing specialist, the founding team then hired Wang, who was a PhD student from [[Stanford University]]. {{as of|September 2012}}, Wang is the only member of the original team to remain in the company,&lt;ref name=&quot;DirectorDec2009&quot; /&gt; and serves as Shazam's Chief Scientist.&lt;ref name=&quot;Shazam Team&quot;&gt;{{cite web|title=About Shazam – Team|url=//www.shazam.com/music/web/team.html|accessdate=27 September 2012}}&lt;/ref&gt;

[[Rich Riley]] joined Shazam as CEO in April 2013 to increase the company’s growth,&lt;ref&gt;http://www.huffingtonpost.co.uk/2013/08/30/shazam-rich-riley_n_3762179.html&lt;/ref&gt; after over 13 years at Yahoo!&lt;ref&gt;http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo &lt;/ref&gt; and with more than 17 years of experience as an entrepreneur and leading Internet executive.&lt;ref&gt;http://www.crunchbase.com/person/rich-riley &lt;/ref&gt; &quot;I look forward to extending our dominance in media engagement, from our roots in music to our leadership position in second-screen TV and want to ensure that Shazam is the company that helps people recognize and engage with the world around them,” Riley said in a statement at the time.&lt;ref&gt;http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo &lt;/ref&gt; Riley replaced Andrew Fisher, who was hired from [[Infospace]] into the CEO role in 2005 to strengthen industry partnerships and grow the userbase.&lt;ref name=DirectorDec2009 /&gt; Fisher is now executive chairman.

===Partnerships===
The first partnership was with Entertainment UK, part of Woolworths, whom they approached to digitise their music catalogue of 1.5 million songs in return for permission to create a proprietary database. As the service grew to have a worldwide userbase, it needed to keep its database up-to-date, which it does by having relationships with labels globally.&lt;ref name=&quot;DirectorDec2009&quot; /&gt; By December 2008, the database had grown to 8 million songs.&lt;ref&gt;{{cite news|last=Reisinger|first=Don|title=Shazam adds 2 million tracks to music library|url=http://news.cnet.com/8301-17939_109-10113274-2.html|accessdate=29 September 2012|newspaper=CNET|date=4 December 2008}}&lt;/ref&gt;

In February 2013, Shazam announced a partnership with the music store [[Beatport]], adding its library of [[electronic music]] to the service.&lt;ref name=bb-shazambeatport&gt;{{cite web|title=Beatport's Matthew Adell on Shazam Deal, Why Music Biz Is a 'Disaster Model'|url=http://www.billboard.com/biz/articles/news/digital-and-mobile/1538517/beatports-matthew-adell-on-shazam-deal-why-music-biz-is|work=Billboard.biz|accessdate=21 September 2013}}&lt;/ref&gt; On 3 April 2013, Shazam announced an exclusive partnership with [[Saavn]], an Indian online music streaming service. The deal will add nearly 1 million songs in [[Languages of India|Indian languages]] to Shazam's database.&lt;ref&gt;[http://www.financialmirror.com/newsml_story.php?id=5458 Shazam Forms Exclusive New Partnership with Saavn for the Best Indian Music Discovery Experience&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;&lt;ref&gt;{{cite news| url=http://blogs.wsj.com/speakeasy/2013/04/03/shazam-broadens-its-horizons/ | work=The Wall Street Journal | title=Shazam Broadens Its Horizons – Speakeasy – WSJ}}&lt;/ref&gt;&lt;ref&gt;[http://techcrunch.com/2013/04/03/shazam-partners-with-the-spotify-of-india-saavn-to-improve-its-south-asian-music-recognition/ Shazam Partners With The ‘Spotify Of India’, Saavn, To Improve Its South Asian Music Recognition | TechCrunch&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;&lt;ref&gt;[http://www.medianama.com/2013/04/223-shazam-saavn-tieup/ Updated: Shazam Ties Up With Saavn To Identify Hindi &amp; Regional Music; Implications – MediaNama&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; In July 2014, Shazam announced a partnership with Rdio that allows Shazam users to stream full songs within the app.&lt;ref&gt;http://www.billboard.com/biz/articles/news/digital-and-mobile/6157583/shazam-partners-with-rdio-to-stream-full-songs-inside &lt;/ref&gt;

In addition to music, Shazam has announced collaborations with partners across television, advertising and cinema. In May of 2014, NCM Media Networks announced a partnership with Shazam to incorporate Shazam into FirstLook pre-show segments that run in Regal, AMC and Cinemark theaters.&lt;ref&gt;http://techcrunch.com/2014/05/14/shazam-partners-with-ncm/ &lt;/ref&gt; In November of 2014, NCM and Shazam announced that NCM FirstLook pre-shows are now Shazam enabled on over 20,000 movie screens across the United States.&lt;ref&gt;http://mashable.com/2014/11/07/shazam-firstlook/ &lt;/ref&gt;

In August of 2014, Shazam announced the launch of Resonate, a sales product that allows TV networks to access its technology and user base. The news included the announcement of partnerships with AMC, A+E, dick clark productions and FUSE.&lt;ref&gt;http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform &lt;/ref&gt;

Shazam recently announced a partnership with Sun Broadcast Group on Shazam for Radio, a new offering that will allow radio stations to push customized content to listeners on Sun Broadcast’s over 8,000 radio stations in the U.S.&lt;ref&gt;http://thenextweb.com/insider/2014/10/09/shazam-makes-big-move-interactive-radio-content/ &lt;/ref&gt;

===Early days of the service===
Initially, in 2002, the service was launched only in the UK and was known as &quot;2580&quot;, as the number was the [[shortcode]] that customers dialled from their mobile phone to get music recognised.&lt;ref name=DirectorDec2009 /&gt; The phone would automatically hang up after 30 seconds. A result was then sent to the user in the form of a text message containing the song title and artist name. At a later date, the service also began to add hyperlinks in the text message to allow the user to download the song online.&lt;ref name=CNETUKApril06&gt;{{cite news|last=Lim|first=Andrew|title=Shazam &amp; AQA: The answer is on your mobile|url=http://crave.cnet.co.uk/mobiles/shazam-and-aqa-the-answer-is-on-your-mobile-49264359/|accessdate=29 September 2012|newspaper=CNET UK|date=24 April 2006}}&lt;/ref&gt;

Shazam launched in the US on the AT&amp;T Wireless network in 2004 in a joint offering with Musicphone, a now defunct San Francisco-based company. The service was free at launch with AT&amp;T saying that it would charge USD0.99 for each use in future.&lt;ref name=&quot;cnet040415&quot;&gt;{{cite news | url=http://news.cnet.com/Dial-that-tune-comes-to-U.S./2110-1039_3-5192105.html | title=Dial-that-tune comes to U.S. | work=CNET | date=15 April 2004 | accessdate=29 September 2012 | author=Charny, Ben}}&lt;/ref&gt;

In 2006, users were charged £0.60 per call or had unlimited use for £4.50 a month, as well as an online service to keep track of all tags.&lt;ref name=CNETUKApril06/&gt;

===Smartphone app===
Shazam for iPhone 2.0 debuted on 10 July 2008, with the launch of Apple's App Store. The free app simplified the service by enabling the user to launch iTunes and buy the song directly if the user was on a Wi-Fi connection &lt;ref name=CNET080710&gt;{{cite news|last=Rosoff|first=Matt|title=Shazam on iPhone could change music discovery|url=http://news.cnet.com/8301-13526_3-9988219-27.html|accessdate=29 September 2012|newspaper=CNET|date=10 July 2008}}&lt;/ref&gt; (at the time, iTunes did not allow music downloads over 3G). It was also possible to launch the iPhone YouTube app, if a video was available.&lt;ref name=CNET080716&gt;{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=29 September 2012|newspaper=CNET|date=16 July 2008}}&lt;/ref&gt;

In 2008, the service struggled to identify classical music.&lt;ref&gt;{{cite news|last=Ho|first=Kevin|title=iPhone apps: Testing Shazam's limits – classical music|url=http://news.cnet.com/8301-13544_3-9993320-35.html|accessdate=29 September 2012|newspaper=CNET|date=17 July 2008}}&lt;/ref&gt;

Shazam launched on the [[Android operating system|Android platform]] in October 2008. The Android app connected to [[Amazon Appstore|Amazon's MP3 store]] instead of iTunes.&lt;ref name=AndroidLaunch&gt;{{cite news|last=Reisinger|first=Don|title=Shazam moves to Android, works with Amazon MP3 Store|url=http://news.cnet.com/8301-17939_109-10071167-2.html|accessdate=29 September 2012|newspaper=CNET|date=21 October 2008}}&lt;/ref&gt;

Alongside the iOS 3 update in July 2009, Shazam updated its app to include a number of new features: marking the tag with GPS coordinates; sending tags to others as 'postcards', enabling them to buy the song; and Twitter integration.&lt;ref&gt;{{cite news|last=Lee|first=Nicola|title=Latest Shazam lets you track musical journey in iPhone OS 3.0|url=http://download.cnet.com/8301-2007_4-10267205-12.html|accessdate=29 September 2012|newspaper=CNET|date=17 June 2009}}&lt;/ref&gt;

The app launched on the [[Windows Marketplace for Mobile|Windows Mobile Marketplace]] in October 2009 as a [[freemium]] offering, with the first release of Shazam Encore. The free version was now limited to five tags per month: users typically tagged ten songs per month. Encore, priced at USD4.69, added several features such as song popularity charts and recommendations.&lt;ref name=CNETWindowsLaunch&gt;{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=30 September 2012|newspaper=CNET|date=7 October 2009}}&lt;/ref&gt; Encore first appeared for iPhone in November 2009.&lt;ref&gt;{{cite news|last=Dolcourt|first=Jessica|title=Shazam iPhone app gets premium Encore|url=http://download.cnet.com/8301-2007_4-10393035-12.html|accessdate=30 September 2012|newspaper=CNET|date=9 November 2009}}&lt;/ref&gt;

By December 2009, Shazam was downloaded 10 million times in 150 countries across 350 mobile operators. Around eight percent of users purchased a track after it was identified by the service.&lt;ref name=DirectorDec2009 /&gt; Its success led to a funding round from [[Kleiner Perkins Caufield &amp; Byers]] in October 2009.&lt;ref name=DirectorDec2009 /&gt;&lt;ref&gt;{{cite news|last=Saint|first=Nick|title=Shazam Draws Investment, Is Already Profitable|url=http://www.businessinsider.com/shazam-draws-investment-is-already-profitable-2009-10|accessdate=30 September 2012|newspaper=Business Insider|date=15 October 2009}}&lt;/ref&gt; In January 2011, Apple announced that Shazam was the fourth most downloaded free app of all time on the App Store, while rival [[SoundHound]] had the top paid iPad app.&lt;ref&gt;{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=30 September 2012|newspaper=CNET|date=19 January 2011}}&lt;/ref&gt;

Early adopters of the free application are still allowed unlimited tagging.&lt;ref&gt;[http://androidforums.com/android-applications/132182-shazam-how-preserve-unlimited-tagging-feature-after-reflash-root.html#post1488306 Shazam: How to preserve the &quot;unlimited tagging&quot; feature after REFLASH and Root? – Android Forums&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

[[GetJar]], an app store for Android, Blackberry and Symbian, added Shazam in November 2010.&lt;ref&gt;{{cite news|last=Reisinger|first=Don|title=AT&amp;T ladles out GetJar apps – iPhone excluded|url=http://news.cnet.com/8301-13506_3-20022340-17.html|accessdate=30 September 2012|newspaper=CNET|date=10 November 2010}}&lt;/ref&gt;

In January 2011, Shazam and [[Spotify]] announced a partnership for iOS and Android to help users identify music with Shazam and listen to tracks through Spotify.&lt;ref&gt;{{cite news|last=Morris|first=Natali|title=Space love|url=http://cnettv.cnet.com/8301-13991_53-20028388-10391624.html|accessdate=30 September 2012|newspaper=CNET|date=13 January 2011}}&lt;/ref&gt;

While Shazam already had Facebook and Twitter share buttons, deeper Facebook integration was released in March 2011. With Shazam Friends users can see what their Facebook friends have tagged, listen to the tracks and buy them.&lt;ref&gt;{{cite news|last=McCarthy|first=Caroline|title=Music app Shazam gets new Facebook features|url=http://news.cnet.com/8301-13577_3-20045965-36.html|accessdate=1 October 2012|newspaper=CNET|date=22 March 2011}}&lt;/ref&gt;

With Shazam 5.0, released in April 2012, the app begins 'listening' as soon as it is launched and can take as little as one second to identify media. In addition to music, the app can identify TV programs and ads, if they are Shazam-enabled.&lt;ref&gt;{{cite news|last=Parker|first=Jason|title=Shazam for iOS adds TV to its list of media it can identify|url=http://reviews.cnet.com/8301-19512_7-57408964-233/shazam-for-ios-adds-tv-to-its-list-of-media-it-can-identify/|accessdate=1 October 2012|newspaper=CNET|date=3 April 2012}}&lt;/ref&gt;

In August 2012, Shazam announced the service had been used to tag five billion songs, TV shows and advertisements. In addition, Shazam claimed to have over 225 million users across 200 countries.&lt;ref&gt;{{cite news|last=Sawers|first=Paul|title=Shazam: Five billion songs, TV shows and ads tagged|url=http://thenextweb.com/insider/2012/08/07/shazam-five-billion-songs-tv-shows-and-ads-tagged/|accessdate=30 September 2012|newspaper=The Next Web|date=7 August 2012}}&lt;/ref&gt; A month later, the service claimed to have more than 250 million users with 2 million active users per week.&lt;ref name=&quot;TV tags&quot;&gt;{{cite news|last=Kinder|first=Lucy|title=Shazam hits 250 million users and adds TV tagging capability|url=http://www.telegraph.co.uk/technology/news/9547632/Shazam-hits-250-million-users-and-adds-TV-tagging-capability.html|accessdate=17 September 2012|newspaper=The Telegraph|date=17 September 2012|location=London}}&lt;/ref&gt; The Shazam app currently has more than 100 million monthly active users and has been used on more than 500 million mobile devices.&lt;ref&gt;http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/ &lt;/ref&gt; In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.&lt;ref&gt;http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been &lt;/ref&gt;

The Shazam app was listed among Techland's 50 Best Android Applications for 2013.&lt;ref&gt;{{cite news |url=http://techland.time.com/2013/07/01/50-best-android-apps-for-2013/slide/pulse-news/ | title=50 Best Android Apps for 2013 | author=Jared Newman | work=Techland | accessdate=30 June 2013 | date=1 July 2013}}&lt;/ref&gt;

In August 2014, Shazam announced there would be no more updates for Shazam(RED) after August 7.&lt;ref&gt;[https://support.shazam.com/hc/en-us/articles/202604996-Important-News-About-SHAZAM-RED Important News About Shazam(RED)] — Shazam Support&lt;/ref&gt; Current users are advised to switch to the free version with tags transferred and ads removed (for free).

Apple’s launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apple’s intelligent personal assistant Siri function.&lt;ref&gt;http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html &lt;/ref&gt;

In October of 2014, Shazam introduced version 8.0 of the app, which features a new and improved News feed, as well as a section featuring Shazam charts and an “explore” option which lets user explore Shazamed tracks near them and around the world.&lt;ref&gt;http://appadvice.com/appnn/2014/10/shazam-8-0-features-interactive-notifications-in-ios-8-revamped-news-feed-and-more &lt;/ref&gt;

===Desktop app=== 
Shazam announced the launch of Shazam for Mac, a desktop application, in July of 2014. When enabled, the app runs in the background of a Mac and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.&lt;ref&gt;http://mashable.com/2014/07/31/shazam-mac-app/ &lt;/ref&gt;

==Similar apps==

*[[SoundHound]], previously known as Midomi, uses [[Query by humming]] to identify songs.{{citation needed|date=October 2012}}
*[[Gracenote]]'s MusicID-Stream has the main advantage of having the largest database of all music IDs (with more than 28 million songs).{{citation needed|date=October 2012}}
*Musipedia is a music search engine that works differently from others because instead of using techniques to identify recorded music, it can identify pieces of music from a single melody or rhythm.{{citation needed|date=October 2012}}
*Play by Yahoo Music.
*Bing music identification.
*Sony TrackID
*Path also has a music-identification feature.&lt;ref&gt;{{cite news|last=Cabebe|first=Jaymar|title=Path: The smaller, simpler alternative to Facebook|url=http://news.cnet.com/8301-1035_3-57416066-94/path-the-smaller-simpler-alternative-to-facebook/|accessdate=1 October 2012|newspaper=CNET|date=18 April 2012}}&lt;/ref&gt;
*Stream That Song by Orange Innovation UK Ltd

==Patent infringement lawsuit==
In May 2009, Tune Hunter accused Shazam of violating {{US Patent|6941275}}, which covers music identification and purchase in a portable device.&lt;ref&gt;{{cite news|last=Ogg|first=Erica|title=Apple, AT&amp;T, Samsung, Verizon, and others sued over Shazam app|url=http://news.cnet.com/8301-13579_3-10241309-37.html|accessdate=29 September 2012|newspaper=CNET|date=14 May 2009}}&lt;/ref&gt; Shazam settled the case in January 2010.&lt;ref&gt;{{cite news
|title=Shazam Settles Patent Infringement Case With Tune Hunter
|url=http://techcrunch.com/2010/01/06/shazam-tune-hunter-settlement/
|date=Jan 6, 2010
|first=Robin
|last=Wauters
}}&lt;/ref&gt;

==Funding==

As of September 2012, Shazam had raised $32 million in funding.&lt;ref name=&quot;Techcrunch&quot;&gt;{{cite news|last=Kincaid|first=Jason|title=Shazam Raises A Huge Round to the Tune of $32 Million|url=http://techcrunch.com/2011/06/22/shazam-raises-a-huge-round-to-the-tune-of-32-million/|accessdate=20 September 2012|newspaper=TechCrunch|date=22 June 2011}}&lt;/ref&gt; In July 2013, [[Carlos Slim]] invested $40 million in Shazam for an undisclosed share.&lt;ref&gt;[http://www.ft.com/intl/cms/s/0/97d2c46a-e58d-11e2-8d0b-00144feabdc0.html#axzz2ag6DhVhD Carlos Slim invests $40m in music app Shazam – FT.com&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; And in March of 2014, Shazam confirmed another $20 million in new funding, raising the total value of the company to half a billion dollars.&lt;ref&gt;http://www.billboard.com/biz/articles/5930359/shazam-confirms-20m-in-new-funding-raising-value-to-500m &lt;/ref&gt;

==See also==
* [[Query by humming]]
* [[Acoustic fingerprint]]
* [[Spectrogram]]
* [[Sound recording copyright symbol]]

==References==
{{Reflist|30em}}

==Further reading==
* {{cite news |last=Dredge |first=Stuart |title=Shazam: 'TV advertising is going to become our primary revenue stream' |url=http://www.guardian.co.uk/media/appsblog/2013/feb/27/shazam-tv-advertising-future |accessdate=27 February 2013 |newspaper=[[The Guardian]] |date=27 February 2013|location=London}}

==External links==
* {{Official website|www.shazam.com/music/web/home.html}}

[[Category:Companies based in London]]
[[Category:Acoustic fingerprinting]]
[[Category:Android (operating system) software]]
[[Category:BlackBerry software]]
[[Category:IOS software]]
[[Category:Symbian software]]
[[Category:Music search engines]]
[[Category:Companies established in 1999]]
[[Category:Windows Phone software]]</text>
      <sha1>j2xjd5yodzsu7zy8v5he2pysi8cojir</sha1>
    </revision>
  </page>
  <page>
    <title>SoundHound</title>
    <ns>0</ns>
    <id>17540116</id>
    <revision>
      <id>645332482</id>
      <parentid>644152065</parentid>
      <timestamp>2015-02-02T17:52:17Z</timestamp>
      <contributor>
        <username>Ekkt0r</username>
        <id>19479182</id>
      </contributor>
      <minor/>
      <comment>Converted each &quot;http://www.soundhound.com/&quot; link into a [[protocol-relative URL]], as https works and matches http for www.soundhound.com). See [[Wikipedia:Village_pump_(policy)/Archive_111#As WP uses HTTPS, should (some) external links, too?|VPP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12731">{{distinguish|SoundCloud}}
{{Infobox software
| name                   = Soundcloud
| title                  = 
| logo                   = [[File:SoundHound Mobile Icon.png|250px]]
| logo caption           = SoundHound Mobile Icon
| screenshot             = &lt;!-- [[File: ]] --&gt;
| caption                = 
| collapsible            = 
| author                 = 
| developer              = SoundHound, Inc
| released               = {{Start date|2009|01|29|df=yes}}
| discontinued           = 
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = 
| operating system       = 
| platform               = 
| size                   = 
| language               = 
| language count         = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| language footnote      = 
| status                 = 
| genre                  = 
| license                = 
| alexa                  = 
| website                = {{URL|//www.soundhound.com/}}
}}
'''SoundHound''' (known as '''Midomi''' until December 2009) is a [[mobile device]] service that allows users to identify music by [[Query by humming|humming]], singing or playing a recorded track. The service was launched by Melodis Corporation (now SoundHound Inc), under [[Chief Executive]] Keyvan Mohajer in 2007 and has received funding from Global Catalyst Partners, TransLink Capital and Walden Venture Capital.

==Features==
SoundHound is a music search engine available on the [[Apple App Store]],&lt;ref name=&quot;Apple App Store&quot; /&gt; [[Google Play]],&lt;ref name=&quot;Google Play&quot; /&gt; [[Windows Phone Store]], and on June 5, 2013, was available on the BlackBerry 10 platform.&lt;ref name=&quot;Windows&quot; /&gt; It enables users to identify music recorded through their device's microphone.&lt;ref name=CNETv3 /&gt; It is also possible to speak or type the name of the artist, composer, song and piece.&lt;ref name=CNETv3 /&gt; Unlike competitor [[Shazam (service)|Shazam]], SoundHound can recognise tracks from singing, humming, speaking, or typing, as well as from a recording.&lt;ref&gt;{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=2 October 2012|newspaper=CNET|date=16 July 2008}}&lt;/ref&gt; Sound matching is achieved through the company's 'Sound2Sound' technology, which  can match even poorly-hummed performances to professional recordings.&lt;ref name=&quot;CNETWVC&quot; /&gt;

The app then returns the lyrics (if any), links to videos on YouTube, links to iTunes, ringtones, the ability to launch [[Pandora Radio]],&lt;ref name=&quot;TNWHound&quot;&gt;{{cite news | url=http://thenextweb.com/apps/2011/05/26/soundhounds-new-voice-app-hound-wants-to-change-the-way-we-search/ | title=SoundHound’s new voice app &quot;Hound&quot; wants to change the way we search | date=26 May 2011 | accessdate=2 October 2012 | last=Boyd Myers | first=Courtney | newspaper=The Next Web}}&lt;/ref&gt; as well as recommendations for other music.&lt;ref&gt;{{cite news|last=Dolcourt|first=Jessica|title=SoundHound for iPhone channels iTunes, recommends beats|url=http://reviews.cnet.com/8301-19512_7-20037798-233.html|accessdate=2 October 2012|newspaper=CNET|date=2 March 2011}}&lt;/ref&gt; A feature called LiveLyrics displays a song's lyrics in time with the music, if they are available. Double-tapping on those lyrics moves the music to that point in the song.&lt;ref&gt;{{cite news|last=Cabebe|first=Jaymar|title=SoundHound adds LiveLyrics|url=http://download.cnet.com/8301-2007_4-20081243-12/soundhound-adds-livelyrics/|accessdate=3 October 2012|newspaper=CNET|date=20 July 2011}}&lt;/ref&gt; It is also possible for users to play music from their iPhone's iPod library through the app. If lyrics are available for a song, it will show them as it plays.&lt;ref name=&quot;CNETv3&quot; /&gt;

There are three versions of the app: SoundHound, SoundHound Infinity and Hound. SoundHound is free but has banner ads, while SoundHound Infinity (styled SoundHound ∞), priced at £4.99 in the UK or $6.99 in the US, is the premium offering and has the same functionality but without banner ads.&lt;ref name=&quot;CNETFree&quot; /&gt; Hound only allows users to search for artists or songs by speaking into it. Similar to the SoundHound app, Hound then returns a song preview, lyrics, album art and videos as well as artist bios and tour dates.&lt;ref name=&quot;TNWHound&quot; /&gt;

==History==
Midomi, renamed SoundHound in December 2009 with the launch of version 3.0 of the mobile app,&lt;ref name=CNETv3&gt;{{cite news|last=Dolcourt|first=Jessica|title=Midomi 3.0 seeks song lyrics, knows what's hot|url=http://reviews.cnet.com/8301-19512_7-10408563-233.html|accessdate=2 October 2012|newspaper=CNET|date=3 December 2009}}&lt;/ref&gt; was launched in [[beta]] in January 2007, as a [http://www.midomi.com website], with 2 million licensed tracks.&lt;ref name=&quot;CNET1&quot; /&gt; The technology, dubbed Multimodal Adaptive Recognition System (MARS), considers pitch, tempo variation, speech content and pauses in order to recognise samples.&lt;ref name=CNET1&gt;{{cite news|last=Mills|first=Elinor|title=This Web site can name that tune|url=http://news.cnet.com/This-Web-site-can-name-that-tune/2100-1027_3-6153657.html|accessdate=2 October 2012|newspaper=CNET|date=26 January 2007}}&lt;/ref&gt; The company behind the site, Melodis Corporation, was started in 2004 by [[Chief Executive]] Keyvan Mohajer, a [[PhD]] in sound-recognition from [[Stanford]].&lt;ref name=CNET1 /&gt; Melodis changed its name to SoundHound Inc in May 2010.&lt;ref&gt;{{cite press release|title=SoundHound Inc. Announces Name Change from Melodis Corporation|publisher=SoundHound Inc|date=20 May 2010|url=//www.soundhound.com/index.php?action=s.press_release&amp;pr=15|accessdate=2 October 2012}}&lt;/ref&gt;

The first version of the app was released on the [[Apple App Store]] in July 2008.&lt;ref name=&quot;Apple App Store&quot;&gt;{{cite news | url=http://news.cnet.com/8301-17938_105-9987892-1.html | title=Sing for search results with iPhone app | date=10 July 2008 | accessdate=2 October 2012 | last=Jackson | first=Holly | newspaper=CNET}}&lt;/ref&gt; At the launch of [[Windows Marketplace for Mobile]] in October 2009, Midomi was one of the apps included in the store&lt;ref name=&quot;Windows&quot;&gt;{{cite news | url=http://reviews.cnet.com/8301-12261_7-10368174-10356022.html | title=Windows mobile app store, My Phone service officially opening | date=6 October 2009 | accessdate=2 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}&lt;/ref&gt; and could be purchased for $4.99.&lt;ref&gt;{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=2 October 2012|newspaper=CNET|date=7 October 2009}}&lt;/ref&gt; It joined the [[Android OS|Android]] app store in June 2010.&lt;ref name=&quot;Google Play&quot;&gt;{{cite news | url=http://www.cnet.com/8301-19736_1-20007745-251.html | title=New SoundHound names that tune--for free (Android) | date=15 June 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}&lt;/ref&gt; On January 2013, the [[BlackBerry]] version of the app was then available in [[BlackBerry World]] following the announcement and launch of [[BlackBerry 10]].&lt;ref name=&quot;BlackBerry&quot;&gt;{{cite web
  |title=BlackBerry shows off some of its 70,000 new third-party apps, including Skype, Rdio, Kindle, and Whatsapp
  |publisher=[[The Verge]]
  |url=http://www.theverge.com/2013/1/30/3932042/blackberry-10-apps-announcement
  |accessdate=2013-01-30}}&lt;/ref&gt;

A free version of the app was released in April 2010, with all the functionality of the premium version, while limiting the number of searches to five per month, and adding banners ads.&lt;ref name=&quot;CNETFree&quot;&gt;{{cite news | url=http://reviews.cnet.com/8301-19512_7-20003228-233.html | title=Sonic freebie: New, free SoundHound music-ID app for iPhone, iPad | date=27 April 2010 | accessdate=3 October 2012 | last=Dolcourt | first=Jessica | newspaper=CNET}}&lt;/ref&gt; The premium version was now renamed SoundHound Infinity.&lt;ref name=&quot;CNETFree&quot; /&gt; A stripped-down version, Hound, was released in May 2011.&lt;ref name=&quot;TNWHound&quot; /&gt;

In January 2011, Apple revealed that SoundHound was the top paid iPad app  on its [[Apple App Store|App Store]] was SoundHound, while rival Shazam was fourth in the top ten list of free iPhone apps.&lt;ref&gt;{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=2 October 2012|newspaper=CNET|date=19 January 2011}}&lt;/ref&gt;

In June 2012, the firm announced that it had 80 million users while version 5.0 was released, with a new design and features that include an in-built player and integration with LiveLyrics.&lt;ref name=&quot;TNW80m&quot;&gt;{{cite news | url=http://thenextweb.com/apps/2012/06/07/shazam-competitor-soundhound-passes-80m-users-and-rolls-out-updated-mobile-apps/ | title=Shazam competitor SoundHound passes 80m users and rolls out updated mobile apps | work=The Next Web | date=7 June 2012 | accessdate=2 October 2012 | author=Sawers, Paul}}&lt;/ref&gt;

In December 2013, the app passed 185 million users.&lt;ref&gt;{{cite news|title=SoundHound Reveals Its Top Songs of 2013|url=http://www.heraldonline.com/2013/12/16/5509030/soundhound-reveals-its-top-songs.html|accessdate=16 December 2013|newspaper=The Next Web|date=16 December 2013}}&lt;/ref&gt;

In December 2013, the app launches iTunes Radio integration.&lt;ref&gt;{{cite news|title=SoundHound App Update Adds iTunes Radio Integration for iPad and iPhone Users. |url=http://www.padgadget.com/2013/12/20/soundhound-app-update-adds-itunes-radio-integration-for-ipad-and-iphone-users/|accessdate=11 February 2014|newspaper=PadGadget|date=20 December 2013}}&lt;/ref&gt;

In September 2013, the app enables 170 million global users to sync, save, and transfer music search &amp; discovery history across multiple devices.&lt;ref&gt;{{cite news|title=SoundHound adds cloud history sync on iOS and Android apps|url=http://www.intomobile.com/2013/09/25/soundhound-adds-cloud-history-sync-ios-and-android-apps/|accessdate=11 February 2014|newspaper=INTOMOBILE|date=20 September 2013}}&lt;/ref&gt;

In January 2014, SoundHound and Hyundai Motor Group partnered to embed music search and discovery into select 2014 Hyundai &amp; Kia models.&lt;ref&gt;{{cite news|title=Hyundai and Kia tap SoundHound to help you identify music in your car|url=http://www.engadget.com/2014/01/14/hyundai-kia-soundhound-music-tagging/|accessdate=11 February 2014|newspaper=Engadget|date= January 14, 2014}}&lt;/ref&gt;

In January 2014, the app launched an &quot;immersive second screen GRAMMYs experience&quot;.&lt;ref&gt;{{cite news|title=SoundHound's music search app turns its focus to the Grammys with real-time updates and more|url=http://www.engadget.com/2014/01/25/soundhound-grammys-2014/|accessdate=11 February 2014|newspaper=Engadget|date= January 24, 2014}}&lt;/ref&gt;

In April 2014, the app passed 200 million users.&lt;ref&gt;//www.soundhound.com/index.php?action=s.press_release&amp;pr=67&lt;/ref&gt;

===Funding===
Melodis secured $7 million in a Series B funding round in October 2008, bringing total funds raised to $12 million. The round was led by TransLink Capital with the participation of JAIC America and [[Series A round|Series A]] investor Global Catalyst Partners.&lt;ref&gt;{{cite press release|title=Search and Sound Recognition Innovator MELODIS and Creator of Midomi Raises $7 Million in Series B Funding|publisher=Melodis Corporation|date=7 October 2008|url=//www.soundhound.com/index.php?action=s.press_release&amp;pr=5|accessdate=2 October 2012}}&lt;/ref&gt;

In 2009, Melodis attracted additional funding from Larry Marcus at Walden Venture Capital, who had previously invested in music startups [[Pandora Radio|Pandora]] and [[SNOCAP|Snocap]].&lt;ref name=CNETWVC&gt;{{cite news|last=Needleman|first=Rafe|title=Midomi music search gets funding and opportunities|url=http://news.cnet.com/8301-19882_3-10298068-250.html|accessdate=2 October 2012|newspaper=CNET|date=28 July 2009}}&lt;/ref&gt; The $4 million funding round was led by Walden Venture Capital VII, with the participation of an unnamed device manufacturer.&lt;ref&gt;{{cite press release|title=Melodis, Sound Search Technology and Applications Innovator, Raises $4M Led by Walden Venture Capital and a Strategic Investor|publisher=Melodis Corporation|date=4 August 2009|url=//www.soundhound.com/index.php?action=s.press_release&amp;pr=12|accessdate=2 October 2012}}&lt;/ref&gt;

==See also==
*[[Query by humming]]

==References==
{{reflist|2}}

==External links==
*[http://midomi.com midomi.com]
*[//www.soundhound.com/ SoundHound website]

[[Category:Android (operating system) software]]
[[Category:IOS software]]
[[Category:Symbian software]]
[[Category:Music search engines]]
[[Category:Companies established in 2005]]
[[Category:Companies based in California]]
[[Category:BlackBerry software]]</text>
      <sha1>9h6frgkezo3rgwoou9tjls4zm4mewtd</sha1>
    </revision>
  </page>
  <page>
    <title>MusicRadar (service)</title>
    <ns>0</ns>
    <id>40254525</id>
    <revision>
      <id>573680509</id>
      <parentid>572288776</parentid>
      <timestamp>2013-09-19T18:39:28Z</timestamp>
      <contributor>
        <username>LilHelpa</username>
        <id>8024439</id>
      </contributor>
      <minor/>
      <comment>copy edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2510">{{multiple issues|
{{advert|date=August 2013}}
{{cleanup|reason=Syntax, capitals|date=August 2013}}
{{fanpov|date=August 2013}}
{{Orphan|date=August 2013}}
}}
{{Infobox Website
|name=MusicRadar(音乐雷达)
|logo=
|screenshot=
|caption=
|url=http://www.doreso.com/
|commercial=Yes
|type=[[Music]] [[website]]
|registration=Optional
|owner=Shanghai Yinlong Information Technology Co., LTD
|author=Shanghai Yinlong Information Technology Co., LTD
|launch date=January 2013
|current status=
|revenue=}}

'''Music radar''' is a sound-to-sound music search engine, which allows users to obtain more detailed information of music/songs by singing/humming or by recording original music. It is available on [[App Store (iOS)|App Store]]&lt;ref&gt;https://itunes.apple.com/cn/app/yin-le-lei-da/id635262613&lt;/ref&gt; for [[iPhone]] and [[Google Play]]&lt;ref&gt;https://play.google.com/store/apps/details?id=com.voicedragon.musicclient.googleplay&lt;/ref&gt; for [[Android (operating system)|Android]] mobiles. Music radar was launched by Shanghai Yinlong Information Technology Co., LTD in Jan. 2013.&lt;ref&gt;http://www.doreso.com/&lt;/ref&gt;

==Features==
The app (MusicRadar) currently has three ways of searching music: by identifying recorded original music fragment; by humming or singing the melody using microphone; and by direct input of the name of song or singer. Users could share their searching results on [[Facebook]], [[Twitter]] or other SNS website.

==History==
The music radar team got the 1st place on Query by Singing/Humming (QBSH) task, Music Information Retrieval Evaluation eXchange (MIREX) 2012.&lt;ref&gt;http://www.music-ir.org/mirex/wiki/2012:Main_Page&lt;/ref&gt; The app was launched for business intention at the end of January 2013, supporting query by singing/humming &amp; audio fingerprinting. After two months, the app has reached its first one million user milestone in April, 2013. In May 2013, Music radar announces that it has integrated deep learning techniques in its query by singing/humming module to promote the recognize rate and reduce the user’s waiting time. In July 2013, Music radar released China's first cloud based music recognizing openAPI to public.&lt;ref&gt;[http://roll.sohu.com/20130731/n383076514.shtml 2013年7月，音乐雷达发布了国内第一个“音频检索开放云平台”，提供开放的音频检索API。]&lt;/ref&gt;

==References==
{{reflist}}



[[Category:Acoustic fingerprinting]]
[[Category:Music search engines]]
[[Category:IOS software]]
[[Category:Android (operating system) software]]</text>
      <sha1>984ryox66xf93zmbxejp803dvly6awp</sha1>
    </revision>
  </page>
  <page>
    <title>Songza</title>
    <ns>0</ns>
    <id>19017298</id>
    <revision>
      <id>638780554</id>
      <parentid>630529559</parentid>
      <timestamp>2014-12-19T14:14:40Z</timestamp>
      <contributor>
        <username>Dotronsya</username>
        <id>23016580</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12790">{{use mdy dates|date=July 2014}}
{{use American English|date=July 2014}}
{{Infobox website
|name           = Songza
|logo           = [[File:Songza Logo.jpg|frameless|150px]]
|screenshot     = 
|caption        = 
|url            = {{URL|songza.com}}
|alexa          = {{Loss}} 9,279 ({{as of|2014|4|1|alt=April 2014}})&lt;ref name=&quot;alexa&quot;&gt;{{cite web|url= http://www.alexa.com/siteinfo/songza.com |title= Songza.com Site Info | publisher= [[Alexa Internet]] |accessdate= April 1, 2014 }}&lt;/ref&gt;&lt;!--Updated monthly by OKBot.--&gt;
|commercial     = 
|type           = Free [[internet radio]]
|language       = [[English language|English]]
|location       = [[Long Island City, New York|Long Island City]], [[Queens]], [[New York City]], [[New York]], United States
|registration   = 
|owner          = [[Google Inc.]]
|author         = [[Aza Raskin]] and Scott Robbin
|launch date    = {{start date and age|2007|11|08|paren=yes}}
|current status = Active
|revenue        = 
|slogan         = Good music makes good times.&lt;ref&gt;{{cite web|url= http://songza.com |title= Songza.com Site Info | publisher= Songza Media, Inc. |accessdate= August 15, 2012 }}&lt;/ref&gt;
}}

'''Songza''' is a free [[music streaming]] and [[Recommender system|recommendation]] service for Internet users in the United States and Canada. 

Stating that its playlists are made by music experts, the service recommends various playlists based on time of day and mood or activity.&lt;ref name=&quot;The New York Times&quot;&gt;{{cite news| first= Ben| last= Sisaro| work = [[The New York Times]] |title= Pandora Faces Rivals for Ears and Ads| accessdate = June 20, 2012| url= http://www.nytimes.com/2012/06/21/business/songza-and-spotify-challenge-pandora-for-ears-and-ads.html?_r=3| date= June 20, 2012}}&lt;/ref&gt;&lt;ref name=PandoDaily&gt;{{cite web| first= Erin|last= Griffith| publisher= [[PandoDaily]]|title= Songza's Founders Realized They Weren't Thinking Radically Enough{{spaced ndash}} Here's How They Changed That| accessdate = August 15, 2012|url= http://pandodaily.com/2012/08/15/songzas-founders-realized-they-werent-thinking-radically-enough-heres-how-they-changed-that/}}&lt;/ref&gt; Songza offers playlists for activities such as waking up, working out, commuting, concentrating, unwinding, entertaining, and sleeping.&lt;ref name=&quot;The Washington Post&quot; &gt;{{cite news| first= Hayley| last= Tsukayama| work = [[The Washington Post]] |title=TechBits: Songza adapts the music to your mood| accessdate = June 23, 2012| url = http://www.washingtonpost.com/techbits-songza-adapts-the-music-to-your-mood/2012/06/23/gJQAYRzKyV_story.html| date= June 25, 2012}}&lt;/ref&gt;  Users can vote songs up or down, and the service will adapt to the user's personal music preferences.&lt;ref name=&quot;The Washington Post&quot; /&gt; Users can find playlists not just based on artists, songs, or genres, but also based on themes, interests, and eras, such as &quot;[[List of 1990s one-hit wonders in the United States|90s One-Hit Wonders]]&quot;, or &quot;Music of [[Fashion Week]]&quot;.&lt;ref name=SongzaAbout&gt;{{cite web| publisher= Songza|title= About Us| accessdate = March 25, 2011| url = http://songza.com/page/about/}}&lt;/ref&gt;

Songza is headquartered in the [[Long Island City]] neighborhood of the [[Queens]] [[borough (New York City)|borough]] of [[New York City]], [[New York]].&lt;ref name=&quot;NY Daily News&quot;&gt;{{cite news| first= Clare | last= Trapasso| work = [[Daily News (New York)|Daily News]] |title= Songza music service streams for success| accessdate = July 27, 2012| url= http://articles.nydailynews.com/2012-07-27/news/32874462_1_spotify-apps-music-download}}&lt;/ref&gt;

== History ==
[[Amie Street]] acquired Songza, a product created by [[Aza Raskin]] and Scott Robbin, in October 2008.&lt;ref&gt;{{cite web| first= Kristen| last=Nicole| publisher= bub.blicio.us |title= Interview with Amie Street: Why Keep Acquisition of Songza a Secret?| accessdate = March 25, 2011| url = http://bub.blicio.us/interview-with-amie-street-why-keep-acquisition-of-songza-a-secret/}}&lt;/ref&gt; In August 2010, Amie Street was sold to Amazon for an undisclosed amount.&lt;ref&gt;{{cite web| first= Michael | last= Arrington| publisher= [[TechCrunch]]|title= Amazon Acquires Amie Street, But Not in a Good Way| accessdate = September 8, 2010| url= http://techcrunch.com/2010/09/08/amazon-acquires-amie-street-but-not-in-a-good-way/}}&lt;/ref&gt;  Shortly after this the co-founders{{spaced ndash}} CEO Elias Roman, COO Peter Asbill, CPO Elliott Breece and CCO Eric Davich{{spaced ndash}} refocused their efforts on Songza.&lt;ref name=&quot;The New York Times&quot; /&gt;&lt;ref&gt;{{cite web| publisher= [[Internships.com]]|title= 5 in 5! with Eric Davich, Chief Content Officer and Co-Founder of Songza| accessdate = August 6, 2012| url= http://www.internships.com/eyeoftheintern/applying-2/employers-applying-2/5-5-eric-davichchief-content-officer-cofounder-songza/?cid=SO_ST_TW_080612_5IN5_SONGZA}}&lt;/ref&gt;  The team discontinued the original version and relaunched a new alpha version of Songza, keeping nothing of the original product but the name.&lt;ref name=Upstart&gt;{{cite news| first= Michael| last= del Castillo| work =  [[American City Business Journals|Upstart Business Journal]] |title= Downtime: The birth of Songza| accessdate = June 15, 2012| url= http://upstart.bizjournals.com/entrepreneurs/hot-shots/2012/06/15/songza-minigolfs-to-no-1-app.html?page=2}}&lt;/ref&gt;

Over the next year the founders experimented with various iterations, when the app originally launched in 2010 &quot;it was like a pre-Turntable.fm.  A function called Social Radio allowed users to be DJs for their friends&quot; stated PandoDaily.&lt;ref name=&quot;PandoDaily&quot; /&gt;  This version of the app allowed it to be social and crowdsourced; the problem with it was that the service as it stood was not sufficiently differentiated from other services on the market and the quality of the crowd sourced playlists was low.&lt;ref name=PandoDaily/&gt;  Following a year of testing various iterations of the alpha version of the app, Songza relaunched in beta on iPhone and Android apps on September 13, 2011, armed with a team of 25 expert music curators.&lt;ref name=&quot;The New York Times&quot; /&gt;&lt;ref name=&quot;PandoDaily&quot; /&gt;&lt;ref name=TechCrunch&gt;{{cite web| first= Rip| last= Empson| publisher= [[TechCrunch]]|title= Songza Raises Seven Figure Round; Launches Mobile, Sharable Music Collections in the Cloud| accessdate = September 13, 2011| url= http://techcrunch.com/2011/09/13/songza-raises-seven-figure-round-launches-mobile-sharable-music-collections-in-the-cloud/}}&lt;/ref&gt;&lt;ref&gt;[http://www.ad60.com/2011/09/19/songza-launches-iphone-android-apps-digitize-mix-tape/ &quot;Songza launches iPhone and Android apps to digitize the mix tape&quot;].&lt;/ref&gt;

In March 2012, Songza released its Music Concierge feature, on iPhone and the web.&lt;ref name=&quot;The New York Times&quot; /&gt;&lt;ref name = TechCrunch&gt;{{cite web| first= Jordan| last= Crook| publisher= [[TechCrunch.com]]|title= Songza, the Music Streaming Service That Does All Work for You, Launches an iPad App| accessdate = June 7, 2012| url= http://techcrunch.com/2012/06/07/songza-the-music-streaming-service-that-does-all-work-for-you-launches-an-ipad-app/}}&lt;/ref&gt;  The concierge presents users with up to six situations based on time of day, with filters for whatever mood they might be in.  For example, on a Wednesday morning a user might be presented with situations for &quot;Waking Up&quot;, &quot;Singing in the Shower&quot;, &quot;Working Out&quot; and so on.  This feature was rolled out to iPad on June 7, 2012; during the first ten days following the iPad app launch, Songza saw over 1.15 million downloads.&lt;ref&gt;{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza Hits 1.15 Million iOS Downloads in 10 Days| accessdate = June 18, 2012| url= http://www.pcmag.com/article2/0,2817,2405952,00.asp}}&lt;/ref&gt;

On June 12, 2012, Songza was listed as the top free app on iTunes for the iPad and the number two free app for the iPhone.&lt;ref&gt;{{cite web| first= Glenn| last= Peoples| publisher= [[Billboard.biz]]|title= Songza Reaches One Million iOs Downloads in Ten Days, But Is It the Next Big Thing?| accessdate = June 19, 2012| url= http://www.billboard.biz/bbbiz/others/songza-reaches-one-million-ios-downloads-1007360352.story}}&lt;/ref&gt;  Concierge was released on Android on July 10, 2012, and for Android tablets on August 14, 2012.&lt;ref&gt;{{cite web| first= Andrew| last= Kameka| publisher= Androinica.com |title= Songza re-ups with expert Music Concierge playlists, lockscreen controls, and new Holo-like design| accessdate = July 10, 2012| url= http://androinica.com/2012/07/songza-android-app/}}&lt;/ref&gt;&lt;ref&gt;{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza App Now Available on Android Tablets| accessdate = August 14, 2012| url= http://www.pcmag.com/article2/0,2817,2408435,00.asp}}&lt;/ref&gt;  The app expanded to Canada on August 7, 2012, and became the number-one overall free app in Canada on August 13, 2012.&lt;ref name=PandoDaily/&gt;&lt;ref&gt;{{cite web| first= Anand| last= Ram| publisher= o.canada.com |title= Songza's Elias Roman wants to provide the music for every mood | accessdate = August 7, 2012| url= http://o.canada.com/2012/08/05/songzas-elias-roman-wants-to-provide-the-music-for-every-mood/}}&lt;/ref&gt; Within the week of Microsoft's Build developer event in June 2013, Songza snuck in its official Windows 8 App.&lt;ref&gt;[http://www.wpcentral.com/songza-sneaks-windows-store-wins-our-hearts]. WP Central. June 27, 2013.&lt;/ref&gt;

Songza launched in Canada on August 7, 2012, and reached the one million download mark after 70 days.&lt;ref&gt;Dobby, Christine (August 23, 2012).  [http://business.financialpost.com/2012/08/23/songza-startup-singing-a-canadian-tune/ &quot;Songza startup singing a Canadian tune&quot;].  ''[[Financial Post]]''. August 23, 2012.&lt;/ref&gt;&lt;ref&gt;Crook, Jordan (October 18, 2012). [http://techcrunch.com/2012/10/18/songzas-canada-launch-nabs-1-million-new-users-in-70-days/ &quot;Songza's Canada Launch Nabs 1 Million New Users in 70 Days&quot;]. [[TechCrunch]].&lt;/ref&gt;

Starting October 2013, Songza began inserting pop-up audio/video ads when initiating a playlist so it is no longer &quot;audio-ad free&quot;. Songza reported having 5.5 million regular users at the end of 2013.&lt;ref&gt;{{Cite news|url = http://www.nytimes.com/2014/07/02/business/media/google-buys-songza-a-playlist-app-for-any-occasion.html|title = Google in Deal for Songza, a Music Playlist Service|last = Sisario|first = Ben|date = July 1, 2014|work = New York Times|accessdate = }}&lt;/ref&gt;

Songza was acquired by Google on July 1, 2014.&lt;ref&gt;{{cite web | url=http://techcrunch.com/2014/07/01/google-buys-songza/ | title=Google Buys Songza | publisher= [[TechCrunch]] | accessdate= July 1, 2014}}&lt;/ref&gt; No terms were disclosed but speculation put the price at somewhere between $15 million and $39 million. Both companies issued statements saying they were &quot;thrilled&quot; to be doing the deal.&lt;ref name=&quot;GoogleSongza&quot;&gt;{{cite news|title=Google acquires music app start-up Songza|url=http://www.businesssun.com/index.php/sid/223470673/scat/3a8a80d6f705f8cc/ht/Google-acquires-music-app-start-up-Songza|accessdate= July 3, 2014|publisher=''Business Sun''}}&lt;/ref&gt; In October 2014, following the acquisition, the [[Google Play Music|Google Play Music All Access]] service was updated to include functionality adapted from Songza's Concierge system.&lt;ref name=verge-songzagpm&gt;{{cite web|title=Google brings Songza's best feature to Play Music|url=http://www.theverge.com/2014/10/21/7027707/google-brings-best-songza-feature-to-play-music|website=The Verge|accessdate=21 October 2014}}&lt;/ref&gt;

==Similar organizations==
{{div col|colwidth=30em}}
* [[8tracks]]
* [[AccuRadio]]
* [[Deezer]]
* [[Digitally Imported]]
* [[FIT Radio]]
* [[Google Play Music]]
* [[Grooveshark]]
* [[Guvera]]
* [[iHeartRadio]]
* [[Live365]]
* [[MOG (online music)|MOG]]
* [[Musicovery]]
* [[Pandora Radio]]
* [[Rara.com]]
* [[Rdio]]
* [[Rhapsody (online music service)|Rhapsody]]
* [[Slacker Radio]]
* [[Spotify]]
* [[Soundtracker (music streaming)]]
* [[WiMP]]
* [[Xbox Music]]
{{div col end}}


{{portal|Companies|Music}}

==References==
{{reflist|30em}}
==External links==
*{{official website|songza.com}}

{{Digital distribution platforms}}
{{Google Inc.}}

[[Category:American companies established in 2007]]
[[Category:Community websites]]
[[Category:Companies based in Queens, New York]]
[[Category:Domain-specific search engines]]
[[Category:Free music]]
[[Category:Google acquisitions]]
[[Category:Internet advertising]]
[[Category:Internet companies of the United States]]
[[Category:Internet properties established in 2007]]
[[Category:Internet radio in the United States]]
[[Category:Long Island City]]
[[Category:Media companies based in New York City]]
[[Category:Music companies of the United States]]
[[Category:Music search engines]]
[[Category:Recommender systems]]
[[Category:Technology companies established in 2007]]</text>
      <sha1>9vol11oqok99h0k1ideq1b31pn7qk26</sha1>
    </revision>
  </page>
</mediawiki>
